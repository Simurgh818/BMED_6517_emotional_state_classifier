{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simurgh818/BMED_6517_emotional_state_classifier/blob/main/DEAP_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTiKd_g15Kl0",
        "outputId": "9bbf0c8e-55dc-491d-a5da-b4cdcefb7c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "     0K .......... .......... .......... .......... .......... 3.06M\n",
            "    50K .......... .......... .......... .......... .......... 6.41M\n",
            "   100K .......... .......... .......... .......... .......... 58.8M\n",
            "   150K .......... .......... ..                               61.5M=0.02sCloning into 'BMED_6517_emotional_state_classifier'...\n",
            "remote: Enumerating objects: 145, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 145 (delta 10), reused 12 (delta 1), pack-reused 114\u001b[K\n",
            "Receiving objects: 100% (145/145), 78.59 MiB | 18.79 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "Checking out files: 100% (25/25), done.\n"
          ]
        }
      ],
      "source": [
        "#importing our git repo\n",
        "import os\n",
        "if not os.path.exists('/content/BMED_6517_emotional_state_classifier'):\n",
        "  !wget https://github.com/Simurgh818/BMED_6517_emotional_state_classifier/blob/main/requirements.txt -q --show-progress --progress=dot\n",
        "  !git clone https://github.com/Simurgh818/BMED_6517_emotional_state_classifier.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKKDN0tIXKTt",
        "outputId": "4081bc1a-d7b8-45e0-cb18-8b4fc625d438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-05 15:28:48--  https://drive.google.com/drive/folders/1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.139, 74.125.142.113, 74.125.142.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing’\n",
            "\n",
            "1_9n-kRKkpnCC2wVovO     [  <=>               ] 220.95K  1012KB/s    in 0.2s    \n",
            "\n",
            "2022-12-05 15:28:48 (1012 KB/s) - ‘1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing’ saved [226251]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importing the preprocessed dataset files\n",
        "!wget https://drive.google.com/drive/folders/1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPTiWGmT7qwu",
        "outputId": "bfcc9498-d49e-441a-b4cf-0e761bebacc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(880, 5, 32, 32)\n",
            "(880, 5, 512)\n",
            "(880, 32, 6)\n",
            "(880,)\n",
            "(880,)\n",
            "(880,)\n"
          ]
        }
      ],
      "source": [
        "# import data from numpy arrays\n",
        "import numpy as np\n",
        "\n",
        "loaded_features = np.load('/content/BMED_6517_emotional_state_classifier/results/npy/EEG_features.npy', allow_pickle=True)\n",
        "\n",
        "connectivityMatrix = loaded_features.item().get('connectivity_matrix')\n",
        "connectivityLinear = loaded_features.item().get('connectivity_linear')\n",
        "wavelet = loaded_features.item().get('waveletEntropy')\n",
        "Valence = loaded_features.item().get('Valence')\n",
        "Arousal = loaded_features.item().get('Arousal')\n",
        "Classes = loaded_features.item().get('Classes')\n",
        "\n",
        "print(connectivityMatrix.shape)\n",
        "print(connectivityLinear.shape)\n",
        "print(wavelet.shape)\n",
        "print(Valence.shape)\n",
        "print(Arousal.shape)\n",
        "print(Classes.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N8fbUuUbqYp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Importing Deep Learning Libraries\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
        "from keras.models import Model,Sequential\n",
        "from keras.optimizers import Adam,SGD,RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YfwL18POR_d"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2teHjzaOeHr"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "#dataset_labels = np.load('content/gdrive/MyDrive/Colab Notebooks/Copy of labels_1_22.npy', mmap_mode='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofakMc12O3EG"
      },
      "outputs": [],
      "source": [
        "#dataset_name1 = 'Copy of bipolar_feats.npy'\n",
        "\n",
        "#dataset_bipolarfts = np.load(dataset_name1, encoding='bytes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYNle4ynO_uM"
      },
      "outputs": [],
      "source": [
        "#dataset_name2 = 'Copy of labels_1_22.npy'\n",
        "\n",
        "#dataset_labels = np.load(dataset_name2, encoding='bytes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3o_RfZLPRoI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyU4fNdmCL-l",
        "outputId": "d982df64-27aa-445d-993b-93b9a4faa52d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 2. 3.]\n",
            "(880,)\n",
            "[0. 1.]\n",
            "(880, 4)\n",
            "[128. 206. 186. 360.]\n"
          ]
        }
      ],
      "source": [
        "# Algorithm 1 for Convolutional Neural Model :\n",
        "##Require: Training EEG Dataset nntrX, Training Valence/Arousal Values nntrY, Testing subject’s EEG\n",
        "#Dataset nnteX, Testing Valence/Arousal Values nnteY\n",
        "# cnn = model(trainX, trainY )\n",
        "\n",
        "#x = dataset_bipolarfts\n",
        "x = connectivityMatrix\n",
        "#y = dataset_labels[1:881]\n",
        "y = np.vstack([Valence,Arousal]).T\n",
        "#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
        "y[np.where(y<4.5)] = 0\n",
        "y[np.where(y>=4.5)] = 1\n",
        "y_4_class = y[:,0]*2+y[:,1]\n",
        "print(np.unique(y_4_class))\n",
        "print(y_4_class.shape)\n",
        "y_one_hot = np.zeros((y.shape[0],4))\n",
        "y_one_hot[np.where(y_4_class==0),0] = 1\n",
        "y_one_hot[np.where(y_4_class==1),1] = 1\n",
        "y_one_hot[np.where(y_4_class==2),2] = 1\n",
        "y_one_hot[np.where(y_4_class==3),3] = 1\n",
        "print(np.unique(y_one_hot))\n",
        "print(y_one_hot.shape)\n",
        "print(np.sum(y_one_hot,axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dgqnDGha6IE",
        "outputId": "28c3cb95-ed4a-4f20-ec3a-f8b52c621132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(160, 5, 32, 32)\n",
            "(720, 5, 32, 32)\n",
            "(720, 4)\n"
          ]
        }
      ],
      "source": [
        "y_test = y_one_hot[:4*40,:]\n",
        "x_test = x[:4*40,:,:,:]\n",
        "y_train = y_one_hot[4*40:,:]\n",
        "x_train = x[4*40:,:,:,:]\n",
        "print(x_test.shape)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zFYJl5heF2R"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation, concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.metrics import CategoricalAccuracy,CategoricalCrossentropy,Precision,Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gc1zs7Lfex41"
      },
      "outputs": [],
      "source": [
        "from keras.layers.activation.relu import ReLU\n",
        "\n",
        "def make_CNN_layers(input_mat):\n",
        "  # layer 1\n",
        "  model = Conv2D(16,(3,3),padding='same',input_shape=input_shape)(input_mat)\n",
        "  model = ReLU()(model)\n",
        "  model = MaxPooling2D((2,2),padding='same')(model)\n",
        "  model = Dropout(0.2)(model)\n",
        "\n",
        "  # layer 2\n",
        "  model = Conv2D(32,(3,3),padding='same')(model)\n",
        "  model = ReLU()(model)\n",
        "  model = MaxPooling2D((2,2),padding='same')(model)\n",
        "  model = Dropout(0.2)(model)\n",
        "\n",
        "  #layer 3\n",
        "  model = Conv2D(64,(3,3),padding='same')(model)\n",
        "  model = ReLU()(model)\n",
        "  model = MaxPooling2D((2,2),padding='same')(model)\n",
        "  model = Dropout(0.5)(model)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "O4jnV2bo9DiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djeO0LSKdSAQ"
      },
      "outputs": [],
      "source": [
        "input_shape = x_test[0,0].shape\n",
        "input_shape = (32,32,1)\n",
        "delta_input = Input(shape = input_shape)\n",
        "delta_model = make_CNN_layers(delta_input)\n",
        "\n",
        "theta_input = Input(shape = input_shape)\n",
        "theta_model = make_CNN_layers(theta_input)\n",
        "\n",
        "alpha_input = Input(shape = input_shape)\n",
        "alpha_model = make_CNN_layers(alpha_input)\n",
        "\n",
        "beta_input = Input(shape = input_shape)\n",
        "beta_model = make_CNN_layers(beta_input)\n",
        "\n",
        "gamma_input = Input(shape = input_shape)\n",
        "gamma_model = make_CNN_layers(gamma_input)\n",
        "\n",
        "conv = concatenate([delta_model,theta_model,alpha_model,beta_model,gamma_model])\n",
        "\n",
        "conv = Flatten()(conv)\n",
        "\n",
        "dense = Dense(512)(conv)\n",
        "dense = ReLU()(dense)\n",
        "dense = Dropout(0.5)(dense)\n",
        "\n",
        "output = Dense(4,activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=[delta_input,theta_input,alpha_input,beta_input,gamma_input],\n",
        "              outputs=[output])\n",
        "\n",
        "# opt = optimizers.SGD()\n",
        "\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "              #loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              #metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
        "                       #tf.keras.metrics.FalseNegatives()])\n",
        "          \n",
        "\n",
        "\n",
        "# lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    #initial_learning_rate=1e-4,\n",
        "    #decay_steps=10000,\n",
        "    #decay_rate=0.96,\n",
        "    #staircase = True)\n",
        "\n",
        "# opt = keras.optimizers.SGD(lr=0.01, decay=1e-4, momentum=0.6, nesterov=True)\n",
        "opt = keras.optimizers.SGD(learning_rate= 1e-3)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer= opt ,metrics=['acc',Recall(),Precision()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sspGx1ekjYhR",
        "outputId": "05edf9e6-cc15-41f8-99a3-202818ad9d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_86 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_87 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_88 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_89 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_90 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_255 (Conv2D)            (None, 32, 32, 16)   160         ['input_86[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_258 (Conv2D)            (None, 32, 32, 16)   160         ['input_87[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_261 (Conv2D)            (None, 32, 32, 16)   160         ['input_88[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_264 (Conv2D)            (None, 32, 32, 16)   160         ['input_89[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_267 (Conv2D)            (None, 32, 32, 16)   160         ['input_90[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_272 (ReLU)               (None, 32, 32, 16)   0           ['conv2d_255[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_275 (ReLU)               (None, 32, 32, 16)   0           ['conv2d_258[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_278 (ReLU)               (None, 32, 32, 16)   0           ['conv2d_261[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_281 (ReLU)               (None, 32, 32, 16)   0           ['conv2d_264[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_284 (ReLU)               (None, 32, 32, 16)   0           ['conv2d_267[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_255 (MaxPooling2  (None, 16, 16, 16)  0           ['re_lu_272[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_258 (MaxPooling2  (None, 16, 16, 16)  0           ['re_lu_275[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_261 (MaxPooling2  (None, 16, 16, 16)  0           ['re_lu_278[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_264 (MaxPooling2  (None, 16, 16, 16)  0           ['re_lu_281[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_267 (MaxPooling2  (None, 16, 16, 16)  0           ['re_lu_284[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_272 (Dropout)          (None, 16, 16, 16)   0           ['max_pooling2d_255[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_275 (Dropout)          (None, 16, 16, 16)   0           ['max_pooling2d_258[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_278 (Dropout)          (None, 16, 16, 16)   0           ['max_pooling2d_261[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_281 (Dropout)          (None, 16, 16, 16)   0           ['max_pooling2d_264[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_284 (Dropout)          (None, 16, 16, 16)   0           ['max_pooling2d_267[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_256 (Conv2D)            (None, 16, 16, 32)   4640        ['dropout_272[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_259 (Conv2D)            (None, 16, 16, 32)   4640        ['dropout_275[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_262 (Conv2D)            (None, 16, 16, 32)   4640        ['dropout_278[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_265 (Conv2D)            (None, 16, 16, 32)   4640        ['dropout_281[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_268 (Conv2D)            (None, 16, 16, 32)   4640        ['dropout_284[0][0]']            \n",
            "                                                                                                  \n",
            " re_lu_273 (ReLU)               (None, 16, 16, 32)   0           ['conv2d_256[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_276 (ReLU)               (None, 16, 16, 32)   0           ['conv2d_259[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_279 (ReLU)               (None, 16, 16, 32)   0           ['conv2d_262[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_282 (ReLU)               (None, 16, 16, 32)   0           ['conv2d_265[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_285 (ReLU)               (None, 16, 16, 32)   0           ['conv2d_268[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_256 (MaxPooling2  (None, 8, 8, 32)    0           ['re_lu_273[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_259 (MaxPooling2  (None, 8, 8, 32)    0           ['re_lu_276[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_262 (MaxPooling2  (None, 8, 8, 32)    0           ['re_lu_279[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_265 (MaxPooling2  (None, 8, 8, 32)    0           ['re_lu_282[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_268 (MaxPooling2  (None, 8, 8, 32)    0           ['re_lu_285[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_273 (Dropout)          (None, 8, 8, 32)     0           ['max_pooling2d_256[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_276 (Dropout)          (None, 8, 8, 32)     0           ['max_pooling2d_259[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_279 (Dropout)          (None, 8, 8, 32)     0           ['max_pooling2d_262[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_282 (Dropout)          (None, 8, 8, 32)     0           ['max_pooling2d_265[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_285 (Dropout)          (None, 8, 8, 32)     0           ['max_pooling2d_268[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_257 (Conv2D)            (None, 8, 8, 64)     18496       ['dropout_273[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_260 (Conv2D)            (None, 8, 8, 64)     18496       ['dropout_276[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_263 (Conv2D)            (None, 8, 8, 64)     18496       ['dropout_279[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_266 (Conv2D)            (None, 8, 8, 64)     18496       ['dropout_282[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_269 (Conv2D)            (None, 8, 8, 64)     18496       ['dropout_285[0][0]']            \n",
            "                                                                                                  \n",
            " re_lu_274 (ReLU)               (None, 8, 8, 64)     0           ['conv2d_257[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_277 (ReLU)               (None, 8, 8, 64)     0           ['conv2d_260[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_280 (ReLU)               (None, 8, 8, 64)     0           ['conv2d_263[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_283 (ReLU)               (None, 8, 8, 64)     0           ['conv2d_266[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_286 (ReLU)               (None, 8, 8, 64)     0           ['conv2d_269[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_257 (MaxPooling2  (None, 4, 4, 64)    0           ['re_lu_274[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_260 (MaxPooling2  (None, 4, 4, 64)    0           ['re_lu_277[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_263 (MaxPooling2  (None, 4, 4, 64)    0           ['re_lu_280[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_266 (MaxPooling2  (None, 4, 4, 64)    0           ['re_lu_283[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_269 (MaxPooling2  (None, 4, 4, 64)    0           ['re_lu_286[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_274 (Dropout)          (None, 4, 4, 64)     0           ['max_pooling2d_257[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_277 (Dropout)          (None, 4, 4, 64)     0           ['max_pooling2d_260[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_280 (Dropout)          (None, 4, 4, 64)     0           ['max_pooling2d_263[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_283 (Dropout)          (None, 4, 4, 64)     0           ['max_pooling2d_266[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_286 (Dropout)          (None, 4, 4, 64)     0           ['max_pooling2d_269[0][0]']      \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 4, 4, 320)    0           ['dropout_274[0][0]',            \n",
            "                                                                  'dropout_277[0][0]',            \n",
            "                                                                  'dropout_280[0][0]',            \n",
            "                                                                  'dropout_283[0][0]',            \n",
            "                                                                  'dropout_286[0][0]']            \n",
            "                                                                                                  \n",
            " flatten_17 (Flatten)           (None, 5120)         0           ['concatenate_17[0][0]']         \n",
            "                                                                                                  \n",
            " dense_34 (Dense)               (None, 512)          2621952     ['flatten_17[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_287 (ReLU)               (None, 512)          0           ['dense_34[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_287 (Dropout)          (None, 512)          0           ['re_lu_287[0][0]']              \n",
            "                                                                                                  \n",
            " dense_35 (Dense)               (None, 4)            2052        ['dropout_287[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,740,484\n",
            "Trainable params: 2,740,484\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FutIrekXjult",
        "outputId": "5403e5c7-1e8e-44d1-d9e5-f127bd4dc7bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.4620 - acc: 0.2062 - recall_16: 0.0000e+00 - precision_16: 0.0000e+00\n",
            "Epoch 1: val_loss improved from inf to 1.37756, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 2s 55ms/step - loss: 1.4595 - acc: 0.2103 - recall_16: 0.0000e+00 - precision_16: 0.0000e+00 - val_loss: 1.3776 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 2/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.4183 - acc: 0.2615 - recall_16: 0.0000e+00 - precision_16: 0.0000e+00\n",
            "Epoch 2: val_loss improved from 1.37756 to 1.35577, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 23ms/step - loss: 1.4149 - acc: 0.2632 - recall_16: 0.0015 - precision_16: 1.0000 - val_loss: 1.3558 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 3/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3874 - acc: 0.2640 - recall_16: 0.0000e+00 - precision_16: 0.0000e+00\n",
            "Epoch 3: val_loss improved from 1.35577 to 1.34057, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 1.3857 - acc: 0.2897 - recall_16: 0.0000e+00 - precision_16: 0.0000e+00 - val_loss: 1.3406 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 4/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3795 - acc: 0.3145 - recall_16: 0.0000e+00 - precision_16: 0.0000e+00\n",
            "Epoch 4: val_loss improved from 1.34057 to 1.32824, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 1.3791 - acc: 0.3132 - recall_16: 0.0000e+00 - precision_16: 0.0000e+00 - val_loss: 1.3282 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 5/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3483 - acc: 0.3527 - recall_16: 0.0018 - precision_16: 1.0000        \n",
            "Epoch 5: val_loss improved from 1.32824 to 1.31967, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.3479 - acc: 0.3544 - recall_16: 0.0015 - precision_16: 0.5000 - val_loss: 1.3197 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 6/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3367 - acc: 0.3588 - recall_16: 0.0029 - precision_16: 0.3333\n",
            "Epoch 6: val_loss improved from 1.31967 to 1.31237, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 21ms/step - loss: 1.3367 - acc: 0.3588 - recall_16: 0.0029 - precision_16: 0.3333 - val_loss: 1.3124 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 7/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3292 - acc: 0.3940 - recall_16: 0.0060 - precision_16: 0.7500\n",
            "Epoch 7: val_loss improved from 1.31237 to 1.30737, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.3468 - acc: 0.3706 - recall_16: 0.0074 - precision_16: 0.5556 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 8/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3390 - acc: 0.3636 - recall_16: 0.0073 - precision_16: 0.4000\n",
            "Epoch 8: val_loss improved from 1.30737 to 1.30352, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.3315 - acc: 0.3750 - recall_16: 0.0118 - precision_16: 0.5714 - val_loss: 1.3035 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 9/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3080 - acc: 0.4000 - recall_16: 0.0127 - precision_16: 0.5385\n",
            "Epoch 9: val_loss improved from 1.30352 to 1.29964, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 21ms/step - loss: 1.3178 - acc: 0.3926 - recall_16: 0.0132 - precision_16: 0.5294 - val_loss: 1.2996 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 10/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3302 - acc: 0.3964 - recall_16: 0.0145 - precision_16: 0.2963\n",
            "Epoch 10: val_loss improved from 1.29964 to 1.29795, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.3230 - acc: 0.3985 - recall_16: 0.0162 - precision_16: 0.3235 - val_loss: 1.2979 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 11/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3414 - acc: 0.3818 - recall_16: 0.0182 - precision_16: 0.4348\n",
            "Epoch 11: val_loss improved from 1.29795 to 1.29639, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 1.3337 - acc: 0.3926 - recall_16: 0.0191 - precision_16: 0.4643 - val_loss: 1.2964 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 12/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3148 - acc: 0.4018 - recall_16: 0.0345 - precision_16: 0.5278\n",
            "Epoch 12: val_loss improved from 1.29639 to 1.29493, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.3233 - acc: 0.3971 - recall_16: 0.0309 - precision_16: 0.5122 - val_loss: 1.2949 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 13/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3171 - acc: 0.4091 - recall_16: 0.0200 - precision_16: 0.3333\n",
            "Epoch 13: val_loss improved from 1.29493 to 1.29401, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.3114 - acc: 0.4147 - recall_16: 0.0191 - precision_16: 0.3250 - val_loss: 1.2940 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 14/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3249 - acc: 0.3985 - recall_16: 0.0382 - precision_16: 0.4561\n",
            "Epoch 14: val_loss improved from 1.29401 to 1.29333, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 22ms/step - loss: 1.3249 - acc: 0.3985 - recall_16: 0.0382 - precision_16: 0.4561 - val_loss: 1.2933 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 15/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3052 - acc: 0.4236 - recall_16: 0.0418 - precision_16: 0.4694\n",
            "Epoch 15: val_loss did not improve from 1.29333\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3118 - acc: 0.4176 - recall_16: 0.0397 - precision_16: 0.4355 - val_loss: 1.2936 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 16/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3468 - acc: 0.3836 - recall_16: 0.0345 - precision_16: 0.3800\n",
            "Epoch 16: val_loss improved from 1.29333 to 1.29298, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.3228 - acc: 0.4059 - recall_16: 0.0382 - precision_16: 0.3939 - val_loss: 1.2930 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 17/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3235 - acc: 0.3982 - recall_16: 0.0436 - precision_16: 0.4444\n",
            "Epoch 17: val_loss improved from 1.29298 to 1.29263, saving model to weights.best.hdf5\n",
            "14/14 [==============================] - 0s 20ms/step - loss: 1.3114 - acc: 0.4147 - recall_16: 0.0441 - precision_16: 0.4545 - val_loss: 1.2926 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 18/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3362 - acc: 0.3980 - recall_16: 0.0640 - precision_16: 0.5818\n",
            "Epoch 18: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3255 - acc: 0.4029 - recall_16: 0.0544 - precision_16: 0.5139 - val_loss: 1.2936 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 19/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2946 - acc: 0.4400 - recall_16: 0.0460 - precision_16: 0.4894\n",
            "Epoch 19: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3070 - acc: 0.4279 - recall_16: 0.0412 - precision_16: 0.4242 - val_loss: 1.2929 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 20/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3219 - acc: 0.4180 - recall_16: 0.0340 - precision_16: 0.3091\n",
            "Epoch 20: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3177 - acc: 0.4088 - recall_16: 0.0397 - precision_16: 0.3803 - val_loss: 1.2930 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 21/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2924 - acc: 0.4300 - recall_16: 0.0500 - precision_16: 0.4098\n",
            "Epoch 21: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3039 - acc: 0.4235 - recall_16: 0.0529 - precision_16: 0.4235 - val_loss: 1.2931 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 22/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3194 - acc: 0.3860 - recall_16: 0.0460 - precision_16: 0.3833\n",
            "Epoch 22: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3194 - acc: 0.3956 - recall_16: 0.0500 - precision_16: 0.4146 - val_loss: 1.2933 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 23/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3218 - acc: 0.3691 - recall_16: 0.0436 - precision_16: 0.4615\n",
            "Epoch 23: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3187 - acc: 0.3750 - recall_16: 0.0441 - precision_16: 0.4762 - val_loss: 1.2937 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 24/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3143 - acc: 0.4145 - recall_16: 0.0618 - precision_16: 0.4928\n",
            "Epoch 24: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3080 - acc: 0.4162 - recall_16: 0.0544 - precision_16: 0.4684 - val_loss: 1.2940 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 25/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3121 - acc: 0.4073 - recall_16: 0.0436 - precision_16: 0.4138\n",
            "Epoch 25: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3115 - acc: 0.4132 - recall_16: 0.0485 - precision_16: 0.4521 - val_loss: 1.2942 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 26/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3145 - acc: 0.4127 - recall_16: 0.0327 - precision_16: 0.3673\n",
            "Epoch 26: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3150 - acc: 0.4029 - recall_16: 0.0324 - precision_16: 0.3729 - val_loss: 1.2937 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 27/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3067 - acc: 0.4273 - recall_16: 0.0509 - precision_16: 0.4590\n",
            "Epoch 27: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3178 - acc: 0.4118 - recall_16: 0.0471 - precision_16: 0.4706 - val_loss: 1.2939 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 28/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3073 - acc: 0.3891 - recall_16: 0.0564 - precision_16: 0.5082\n",
            "Epoch 28: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2991 - acc: 0.4015 - recall_16: 0.0529 - precision_16: 0.4932 - val_loss: 1.2941 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 29/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3241 - acc: 0.3855 - recall_16: 0.0455 - precision_16: 0.3906\n",
            "Epoch 29: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3163 - acc: 0.3985 - recall_16: 0.0471 - precision_16: 0.4000 - val_loss: 1.2941 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 30/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2964 - acc: 0.4309 - recall_16: 0.0309 - precision_16: 0.3953\n",
            "Epoch 30: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3084 - acc: 0.4206 - recall_16: 0.0309 - precision_16: 0.3231 - val_loss: 1.2947 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 31/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3283 - acc: 0.4055 - recall_16: 0.0327 - precision_16: 0.3462\n",
            "Epoch 31: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3228 - acc: 0.4044 - recall_16: 0.0353 - precision_16: 0.3529 - val_loss: 1.2956 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 32/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3129 - acc: 0.4218 - recall_16: 0.0455 - precision_16: 0.3788\n",
            "Epoch 32: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3101 - acc: 0.4279 - recall_16: 0.0485 - precision_16: 0.3976 - val_loss: 1.2955 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 33/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3185 - acc: 0.4060 - recall_16: 0.0360 - precision_16: 0.3462\n",
            "Epoch 33: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3128 - acc: 0.4162 - recall_16: 0.0368 - precision_16: 0.3788 - val_loss: 1.2956 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 34/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3064 - acc: 0.4218 - recall_16: 0.0545 - precision_16: 0.4688\n",
            "Epoch 34: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3129 - acc: 0.4221 - recall_16: 0.0574 - precision_16: 0.4815 - val_loss: 1.2956 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 35/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3119 - acc: 0.4018 - recall_16: 0.0327 - precision_16: 0.4615\n",
            "Epoch 35: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3113 - acc: 0.4044 - recall_16: 0.0309 - precision_16: 0.4468 - val_loss: 1.2951 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 36/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3140 - acc: 0.4055 - recall_16: 0.0436 - precision_16: 0.3478\n",
            "Epoch 36: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.3143 - acc: 0.4074 - recall_16: 0.0426 - precision_16: 0.3766 - val_loss: 1.2957 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 37/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3118 - acc: 0.3982 - recall_16: 0.0255 - precision_16: 0.3256\n",
            "Epoch 37: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3123 - acc: 0.3956 - recall_16: 0.0397 - precision_16: 0.4286 - val_loss: 1.2960 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 38/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3070 - acc: 0.4236 - recall_16: 0.0473 - precision_16: 0.4062\n",
            "Epoch 38: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3145 - acc: 0.4250 - recall_16: 0.0441 - precision_16: 0.4054 - val_loss: 1.2956 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 39/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3018 - acc: 0.4036 - recall_16: 0.0436 - precision_16: 0.3810\n",
            "Epoch 39: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3092 - acc: 0.4059 - recall_16: 0.0456 - precision_16: 0.3974 - val_loss: 1.2962 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 40/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2889 - acc: 0.4200 - recall_16: 0.0436 - precision_16: 0.5106\n",
            "Epoch 40: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2875 - acc: 0.4279 - recall_16: 0.0485 - precision_16: 0.5593 - val_loss: 1.2961 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 41/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3226 - acc: 0.4055 - recall_16: 0.0327 - precision_16: 0.3600\n",
            "Epoch 41: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3183 - acc: 0.4103 - recall_16: 0.0397 - precision_16: 0.3857 - val_loss: 1.2959 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 42/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3140 - acc: 0.4200 - recall_16: 0.0455 - precision_16: 0.4386\n",
            "Epoch 42: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3124 - acc: 0.4206 - recall_16: 0.0456 - precision_16: 0.4306 - val_loss: 1.2963 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 43/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3146 - acc: 0.4091 - recall_16: 0.0473 - precision_16: 0.5652\n",
            "Epoch 43: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3155 - acc: 0.4118 - recall_16: 0.0500 - precision_16: 0.5312 - val_loss: 1.2959 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 44/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3208 - acc: 0.4127 - recall_16: 0.0327 - precision_16: 0.3673\n",
            "Epoch 44: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3135 - acc: 0.4132 - recall_16: 0.0412 - precision_16: 0.4308 - val_loss: 1.2958 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 45/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2986 - acc: 0.4018 - recall_16: 0.0636 - precision_16: 0.5385\n",
            "Epoch 45: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 12ms/step - loss: 1.2986 - acc: 0.4118 - recall_16: 0.0544 - precision_16: 0.5139 - val_loss: 1.2956 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 46/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2946 - acc: 0.4200 - recall_16: 0.0473 - precision_16: 0.4262\n",
            "Epoch 46: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3112 - acc: 0.4103 - recall_16: 0.0441 - precision_16: 0.4000 - val_loss: 1.2951 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 47/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.3178 - acc: 0.4067 - recall_16: 0.0367 - precision_16: 0.3607\n",
            "Epoch 47: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3178 - acc: 0.4074 - recall_16: 0.0338 - precision_16: 0.3382 - val_loss: 1.2958 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 48/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3037 - acc: 0.4073 - recall_16: 0.0309 - precision_16: 0.2698\n",
            "Epoch 48: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3090 - acc: 0.4059 - recall_16: 0.0294 - precision_16: 0.2985 - val_loss: 1.2960 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 49/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3180 - acc: 0.4206 - recall_16: 0.0515 - precision_16: 0.4605\n",
            "Epoch 49: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3180 - acc: 0.4206 - recall_16: 0.0515 - precision_16: 0.4605 - val_loss: 1.2962 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 50/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2923 - acc: 0.4291 - recall_16: 0.0491 - precision_16: 0.5400\n",
            "Epoch 50: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3016 - acc: 0.4176 - recall_16: 0.0485 - precision_16: 0.5323 - val_loss: 1.2960 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 51/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2961 - acc: 0.4040 - recall_16: 0.0500 - precision_16: 0.4902\n",
            "Epoch 51: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2957 - acc: 0.4029 - recall_16: 0.0515 - precision_16: 0.5224 - val_loss: 1.2956 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 52/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3240 - acc: 0.3900 - recall_16: 0.0360 - precision_16: 0.3462\n",
            "Epoch 52: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3174 - acc: 0.3956 - recall_16: 0.0426 - precision_16: 0.4143 - val_loss: 1.2960 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 53/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2975 - acc: 0.4200 - recall_16: 0.0436 - precision_16: 0.5000\n",
            "Epoch 53: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2992 - acc: 0.4191 - recall_16: 0.0397 - precision_16: 0.4500 - val_loss: 1.2957 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 54/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3059 - acc: 0.4127 - recall_16: 0.0291 - precision_16: 0.3404\n",
            "Epoch 54: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3104 - acc: 0.4103 - recall_16: 0.0279 - precision_16: 0.3220 - val_loss: 1.2963 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 55/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3012 - acc: 0.4103 - recall_16: 0.0515 - precision_16: 0.4795\n",
            "Epoch 55: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3012 - acc: 0.4103 - recall_16: 0.0515 - precision_16: 0.4795 - val_loss: 1.2967 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 56/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2772 - acc: 0.4364 - recall_16: 0.0455 - precision_16: 0.5000\n",
            "Epoch 56: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2907 - acc: 0.4235 - recall_16: 0.0397 - precision_16: 0.4426 - val_loss: 1.2969 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 57/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3040 - acc: 0.4120 - recall_16: 0.0280 - precision_16: 0.2979\n",
            "Epoch 57: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3054 - acc: 0.4088 - recall_16: 0.0324 - precision_16: 0.3729 - val_loss: 1.2970 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 58/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3362 - acc: 0.4080 - recall_16: 0.0260 - precision_16: 0.3421\n",
            "Epoch 58: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3202 - acc: 0.4162 - recall_16: 0.0309 - precision_16: 0.4038 - val_loss: 1.2968 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 59/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3003 - acc: 0.4060 - recall_16: 0.0440 - precision_16: 0.4074\n",
            "Epoch 59: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3037 - acc: 0.4059 - recall_16: 0.0412 - precision_16: 0.3944 - val_loss: 1.2968 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 60/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2854 - acc: 0.4364 - recall_16: 0.0436 - precision_16: 0.4800\n",
            "Epoch 60: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2978 - acc: 0.4206 - recall_16: 0.0397 - precision_16: 0.4500 - val_loss: 1.2970 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 61/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2963 - acc: 0.4236 - recall_16: 0.0473 - precision_16: 0.4727\n",
            "Epoch 61: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3052 - acc: 0.4132 - recall_16: 0.0471 - precision_16: 0.4923 - val_loss: 1.2975 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 62/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3173 - acc: 0.4091 - recall_16: 0.0418 - precision_16: 0.4340\n",
            "Epoch 62: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3090 - acc: 0.4103 - recall_16: 0.0426 - precision_16: 0.4531 - val_loss: 1.2974 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 63/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3083 - acc: 0.4200 - recall_16: 0.0260 - precision_16: 0.3714\n",
            "Epoch 63: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3149 - acc: 0.4088 - recall_16: 0.0309 - precision_16: 0.3962 - val_loss: 1.2971 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 64/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3031 - acc: 0.4255 - recall_16: 0.0255 - precision_16: 0.3256\n",
            "Epoch 64: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3008 - acc: 0.4250 - recall_16: 0.0309 - precision_16: 0.3684 - val_loss: 1.2972 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 65/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3055 - acc: 0.4000 - recall_16: 0.0400 - precision_16: 0.4167\n",
            "Epoch 65: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3064 - acc: 0.4088 - recall_16: 0.0353 - precision_16: 0.3810 - val_loss: 1.2975 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 66/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2806 - acc: 0.4218 - recall_16: 0.0345 - precision_16: 0.4419\n",
            "Epoch 66: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2913 - acc: 0.4147 - recall_16: 0.0338 - precision_16: 0.3966 - val_loss: 1.2973 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 67/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3125 - acc: 0.4145 - recall_16: 0.0418 - precision_16: 0.4894\n",
            "Epoch 67: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3099 - acc: 0.4176 - recall_16: 0.0382 - precision_16: 0.4643 - val_loss: 1.2970 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 68/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2860 - acc: 0.4255 - recall_16: 0.0382 - precision_16: 0.4565\n",
            "Epoch 68: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2984 - acc: 0.4103 - recall_16: 0.0324 - precision_16: 0.4151 - val_loss: 1.2969 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 69/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3088 - acc: 0.4147 - recall_16: 0.0324 - precision_16: 0.4000\n",
            "Epoch 69: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3088 - acc: 0.4147 - recall_16: 0.0324 - precision_16: 0.4000 - val_loss: 1.2972 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 70/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2997 - acc: 0.4273 - recall_16: 0.0418 - precision_16: 0.5111\n",
            "Epoch 70: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2970 - acc: 0.4235 - recall_16: 0.0426 - precision_16: 0.5000 - val_loss: 1.2970 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 71/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2987 - acc: 0.4255 - recall_16: 0.0291 - precision_16: 0.4000\n",
            "Epoch 71: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3047 - acc: 0.4118 - recall_16: 0.0309 - precision_16: 0.4468 - val_loss: 1.2968 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 72/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3109 - acc: 0.4109 - recall_16: 0.0309 - precision_16: 0.3269\n",
            "Epoch 72: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3149 - acc: 0.4147 - recall_16: 0.0294 - precision_16: 0.3226 - val_loss: 1.2974 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 73/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3086 - acc: 0.4127 - recall_16: 0.0473 - precision_16: 0.4561\n",
            "Epoch 73: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3045 - acc: 0.4191 - recall_16: 0.0471 - precision_16: 0.4638 - val_loss: 1.2972 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 74/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3009 - acc: 0.4180 - recall_16: 0.0400 - precision_16: 0.5000\n",
            "Epoch 74: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2913 - acc: 0.4191 - recall_16: 0.0397 - precision_16: 0.5192 - val_loss: 1.2969 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 75/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2914 - acc: 0.4291 - recall_16: 0.0382 - precision_16: 0.4565\n",
            "Epoch 75: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2969 - acc: 0.4235 - recall_16: 0.0382 - precision_16: 0.4483 - val_loss: 1.2973 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 76/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3061 - acc: 0.4255 - recall_16: 0.0273 - precision_16: 0.3750\n",
            "Epoch 76: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.3093 - acc: 0.4147 - recall_16: 0.0294 - precision_16: 0.3846 - val_loss: 1.2980 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 77/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3049 - acc: 0.4182 - recall_16: 0.0327 - precision_16: 0.3396\n",
            "Epoch 77: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3051 - acc: 0.4118 - recall_16: 0.0324 - precision_16: 0.3492 - val_loss: 1.2984 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 78/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3127 - acc: 0.4055 - recall_16: 0.0273 - precision_16: 0.3571\n",
            "Epoch 78: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3121 - acc: 0.4088 - recall_16: 0.0353 - precision_16: 0.4068 - val_loss: 1.2989 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 79/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3284 - acc: 0.4103 - recall_16: 0.0191 - precision_16: 0.2321\n",
            "Epoch 79: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3284 - acc: 0.4103 - recall_16: 0.0191 - precision_16: 0.2321 - val_loss: 1.2991 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 80/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3186 - acc: 0.3980 - recall_16: 0.0340 - precision_16: 0.3469\n",
            "Epoch 80: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3122 - acc: 0.4103 - recall_16: 0.0309 - precision_16: 0.3443 - val_loss: 1.2993 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 81/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2943 - acc: 0.4200 - recall_16: 0.0309 - precision_16: 0.3953\n",
            "Epoch 81: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2951 - acc: 0.4191 - recall_16: 0.0338 - precision_16: 0.3966 - val_loss: 1.2992 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 82/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2934 - acc: 0.4291 - recall_16: 0.0273 - precision_16: 0.4545\n",
            "Epoch 82: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2955 - acc: 0.4250 - recall_16: 0.0279 - precision_16: 0.4419 - val_loss: 1.2987 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 83/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3115 - acc: 0.4073 - recall_16: 0.0400 - precision_16: 0.5366\n",
            "Epoch 83: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3028 - acc: 0.4191 - recall_16: 0.0382 - precision_16: 0.5417 - val_loss: 1.2985 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 84/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2993 - acc: 0.4127 - recall_16: 0.0255 - precision_16: 0.3784\n",
            "Epoch 84: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2949 - acc: 0.4206 - recall_16: 0.0265 - precision_16: 0.3673 - val_loss: 1.2984 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 85/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3017 - acc: 0.4160 - recall_16: 0.0460 - precision_16: 0.4510\n",
            "Epoch 85: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2990 - acc: 0.4162 - recall_16: 0.0426 - precision_16: 0.4462 - val_loss: 1.2987 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 86/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3044 - acc: 0.4164 - recall_16: 0.0364 - precision_16: 0.4762\n",
            "Epoch 86: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3107 - acc: 0.4103 - recall_16: 0.0338 - precision_16: 0.4694 - val_loss: 1.2991 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 87/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2992 - acc: 0.4250 - recall_16: 0.0368 - precision_16: 0.3788\n",
            "Epoch 87: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2992 - acc: 0.4250 - recall_16: 0.0368 - precision_16: 0.3788 - val_loss: 1.2992 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 88/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3082 - acc: 0.4036 - recall_16: 0.0418 - precision_16: 0.5750\n",
            "Epoch 88: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3088 - acc: 0.4059 - recall_16: 0.0412 - precision_16: 0.5600 - val_loss: 1.2991 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 89/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3153 - acc: 0.4218 - recall_16: 0.0309 - precision_16: 0.4048\n",
            "Epoch 89: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3084 - acc: 0.4176 - recall_16: 0.0412 - precision_16: 0.4828 - val_loss: 1.2991 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 90/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3101 - acc: 0.4145 - recall_16: 0.0255 - precision_16: 0.4516\n",
            "Epoch 90: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3134 - acc: 0.4162 - recall_16: 0.0250 - precision_16: 0.4250 - val_loss: 1.2994 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 91/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2981 - acc: 0.4127 - recall_16: 0.0291 - precision_16: 0.4000\n",
            "Epoch 91: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3036 - acc: 0.4132 - recall_16: 0.0250 - precision_16: 0.3617 - val_loss: 1.2997 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 92/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2933 - acc: 0.4200 - recall_16: 0.0364 - precision_16: 0.5405\n",
            "Epoch 92: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2927 - acc: 0.4221 - recall_16: 0.0368 - precision_16: 0.5208 - val_loss: 1.2995 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 93/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3132 - acc: 0.4273 - recall_16: 0.0345 - precision_16: 0.5278\n",
            "Epoch 93: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3090 - acc: 0.4250 - recall_16: 0.0382 - precision_16: 0.5532 - val_loss: 1.2995 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 94/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3152 - acc: 0.4200 - recall_16: 0.0255 - precision_16: 0.2857\n",
            "Epoch 94: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3162 - acc: 0.4162 - recall_16: 0.0279 - precision_16: 0.3115 - val_loss: 1.3001 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 95/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2850 - acc: 0.4200 - recall_16: 0.0380 - precision_16: 0.4318\n",
            "Epoch 95: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2945 - acc: 0.4118 - recall_16: 0.0338 - precision_16: 0.4107 - val_loss: 1.2999 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 96/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3056 - acc: 0.4080 - recall_16: 0.0300 - precision_16: 0.4167\n",
            "Epoch 96: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3024 - acc: 0.4103 - recall_16: 0.0382 - precision_16: 0.4643 - val_loss: 1.3002 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 97/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3094 - acc: 0.4100 - recall_16: 0.0320 - precision_16: 0.4444\n",
            "Epoch 97: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3110 - acc: 0.4132 - recall_16: 0.0353 - precision_16: 0.4800 - val_loss: 1.3002 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 98/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2997 - acc: 0.4147 - recall_16: 0.0235 - precision_16: 0.3636\n",
            "Epoch 98: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2997 - acc: 0.4147 - recall_16: 0.0235 - precision_16: 0.3636 - val_loss: 1.3001 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 99/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2800 - acc: 0.4309 - recall_16: 0.0564 - precision_16: 0.5962\n",
            "Epoch 99: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2970 - acc: 0.4191 - recall_16: 0.0515 - precision_16: 0.5645 - val_loss: 1.3001 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 100/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3072 - acc: 0.4145 - recall_16: 0.0255 - precision_16: 0.4242\n",
            "Epoch 100: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3014 - acc: 0.4206 - recall_16: 0.0279 - precision_16: 0.4318 - val_loss: 1.3001 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 101/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3025 - acc: 0.4200 - recall_16: 0.0260 - precision_16: 0.3714\n",
            "Epoch 101: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3049 - acc: 0.4191 - recall_16: 0.0294 - precision_16: 0.4167 - val_loss: 1.2998 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 102/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3121 - acc: 0.4040 - recall_16: 0.0240 - precision_16: 0.3243\n",
            "Epoch 102: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3019 - acc: 0.4221 - recall_16: 0.0206 - precision_16: 0.3333 - val_loss: 1.2998 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 103/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2906 - acc: 0.4400 - recall_16: 0.0309 - precision_16: 0.4146\n",
            "Epoch 103: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2977 - acc: 0.4265 - recall_16: 0.0338 - precision_16: 0.4600 - val_loss: 1.2999 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 104/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2993 - acc: 0.4180 - recall_16: 0.0300 - precision_16: 0.4688\n",
            "Epoch 104: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3043 - acc: 0.4265 - recall_16: 0.0279 - precision_16: 0.4222 - val_loss: 1.3004 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 105/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3143 - acc: 0.4000 - recall_16: 0.0240 - precision_16: 0.3871\n",
            "Epoch 105: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3025 - acc: 0.4074 - recall_16: 0.0279 - precision_16: 0.4318 - val_loss: 1.3003 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 106/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2935 - acc: 0.4200 - recall_16: 0.0280 - precision_16: 0.3889\n",
            "Epoch 106: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3074 - acc: 0.4147 - recall_16: 0.0279 - precision_16: 0.3725 - val_loss: 1.3005 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 107/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2851 - acc: 0.4400 - recall_16: 0.0400 - precision_16: 0.5500\n",
            "Epoch 107: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3032 - acc: 0.4191 - recall_16: 0.0397 - precision_16: 0.5510 - val_loss: 1.3005 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 108/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3122 - acc: 0.3900 - recall_16: 0.0180 - precision_16: 0.3750\n",
            "Epoch 108: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3048 - acc: 0.4191 - recall_16: 0.0162 - precision_16: 0.3438 - val_loss: 1.3006 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 109/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3066 - acc: 0.4182 - recall_16: 0.0255 - precision_16: 0.4828\n",
            "Epoch 109: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3081 - acc: 0.4206 - recall_16: 0.0235 - precision_16: 0.4211 - val_loss: 1.3009 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 110/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3163 - acc: 0.3960 - recall_16: 0.0320 - precision_16: 0.5926\n",
            "Epoch 110: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3021 - acc: 0.4176 - recall_16: 0.0309 - precision_16: 0.5250 - val_loss: 1.3009 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 111/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3022 - acc: 0.4200 - recall_16: 0.0255 - precision_16: 0.3684\n",
            "Epoch 111: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2983 - acc: 0.4206 - recall_16: 0.0309 - precision_16: 0.4200 - val_loss: 1.3009 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 112/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3070 - acc: 0.4140 - recall_16: 0.0220 - precision_16: 0.3929\n",
            "Epoch 112: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3047 - acc: 0.4132 - recall_16: 0.0265 - precision_16: 0.4500 - val_loss: 1.3010 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 113/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3077 - acc: 0.4200 - recall_16: 0.0280 - precision_16: 0.5000\n",
            "Epoch 113: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3013 - acc: 0.4235 - recall_16: 0.0279 - precision_16: 0.5278 - val_loss: 1.3009 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 114/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2979 - acc: 0.4309 - recall_16: 0.0218 - precision_16: 0.3529\n",
            "Epoch 114: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3022 - acc: 0.4221 - recall_16: 0.0206 - precision_16: 0.3590 - val_loss: 1.3012 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 115/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3097 - acc: 0.4140 - recall_16: 0.0200 - precision_16: 0.3448\n",
            "Epoch 115: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3037 - acc: 0.4176 - recall_16: 0.0206 - precision_16: 0.3333 - val_loss: 1.3011 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 116/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2963 - acc: 0.4300 - recall_16: 0.0320 - precision_16: 0.5000\n",
            "Epoch 116: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2993 - acc: 0.4250 - recall_16: 0.0338 - precision_16: 0.4694 - val_loss: 1.3013 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 117/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3223 - acc: 0.4000 - recall_16: 0.0273 - precision_16: 0.4412\n",
            "Epoch 117: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3062 - acc: 0.4162 - recall_16: 0.0309 - precision_16: 0.4884 - val_loss: 1.3015 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 118/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2871 - acc: 0.4380 - recall_16: 0.0340 - precision_16: 0.5152\n",
            "Epoch 118: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3000 - acc: 0.4221 - recall_16: 0.0309 - precision_16: 0.4667 - val_loss: 1.3018 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 119/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2898 - acc: 0.4473 - recall_16: 0.0255 - precision_16: 0.3590\n",
            "Epoch 119: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3022 - acc: 0.4279 - recall_16: 0.0279 - precision_16: 0.3800 - val_loss: 1.3022 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 120/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3077 - acc: 0.4200 - recall_16: 0.0182 - precision_16: 0.4000\n",
            "Epoch 120: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2997 - acc: 0.4250 - recall_16: 0.0176 - precision_16: 0.3636 - val_loss: 1.3020 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 121/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3067 - acc: 0.4191 - recall_16: 0.0265 - precision_16: 0.4000\n",
            "Epoch 121: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3067 - acc: 0.4191 - recall_16: 0.0265 - precision_16: 0.4000 - val_loss: 1.3019 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 122/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3042 - acc: 0.4182 - recall_16: 0.0218 - precision_16: 0.3636\n",
            "Epoch 122: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3099 - acc: 0.4176 - recall_16: 0.0176 - precision_16: 0.2857 - val_loss: 1.3020 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 123/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2901 - acc: 0.4364 - recall_16: 0.0255 - precision_16: 0.3784\n",
            "Epoch 123: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3015 - acc: 0.4235 - recall_16: 0.0221 - precision_16: 0.3261 - val_loss: 1.3022 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 124/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2890 - acc: 0.4267 - recall_16: 0.0283 - precision_16: 0.3696\n",
            "Epoch 124: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2934 - acc: 0.4221 - recall_16: 0.0265 - precision_16: 0.3750 - val_loss: 1.3021 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 125/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3035 - acc: 0.4255 - recall_16: 0.0109 - precision_16: 0.2143\n",
            "Epoch 125: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3104 - acc: 0.4279 - recall_16: 0.0162 - precision_16: 0.2683 - val_loss: 1.3023 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 126/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3155 - acc: 0.4120 - recall_16: 0.0140 - precision_16: 0.2692\n",
            "Epoch 126: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3075 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.2778 - val_loss: 1.3022 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 127/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3089 - acc: 0.4164 - recall_16: 0.0273 - precision_16: 0.5172\n",
            "Epoch 127: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3061 - acc: 0.4176 - recall_16: 0.0279 - precision_16: 0.5278 - val_loss: 1.3025 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 128/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3068 - acc: 0.4273 - recall_16: 0.0164 - precision_16: 0.3750\n",
            "Epoch 128: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3095 - acc: 0.4206 - recall_16: 0.0162 - precision_16: 0.3667 - val_loss: 1.3029 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 129/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3103 - acc: 0.3960 - recall_16: 0.0160 - precision_16: 0.3636\n",
            "Epoch 129: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3066 - acc: 0.4103 - recall_16: 0.0132 - precision_16: 0.3000 - val_loss: 1.3025 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 130/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3141 - acc: 0.4127 - recall_16: 0.0273 - precision_16: 0.4167\n",
            "Epoch 130: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3051 - acc: 0.4162 - recall_16: 0.0250 - precision_16: 0.3953 - val_loss: 1.3024 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 131/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2899 - acc: 0.4327 - recall_16: 0.0218 - precision_16: 0.4138\n",
            "Epoch 131: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3051 - acc: 0.4176 - recall_16: 0.0176 - precision_16: 0.3636 - val_loss: 1.3023 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 132/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3040 - acc: 0.4340 - recall_16: 0.0160 - precision_16: 0.3077\n",
            "Epoch 132: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3090 - acc: 0.4250 - recall_16: 0.0176 - precision_16: 0.3158 - val_loss: 1.3024 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 133/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3110 - acc: 0.4140 - recall_16: 0.0240 - precision_16: 0.5000\n",
            "Epoch 133: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3063 - acc: 0.4118 - recall_16: 0.0235 - precision_16: 0.4706 - val_loss: 1.3021 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 134/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3042 - acc: 0.4176 - recall_16: 0.0147 - precision_16: 0.2703\n",
            "Epoch 134: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3042 - acc: 0.4176 - recall_16: 0.0147 - precision_16: 0.2703 - val_loss: 1.3022 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 135/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2857 - acc: 0.4260 - recall_16: 0.0160 - precision_16: 0.3478\n",
            "Epoch 135: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2968 - acc: 0.4221 - recall_16: 0.0176 - precision_16: 0.3636 - val_loss: 1.3022 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 136/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3181 - acc: 0.3927 - recall_16: 0.0182 - precision_16: 0.5000\n",
            "Epoch 136: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2985 - acc: 0.4162 - recall_16: 0.0191 - precision_16: 0.5200 - val_loss: 1.3021 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 137/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3060 - acc: 0.4255 - recall_16: 0.0255 - precision_16: 0.4828\n",
            "Epoch 137: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3046 - acc: 0.4221 - recall_16: 0.0265 - precision_16: 0.4390 - val_loss: 1.3021 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 138/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3192 - acc: 0.4140 - recall_16: 0.0140 - precision_16: 0.3500\n",
            "Epoch 138: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3118 - acc: 0.4176 - recall_16: 0.0132 - precision_16: 0.3214 - val_loss: 1.3022 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 139/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3107 - acc: 0.3980 - recall_16: 0.0120 - precision_16: 0.3158\n",
            "Epoch 139: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3013 - acc: 0.4176 - recall_16: 0.0118 - precision_16: 0.2857 - val_loss: 1.3017 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 140/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2941 - acc: 0.4320 - recall_16: 0.0200 - precision_16: 0.3333\n",
            "Epoch 140: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3009 - acc: 0.4235 - recall_16: 0.0250 - precision_16: 0.4048 - val_loss: 1.3019 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 141/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2946 - acc: 0.4265 - recall_16: 0.0294 - precision_16: 0.4651\n",
            "Epoch 141: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2946 - acc: 0.4265 - recall_16: 0.0294 - precision_16: 0.4651 - val_loss: 1.3018 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 142/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3049 - acc: 0.4164 - recall_16: 0.0200 - precision_16: 0.2821\n",
            "Epoch 142: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2977 - acc: 0.4265 - recall_16: 0.0221 - precision_16: 0.3333 - val_loss: 1.3017 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 143/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3029 - acc: 0.4160 - recall_16: 0.0160 - precision_16: 0.2581\n",
            "Epoch 143: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3029 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.2564 - val_loss: 1.3020 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 144/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3036 - acc: 0.4200 - recall_16: 0.0236 - precision_16: 0.4333\n",
            "Epoch 144: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3034 - acc: 0.4206 - recall_16: 0.0221 - precision_16: 0.4167 - val_loss: 1.3020 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 145/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2966 - acc: 0.4176 - recall_16: 0.0221 - precision_16: 0.3846\n",
            "Epoch 145: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2966 - acc: 0.4176 - recall_16: 0.0221 - precision_16: 0.3846 - val_loss: 1.3022 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 146/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3036 - acc: 0.4300 - recall_16: 0.0120 - precision_16: 0.2727\n",
            "Epoch 146: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3109 - acc: 0.4191 - recall_16: 0.0162 - precision_16: 0.3235 - val_loss: 1.3024 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 147/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2941 - acc: 0.4236 - recall_16: 0.0291 - precision_16: 0.5714\n",
            "Epoch 147: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2989 - acc: 0.4206 - recall_16: 0.0279 - precision_16: 0.5135 - val_loss: 1.3023 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 148/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3093 - acc: 0.4091 - recall_16: 0.0182 - precision_16: 0.3704\n",
            "Epoch 148: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2968 - acc: 0.4191 - recall_16: 0.0191 - precision_16: 0.4333 - val_loss: 1.3020 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 149/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2956 - acc: 0.4255 - recall_16: 0.0309 - precision_16: 0.5152\n",
            "Epoch 149: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2890 - acc: 0.4265 - recall_16: 0.0309 - precision_16: 0.5122 - val_loss: 1.3018 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 150/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2991 - acc: 0.4255 - recall_16: 0.0200 - precision_16: 0.4074\n",
            "Epoch 150: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3066 - acc: 0.4191 - recall_16: 0.0162 - precision_16: 0.3056 - val_loss: 1.3022 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 151/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3083 - acc: 0.4191 - recall_16: 0.0221 - precision_16: 0.4167\n",
            "Epoch 151: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3083 - acc: 0.4191 - recall_16: 0.0221 - precision_16: 0.4167 - val_loss: 1.3023 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 152/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3131 - acc: 0.4145 - recall_16: 0.0182 - precision_16: 0.3571\n",
            "Epoch 152: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3006 - acc: 0.4221 - recall_16: 0.0176 - precision_16: 0.3429 - val_loss: 1.3026 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 153/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2996 - acc: 0.4221 - recall_16: 0.0235 - precision_16: 0.4103\n",
            "Epoch 153: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2996 - acc: 0.4221 - recall_16: 0.0235 - precision_16: 0.4103 - val_loss: 1.3027 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 154/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2992 - acc: 0.4221 - recall_16: 0.0176 - precision_16: 0.3636\n",
            "Epoch 154: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2992 - acc: 0.4221 - recall_16: 0.0176 - precision_16: 0.3636 - val_loss: 1.3027 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 155/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3021 - acc: 0.4280 - recall_16: 0.0140 - precision_16: 0.4667\n",
            "Epoch 155: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3047 - acc: 0.4176 - recall_16: 0.0147 - precision_16: 0.4545 - val_loss: 1.3027 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 156/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2903 - acc: 0.4320 - recall_16: 0.0320 - precision_16: 0.6400\n",
            "Epoch 156: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2944 - acc: 0.4221 - recall_16: 0.0324 - precision_16: 0.6471 - val_loss: 1.3026 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 157/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2961 - acc: 0.4160 - recall_16: 0.0140 - precision_16: 0.3043\n",
            "Epoch 157: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3023 - acc: 0.4132 - recall_16: 0.0206 - precision_16: 0.4000 - val_loss: 1.3024 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 158/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2978 - acc: 0.4250 - recall_16: 0.0147 - precision_16: 0.2941\n",
            "Epoch 158: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2978 - acc: 0.4250 - recall_16: 0.0147 - precision_16: 0.2941 - val_loss: 1.3021 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 159/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3044 - acc: 0.4236 - recall_16: 0.0218 - precision_16: 0.3243\n",
            "Epoch 159: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.3016 - acc: 0.4191 - recall_16: 0.0206 - precision_16: 0.3111 - val_loss: 1.3026 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 160/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2945 - acc: 0.4160 - recall_16: 0.0220 - precision_16: 0.4400\n",
            "Epoch 160: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3019 - acc: 0.4176 - recall_16: 0.0176 - precision_16: 0.3636 - val_loss: 1.3026 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 161/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3156 - acc: 0.4255 - recall_16: 0.0200 - precision_16: 0.3333\n",
            "Epoch 161: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3117 - acc: 0.4191 - recall_16: 0.0206 - precision_16: 0.3500 - val_loss: 1.3030 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 162/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2998 - acc: 0.4236 - recall_16: 0.0273 - precision_16: 0.5172\n",
            "Epoch 162: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3046 - acc: 0.4235 - recall_16: 0.0265 - precision_16: 0.5143 - val_loss: 1.3032 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 163/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2895 - acc: 0.4320 - recall_16: 0.0240 - precision_16: 0.5455\n",
            "Epoch 163: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3036 - acc: 0.4206 - recall_16: 0.0235 - precision_16: 0.4706 - val_loss: 1.3032 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 164/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3052 - acc: 0.4145 - recall_16: 0.0182 - precision_16: 0.4545\n",
            "Epoch 164: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3007 - acc: 0.4176 - recall_16: 0.0176 - precision_16: 0.4138 - val_loss: 1.3031 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 165/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3062 - acc: 0.4055 - recall_16: 0.0091 - precision_16: 0.3333\n",
            "Epoch 165: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 13ms/step - loss: 1.2943 - acc: 0.4162 - recall_16: 0.0147 - precision_16: 0.4167 - val_loss: 1.3030 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 166/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3103 - acc: 0.4200 - recall_16: 0.0220 - precision_16: 0.4400\n",
            "Epoch 166: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2972 - acc: 0.4221 - recall_16: 0.0250 - precision_16: 0.4595 - val_loss: 1.3030 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 167/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2971 - acc: 0.4182 - recall_16: 0.0200 - precision_16: 0.4583\n",
            "Epoch 167: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2951 - acc: 0.4250 - recall_16: 0.0176 - precision_16: 0.4615 - val_loss: 1.3032 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 168/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3074 - acc: 0.4127 - recall_16: 0.0291 - precision_16: 0.4706\n",
            "Epoch 168: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2937 - acc: 0.4279 - recall_16: 0.0250 - precision_16: 0.4722 - val_loss: 1.3031 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 169/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3008 - acc: 0.4262 - recall_16: 0.0169 - precision_16: 0.3793\n",
            "Epoch 169: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3015 - acc: 0.4235 - recall_16: 0.0162 - precision_16: 0.3667 - val_loss: 1.3033 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 170/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3029 - acc: 0.4191 - recall_16: 0.0162 - precision_16: 0.3333\n",
            "Epoch 170: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3029 - acc: 0.4191 - recall_16: 0.0162 - precision_16: 0.3333 - val_loss: 1.3036 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 171/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3028 - acc: 0.4191 - recall_16: 0.0118 - precision_16: 0.3077\n",
            "Epoch 171: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3028 - acc: 0.4191 - recall_16: 0.0118 - precision_16: 0.3077 - val_loss: 1.3035 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 172/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3049 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.3182\n",
            "Epoch 172: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3049 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.3182 - val_loss: 1.3035 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 173/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3049 - acc: 0.4176 - recall_16: 0.0132 - precision_16: 0.3462\n",
            "Epoch 173: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3049 - acc: 0.4176 - recall_16: 0.0132 - precision_16: 0.3462 - val_loss: 1.3036 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 174/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3013 - acc: 0.4215 - recall_16: 0.0215 - precision_16: 0.4667\n",
            "Epoch 174: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3049 - acc: 0.4221 - recall_16: 0.0206 - precision_16: 0.4516 - val_loss: 1.3037 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 175/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3094 - acc: 0.4185 - recall_16: 0.0092 - precision_16: 0.2222\n",
            "Epoch 175: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3112 - acc: 0.4191 - recall_16: 0.0088 - precision_16: 0.2222 - val_loss: 1.3039 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 176/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3099 - acc: 0.4145 - recall_16: 0.0327 - precision_16: 0.6667\n",
            "Epoch 176: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2997 - acc: 0.4250 - recall_16: 0.0294 - precision_16: 0.5882 - val_loss: 1.3037 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 177/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2853 - acc: 0.4360 - recall_16: 0.0220 - precision_16: 0.5789\n",
            "Epoch 177: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2957 - acc: 0.4235 - recall_16: 0.0191 - precision_16: 0.5000 - val_loss: 1.3038 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 178/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3032 - acc: 0.4120 - recall_16: 0.0380 - precision_16: 0.5278\n",
            "Epoch 178: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3044 - acc: 0.4191 - recall_16: 0.0294 - precision_16: 0.4878 - val_loss: 1.3041 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 179/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3082 - acc: 0.4080 - recall_16: 0.0140 - precision_16: 0.3182\n",
            "Epoch 179: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3015 - acc: 0.4235 - recall_16: 0.0206 - precision_16: 0.4242 - val_loss: 1.3044 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 180/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2924 - acc: 0.4200 - recall_16: 0.0262 - precision_16: 0.5152\n",
            "Epoch 180: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2966 - acc: 0.4176 - recall_16: 0.0265 - precision_16: 0.5143 - val_loss: 1.3045 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 181/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3019 - acc: 0.4169 - recall_16: 0.0169 - precision_16: 0.3929\n",
            "Epoch 181: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3005 - acc: 0.4176 - recall_16: 0.0176 - precision_16: 0.4138 - val_loss: 1.3044 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 182/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2829 - acc: 0.4400 - recall_16: 0.0160 - precision_16: 0.4706\n",
            "Epoch 182: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3046 - acc: 0.4206 - recall_16: 0.0176 - precision_16: 0.4286 - val_loss: 1.3046 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 183/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3051 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.3448\n",
            "Epoch 183: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3051 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.3448 - val_loss: 1.3045 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 184/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2948 - acc: 0.4236 - recall_16: 0.0127 - precision_16: 0.3684\n",
            "Epoch 184: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3062 - acc: 0.4162 - recall_16: 0.0132 - precision_16: 0.3333 - val_loss: 1.3047 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 185/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3165 - acc: 0.4140 - recall_16: 0.0180 - precision_16: 0.4737\n",
            "Epoch 185: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3136 - acc: 0.4221 - recall_16: 0.0132 - precision_16: 0.3750 - val_loss: 1.3046 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 186/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2812 - acc: 0.4340 - recall_16: 0.0220 - precision_16: 0.3929\n",
            "Epoch 186: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2946 - acc: 0.4206 - recall_16: 0.0206 - precision_16: 0.3889 - val_loss: 1.3048 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 187/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2953 - acc: 0.4300 - recall_16: 0.0120 - precision_16: 0.3750\n",
            "Epoch 187: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3033 - acc: 0.4250 - recall_16: 0.0088 - precision_16: 0.3000 - val_loss: 1.3045 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 188/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3013 - acc: 0.4250 - recall_16: 0.0176 - precision_16: 0.4000\n",
            "Epoch 188: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3013 - acc: 0.4250 - recall_16: 0.0176 - precision_16: 0.4000 - val_loss: 1.3044 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 189/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3069 - acc: 0.4236 - recall_16: 0.0164 - precision_16: 0.3913\n",
            "Epoch 189: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3054 - acc: 0.4206 - recall_16: 0.0147 - precision_16: 0.3571 - val_loss: 1.3046 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 190/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2943 - acc: 0.4191 - recall_16: 0.0176 - precision_16: 0.4444\n",
            "Epoch 190: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2943 - acc: 0.4191 - recall_16: 0.0176 - precision_16: 0.4444 - val_loss: 1.3045 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 191/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2928 - acc: 0.4380 - recall_16: 0.0240 - precision_16: 0.6667\n",
            "Epoch 191: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2983 - acc: 0.4221 - recall_16: 0.0221 - precision_16: 0.5556 - val_loss: 1.3045 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 192/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3127 - acc: 0.4080 - recall_16: 0.0220 - precision_16: 0.5000\n",
            "Epoch 192: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3000 - acc: 0.4221 - recall_16: 0.0206 - precision_16: 0.4516 - val_loss: 1.3047 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 193/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2938 - acc: 0.4255 - recall_16: 0.0182 - precision_16: 0.4545\n",
            "Epoch 193: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2967 - acc: 0.4176 - recall_16: 0.0176 - precision_16: 0.4444 - val_loss: 1.3048 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 194/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3050 - acc: 0.4280 - recall_16: 0.0160 - precision_16: 0.3636\n",
            "Epoch 194: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3000 - acc: 0.4191 - recall_16: 0.0147 - precision_16: 0.3448 - val_loss: 1.3050 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 195/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2964 - acc: 0.4235 - recall_16: 0.0191 - precision_16: 0.5000\n",
            "Epoch 195: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2964 - acc: 0.4235 - recall_16: 0.0191 - precision_16: 0.5000 - val_loss: 1.3048 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 196/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2863 - acc: 0.4262 - recall_16: 0.0169 - precision_16: 0.6471\n",
            "Epoch 196: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2936 - acc: 0.4191 - recall_16: 0.0176 - precision_16: 0.6316 - val_loss: 1.3047 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 197/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2995 - acc: 0.4250 - recall_16: 0.0162 - precision_16: 0.3793\n",
            "Epoch 197: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2995 - acc: 0.4250 - recall_16: 0.0162 - precision_16: 0.3793 - val_loss: 1.3047 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 198/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2894 - acc: 0.4300 - recall_16: 0.0220 - precision_16: 0.5238\n",
            "Epoch 198: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3008 - acc: 0.4206 - recall_16: 0.0221 - precision_16: 0.4688 - val_loss: 1.3050 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 199/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3005 - acc: 0.4200 - recall_16: 0.0200 - precision_16: 0.6471\n",
            "Epoch 199: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3019 - acc: 0.4176 - recall_16: 0.0162 - precision_16: 0.6471 - val_loss: 1.3052 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 200/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3041 - acc: 0.4320 - recall_16: 0.0180 - precision_16: 0.4500\n",
            "Epoch 200: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3056 - acc: 0.4191 - recall_16: 0.0162 - precision_16: 0.4783 - val_loss: 1.3052 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 201/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3040 - acc: 0.4260 - recall_16: 0.0180 - precision_16: 0.3600\n",
            "Epoch 201: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3075 - acc: 0.4221 - recall_16: 0.0176 - precision_16: 0.3429 - val_loss: 1.3054 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 202/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2825 - acc: 0.4364 - recall_16: 0.0109 - precision_16: 0.3158\n",
            "Epoch 202: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2973 - acc: 0.4206 - recall_16: 0.0118 - precision_16: 0.3077 - val_loss: 1.3054 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 203/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2932 - acc: 0.4191 - recall_16: 0.0250 - precision_16: 0.6296\n",
            "Epoch 203: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2932 - acc: 0.4191 - recall_16: 0.0250 - precision_16: 0.6296 - val_loss: 1.3051 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 204/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3004 - acc: 0.4250 - recall_16: 0.0176 - precision_16: 0.3871\n",
            "Epoch 204: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3004 - acc: 0.4250 - recall_16: 0.0176 - precision_16: 0.3871 - val_loss: 1.3051 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 205/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2945 - acc: 0.4200 - recall_16: 0.0123 - precision_16: 0.3200\n",
            "Epoch 205: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2908 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.3571 - val_loss: 1.3051 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 206/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3058 - acc: 0.4260 - recall_16: 0.0120 - precision_16: 0.4000\n",
            "Epoch 206: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3068 - acc: 0.4191 - recall_16: 0.0118 - precision_16: 0.4211 - val_loss: 1.3052 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 207/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2992 - acc: 0.4206 - recall_16: 0.0221 - precision_16: 0.3846\n",
            "Epoch 207: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2992 - acc: 0.4206 - recall_16: 0.0221 - precision_16: 0.3846 - val_loss: 1.3056 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 208/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2719 - acc: 0.4440 - recall_16: 0.0080 - precision_16: 0.3636\n",
            "Epoch 208: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2960 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.3500 - val_loss: 1.3056 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 209/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3018 - acc: 0.4262 - recall_16: 0.0200 - precision_16: 0.4194\n",
            "Epoch 209: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3053 - acc: 0.4221 - recall_16: 0.0191 - precision_16: 0.3939 - val_loss: 1.3057 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 210/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2931 - acc: 0.4364 - recall_16: 0.0200 - precision_16: 0.5000\n",
            "Epoch 210: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3014 - acc: 0.4206 - recall_16: 0.0162 - precision_16: 0.4231 - val_loss: 1.3057 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 211/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3027 - acc: 0.4221 - recall_16: 0.0118 - precision_16: 0.3333\n",
            "Epoch 211: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3027 - acc: 0.4221 - recall_16: 0.0118 - precision_16: 0.3333 - val_loss: 1.3057 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 212/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3021 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.2917\n",
            "Epoch 212: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3021 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.2917 - val_loss: 1.3057 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 213/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3159 - acc: 0.4140 - recall_16: 0.0060 - precision_16: 0.2727\n",
            "Epoch 213: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3027 - acc: 0.4191 - recall_16: 0.0103 - precision_16: 0.3684 - val_loss: 1.3056 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 214/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2972 - acc: 0.4246 - recall_16: 0.0123 - precision_16: 0.3478\n",
            "Epoch 214: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3001 - acc: 0.4250 - recall_16: 0.0118 - precision_16: 0.3478 - val_loss: 1.3057 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 215/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2994 - acc: 0.4235 - recall_16: 0.0176 - precision_16: 0.3750\n",
            "Epoch 215: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2994 - acc: 0.4235 - recall_16: 0.0176 - precision_16: 0.3750 - val_loss: 1.3056 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 216/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2991 - acc: 0.4191 - recall_16: 0.0103 - precision_16: 0.3182\n",
            "Epoch 216: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2991 - acc: 0.4191 - recall_16: 0.0103 - precision_16: 0.3182 - val_loss: 1.3057 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 217/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2980 - acc: 0.4206 - recall_16: 0.0176 - precision_16: 0.4615\n",
            "Epoch 217: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2980 - acc: 0.4206 - recall_16: 0.0176 - precision_16: 0.4615 - val_loss: 1.3059 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 218/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3068 - acc: 0.4055 - recall_16: 0.0109 - precision_16: 0.4286\n",
            "Epoch 218: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3000 - acc: 0.4176 - recall_16: 0.0088 - precision_16: 0.3750 - val_loss: 1.3055 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 219/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2947 - acc: 0.4235 - recall_16: 0.0221 - precision_16: 0.4412\n",
            "Epoch 219: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2947 - acc: 0.4235 - recall_16: 0.0221 - precision_16: 0.4412 - val_loss: 1.3055 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 220/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3072 - acc: 0.4162 - recall_16: 0.0132 - precision_16: 0.4091\n",
            "Epoch 220: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3072 - acc: 0.4162 - recall_16: 0.0132 - precision_16: 0.4091 - val_loss: 1.3056 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 221/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2958 - acc: 0.4221 - recall_16: 0.0206 - precision_16: 0.5385\n",
            "Epoch 221: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2958 - acc: 0.4221 - recall_16: 0.0206 - precision_16: 0.5385 - val_loss: 1.3055 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 222/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2928 - acc: 0.4360 - recall_16: 0.0240 - precision_16: 0.6000\n",
            "Epoch 222: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2971 - acc: 0.4206 - recall_16: 0.0221 - precision_16: 0.5556 - val_loss: 1.3055 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 223/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2838 - acc: 0.4292 - recall_16: 0.0154 - precision_16: 0.4762\n",
            "Epoch 223: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2926 - acc: 0.4191 - recall_16: 0.0147 - precision_16: 0.4545 - val_loss: 1.3057 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 224/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2970 - acc: 0.4185 - recall_16: 0.0200 - precision_16: 0.4815\n",
            "Epoch 224: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2928 - acc: 0.4221 - recall_16: 0.0206 - precision_16: 0.4828 - val_loss: 1.3054 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 225/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3035 - acc: 0.4215 - recall_16: 0.0123 - precision_16: 0.4211\n",
            "Epoch 225: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3042 - acc: 0.4221 - recall_16: 0.0118 - precision_16: 0.4000 - val_loss: 1.3055 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 226/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3063 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.4000\n",
            "Epoch 226: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3063 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.4000 - val_loss: 1.3058 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 227/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3060 - acc: 0.4180 - recall_16: 0.0140 - precision_16: 0.3500\n",
            "Epoch 227: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3003 - acc: 0.4191 - recall_16: 0.0147 - precision_16: 0.3846 - val_loss: 1.3058 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 228/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2978 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.3704\n",
            "Epoch 228: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2978 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.3704 - val_loss: 1.3059 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 229/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3055 - acc: 0.4154 - recall_16: 0.0062 - precision_16: 0.1905\n",
            "Epoch 229: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3033 - acc: 0.4206 - recall_16: 0.0059 - precision_16: 0.1818 - val_loss: 1.3058 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 230/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3008 - acc: 0.4191 - recall_16: 0.0147 - precision_16: 0.4167\n",
            "Epoch 230: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3008 - acc: 0.4191 - recall_16: 0.0147 - precision_16: 0.4167 - val_loss: 1.3061 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 231/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2885 - acc: 0.4420 - recall_16: 0.0160 - precision_16: 0.5333\n",
            "Epoch 231: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3048 - acc: 0.4235 - recall_16: 0.0176 - precision_16: 0.4800 - val_loss: 1.3064 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 232/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3006 - acc: 0.4292 - recall_16: 0.0123 - precision_16: 0.3636\n",
            "Epoch 232: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3049 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.3750 - val_loss: 1.3066 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 233/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3149 - acc: 0.4123 - recall_16: 0.0062 - precision_16: 0.2222\n",
            "Epoch 233: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3084 - acc: 0.4206 - recall_16: 0.0059 - precision_16: 0.2105 - val_loss: 1.3066 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 234/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2957 - acc: 0.4250 - recall_16: 0.0059 - precision_16: 0.2667\n",
            "Epoch 234: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2957 - acc: 0.4250 - recall_16: 0.0059 - precision_16: 0.2667 - val_loss: 1.3064 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 235/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2921 - acc: 0.4215 - recall_16: 0.0108 - precision_16: 0.4118\n",
            "Epoch 235: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2911 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.3889 - val_loss: 1.3064 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 236/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2572 - acc: 0.4480 - recall_16: 0.0180 - precision_16: 0.6000\n",
            "Epoch 236: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2900 - acc: 0.4265 - recall_16: 0.0176 - precision_16: 0.5217 - val_loss: 1.3064 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 237/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3011 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.2759\n",
            "Epoch 237: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3011 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.2759 - val_loss: 1.3065 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 238/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2933 - acc: 0.4360 - recall_16: 0.0200 - precision_16: 0.5882\n",
            "Epoch 238: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2994 - acc: 0.4235 - recall_16: 0.0162 - precision_16: 0.5000 - val_loss: 1.3066 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 239/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2892 - acc: 0.4308 - recall_16: 0.0138 - precision_16: 0.3750\n",
            "Epoch 239: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2944 - acc: 0.4221 - recall_16: 0.0132 - precision_16: 0.3600 - val_loss: 1.3068 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 240/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2981 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.4545\n",
            "Epoch 240: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2981 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.4545 - val_loss: 1.3069 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 241/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2908 - acc: 0.4220 - recall_16: 0.0160 - precision_16: 0.4706\n",
            "Epoch 241: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2954 - acc: 0.4265 - recall_16: 0.0162 - precision_16: 0.4783 - val_loss: 1.3064 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 242/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2917 - acc: 0.4206 - recall_16: 0.0088 - precision_16: 0.3333\n",
            "Epoch 242: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2917 - acc: 0.4206 - recall_16: 0.0088 - precision_16: 0.3333 - val_loss: 1.3065 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 243/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2974 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.3333\n",
            "Epoch 243: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2974 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.3333 - val_loss: 1.3066 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 244/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2922 - acc: 0.4360 - recall_16: 0.0060 - precision_16: 0.3000\n",
            "Epoch 244: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2978 - acc: 0.4250 - recall_16: 0.0118 - precision_16: 0.4706 - val_loss: 1.3067 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 245/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3101 - acc: 0.4120 - recall_16: 0.0140 - precision_16: 0.3500\n",
            "Epoch 245: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3090 - acc: 0.4191 - recall_16: 0.0147 - precision_16: 0.3704 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 246/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2971 - acc: 0.4277 - recall_16: 0.0092 - precision_16: 0.3333\n",
            "Epoch 246: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2959 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.3333 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 247/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3152 - acc: 0.4080 - recall_16: 0.0120 - precision_16: 0.2500\n",
            "Epoch 247: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3041 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.2800 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 248/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3080 - acc: 0.4200 - recall_16: 0.0036 - precision_16: 0.1818\n",
            "Epoch 248: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3013 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.2222 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 249/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3005 - acc: 0.4206 - recall_16: 0.0103 - precision_16: 0.4118\n",
            "Epoch 249: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3005 - acc: 0.4206 - recall_16: 0.0103 - precision_16: 0.4118 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 250/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3002 - acc: 0.4140 - recall_16: 0.0140 - precision_16: 0.4667\n",
            "Epoch 250: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2967 - acc: 0.4206 - recall_16: 0.0162 - precision_16: 0.5500 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 251/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3017 - acc: 0.4250 - recall_16: 0.0103 - precision_16: 0.3889\n",
            "Epoch 251: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3017 - acc: 0.4250 - recall_16: 0.0103 - precision_16: 0.3889 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 252/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3128 - acc: 0.4138 - recall_16: 0.0108 - precision_16: 0.3684\n",
            "Epoch 252: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3111 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.3500 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 253/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2958 - acc: 0.4235 - recall_16: 0.0191 - precision_16: 0.5652\n",
            "Epoch 253: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2958 - acc: 0.4235 - recall_16: 0.0191 - precision_16: 0.5652 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 254/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2917 - acc: 0.4262 - recall_16: 0.0077 - precision_16: 0.3125\n",
            "Epoch 254: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2986 - acc: 0.4191 - recall_16: 0.0088 - precision_16: 0.3529 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 255/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3018 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.2381\n",
            "Epoch 255: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3018 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.2381 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 256/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3016 - acc: 0.4206 - recall_16: 0.0074 - precision_16: 0.2941\n",
            "Epoch 256: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3016 - acc: 0.4206 - recall_16: 0.0074 - precision_16: 0.2941 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 257/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2882 - acc: 0.4262 - recall_16: 0.0169 - precision_16: 0.5500\n",
            "Epoch 257: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2921 - acc: 0.4221 - recall_16: 0.0162 - precision_16: 0.5500 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 258/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2990 - acc: 0.4250 - recall_16: 0.0103 - precision_16: 0.3043\n",
            "Epoch 258: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2990 - acc: 0.4250 - recall_16: 0.0103 - precision_16: 0.3043 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 259/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2959 - acc: 0.4262 - recall_16: 0.0123 - precision_16: 0.4444\n",
            "Epoch 259: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2968 - acc: 0.4250 - recall_16: 0.0118 - precision_16: 0.4444 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 260/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2979 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.4444\n",
            "Epoch 260: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2979 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.4444 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 261/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3010 - acc: 0.4220 - recall_16: 0.0080 - precision_16: 0.2105\n",
            "Epoch 261: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3090 - acc: 0.4250 - recall_16: 0.0103 - precision_16: 0.2917 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 262/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2930 - acc: 0.4320 - recall_16: 0.0060 - precision_16: 0.2727\n",
            "Epoch 262: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3000 - acc: 0.4221 - recall_16: 0.0059 - precision_16: 0.2353 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 263/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3052 - acc: 0.4040 - recall_16: 0.0120 - precision_16: 0.2727\n",
            "Epoch 263: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2958 - acc: 0.4191 - recall_16: 0.0147 - precision_16: 0.3448 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 264/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2980 - acc: 0.4176 - recall_16: 0.0088 - precision_16: 0.3000\n",
            "Epoch 264: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2980 - acc: 0.4176 - recall_16: 0.0088 - precision_16: 0.3000 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 265/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2970 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.4286\n",
            "Epoch 265: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2970 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.4286 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 266/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3015 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.3529\n",
            "Epoch 266: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3015 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.3529 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 267/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2961 - acc: 0.4231 - recall_16: 0.0092 - precision_16: 0.4615\n",
            "Epoch 267: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2957 - acc: 0.4250 - recall_16: 0.0088 - precision_16: 0.4615 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 268/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3036 - acc: 0.4206 - recall_16: 0.0074 - precision_16: 0.2174\n",
            "Epoch 268: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3036 - acc: 0.4206 - recall_16: 0.0074 - precision_16: 0.2174 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 269/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2874 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.3636\n",
            "Epoch 269: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2874 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.3636 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 270/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2895 - acc: 0.4420 - recall_16: 0.0180 - precision_16: 0.5625\n",
            "Epoch 270: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3002 - acc: 0.4265 - recall_16: 0.0147 - precision_16: 0.4762 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 271/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2986 - acc: 0.4215 - recall_16: 0.0077 - precision_16: 0.2632\n",
            "Epoch 271: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3051 - acc: 0.4176 - recall_16: 0.0074 - precision_16: 0.2632 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 272/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3013 - acc: 0.4280 - recall_16: 0.0100 - precision_16: 0.2778\n",
            "Epoch 272: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2978 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.2500 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 273/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3045 - acc: 0.4221 - recall_16: 0.0059 - precision_16: 0.2353\n",
            "Epoch 273: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3045 - acc: 0.4221 - recall_16: 0.0059 - precision_16: 0.2353 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 274/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2998 - acc: 0.4215 - recall_16: 0.0077 - precision_16: 0.2500\n",
            "Epoch 274: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2994 - acc: 0.4206 - recall_16: 0.0074 - precision_16: 0.2500 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 275/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2914 - acc: 0.4250 - recall_16: 0.0088 - precision_16: 0.3333\n",
            "Epoch 275: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2914 - acc: 0.4250 - recall_16: 0.0088 - precision_16: 0.3333 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 276/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2964 - acc: 0.4138 - recall_16: 0.0108 - precision_16: 0.3500\n",
            "Epoch 276: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2910 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.3500 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 277/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3041 - acc: 0.4231 - recall_16: 0.0092 - precision_16: 0.5000\n",
            "Epoch 277: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3034 - acc: 0.4221 - recall_16: 0.0088 - precision_16: 0.5000 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 278/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3099 - acc: 0.4109 - recall_16: 0.0055 - precision_16: 0.2143\n",
            "Epoch 278: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3030 - acc: 0.4250 - recall_16: 0.0059 - precision_16: 0.2667 - val_loss: 1.3068 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 279/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2931 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.4348\n",
            "Epoch 279: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2931 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.4348 - val_loss: 1.3066 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 280/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2974 - acc: 0.4206 - recall_16: 0.0088 - precision_16: 0.3333\n",
            "Epoch 280: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2974 - acc: 0.4206 - recall_16: 0.0088 - precision_16: 0.3333 - val_loss: 1.3066 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 281/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2892 - acc: 0.4235 - recall_16: 0.0176 - precision_16: 0.4444\n",
            "Epoch 281: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2892 - acc: 0.4235 - recall_16: 0.0176 - precision_16: 0.4444 - val_loss: 1.3067 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 282/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2943 - acc: 0.4221 - recall_16: 0.0118 - precision_16: 0.5000\n",
            "Epoch 282: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2943 - acc: 0.4221 - recall_16: 0.0118 - precision_16: 0.5000 - val_loss: 1.3066 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 283/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3014 - acc: 0.4235 - recall_16: 0.0176 - precision_16: 0.4286\n",
            "Epoch 283: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3014 - acc: 0.4235 - recall_16: 0.0176 - precision_16: 0.4286 - val_loss: 1.3066 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 284/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3081 - acc: 0.4100 - recall_16: 0.0060 - precision_16: 0.1667\n",
            "Epoch 284: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3026 - acc: 0.4191 - recall_16: 0.0074 - precision_16: 0.2174 - val_loss: 1.3069 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 285/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2943 - acc: 0.4206 - recall_16: 0.0074 - precision_16: 0.3333\n",
            "Epoch 285: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2943 - acc: 0.4206 - recall_16: 0.0074 - precision_16: 0.3333 - val_loss: 1.3069 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 286/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2817 - acc: 0.4308 - recall_16: 0.0215 - precision_16: 0.5000\n",
            "Epoch 286: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2916 - acc: 0.4191 - recall_16: 0.0206 - precision_16: 0.5000 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 287/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2968 - acc: 0.4250 - recall_16: 0.0118 - precision_16: 0.4706\n",
            "Epoch 287: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2968 - acc: 0.4250 - recall_16: 0.0118 - precision_16: 0.4706 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 288/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2956 - acc: 0.4236 - recall_16: 0.0073 - precision_16: 0.2222\n",
            "Epoch 288: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2917 - acc: 0.4250 - recall_16: 0.0088 - precision_16: 0.2727 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 289/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2934 - acc: 0.4246 - recall_16: 0.0169 - precision_16: 0.5500\n",
            "Epoch 289: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2949 - acc: 0.4235 - recall_16: 0.0162 - precision_16: 0.5238 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 290/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3014 - acc: 0.4206 - recall_16: 0.0088 - precision_16: 0.3000\n",
            "Epoch 290: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3014 - acc: 0.4206 - recall_16: 0.0088 - precision_16: 0.3000 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 291/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2909 - acc: 0.4277 - recall_16: 0.0200 - precision_16: 0.4643\n",
            "Epoch 291: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2929 - acc: 0.4250 - recall_16: 0.0206 - precision_16: 0.4828 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 292/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2988 - acc: 0.4191 - recall_16: 0.0176 - precision_16: 0.5217\n",
            "Epoch 292: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2988 - acc: 0.4191 - recall_16: 0.0176 - precision_16: 0.5217 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 293/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2990 - acc: 0.4277 - recall_16: 0.0138 - precision_16: 0.4737\n",
            "Epoch 293: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3017 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.4500 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 294/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2973 - acc: 0.4250 - recall_16: 0.0176 - precision_16: 0.4800\n",
            "Epoch 294: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2973 - acc: 0.4250 - recall_16: 0.0176 - precision_16: 0.4800 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 295/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.3043 - acc: 0.4145 - recall_16: 0.0145 - precision_16: 0.5333\n",
            "Epoch 295: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2938 - acc: 0.4250 - recall_16: 0.0162 - precision_16: 0.5789 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 296/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3026 - acc: 0.4262 - recall_16: 0.0138 - precision_16: 0.5625\n",
            "Epoch 296: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3054 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.5294 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 297/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2983 - acc: 0.4250 - recall_16: 0.0074 - precision_16: 0.3125\n",
            "Epoch 297: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2983 - acc: 0.4250 - recall_16: 0.0074 - precision_16: 0.3125 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 298/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3051 - acc: 0.4200 - recall_16: 0.0077 - precision_16: 0.3571\n",
            "Epoch 298: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3020 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.3571 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 299/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3007 - acc: 0.4308 - recall_16: 0.0123 - precision_16: 0.3636\n",
            "Epoch 299: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3028 - acc: 0.4265 - recall_16: 0.0118 - precision_16: 0.3478 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 300/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3056 - acc: 0.4160 - recall_16: 0.0140 - precision_16: 0.4667\n",
            "Epoch 300: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2979 - acc: 0.4235 - recall_16: 0.0162 - precision_16: 0.4583 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 301/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3056 - acc: 0.4100 - recall_16: 0.0140 - precision_16: 0.5833\n",
            "Epoch 301: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2970 - acc: 0.4221 - recall_16: 0.0162 - precision_16: 0.6471 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 302/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2931 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4667\n",
            "Epoch 302: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2931 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4667 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 303/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3035 - acc: 0.4191 - recall_16: 0.0147 - precision_16: 0.5000\n",
            "Epoch 303: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3035 - acc: 0.4191 - recall_16: 0.0147 - precision_16: 0.5000 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 304/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3013 - acc: 0.4277 - recall_16: 0.0046 - precision_16: 0.2500\n",
            "Epoch 304: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3031 - acc: 0.4265 - recall_16: 0.0044 - precision_16: 0.2143 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 305/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2834 - acc: 0.4320 - recall_16: 0.0140 - precision_16: 0.3182\n",
            "Epoch 305: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.3000 - acc: 0.4250 - recall_16: 0.0103 - precision_16: 0.2500 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 306/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2997 - acc: 0.4218 - recall_16: 0.0145 - precision_16: 0.6154\n",
            "Epoch 306: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2915 - acc: 0.4250 - recall_16: 0.0132 - precision_16: 0.6000 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 307/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3048 - acc: 0.4140 - recall_16: 0.0040 - precision_16: 0.4000\n",
            "Epoch 307: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2969 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.3333 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 308/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2899 - acc: 0.4265 - recall_16: 0.0088 - precision_16: 0.6000\n",
            "Epoch 308: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2899 - acc: 0.4265 - recall_16: 0.0088 - precision_16: 0.6000 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 309/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3160 - acc: 0.4060 - recall_16: 0.0020 - precision_16: 0.1111        \n",
            "Epoch 309: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3015 - acc: 0.4265 - recall_16: 0.0029 - precision_16: 0.1667 - val_loss: 1.3069 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 310/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3068 - acc: 0.4260 - recall_16: 0.0080 - precision_16: 0.2667\n",
            "Epoch 310: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3067 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.2000 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 311/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2903 - acc: 0.4338 - recall_16: 0.0154 - precision_16: 0.5556\n",
            "Epoch 311: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2974 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.5000 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 312/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2956 - acc: 0.4133 - recall_16: 0.0183 - precision_16: 0.6875\n",
            "Epoch 312: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2947 - acc: 0.4206 - recall_16: 0.0162 - precision_16: 0.6111 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 313/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3009 - acc: 0.4200 - recall_16: 0.0062 - precision_16: 0.2667\n",
            "Epoch 313: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2963 - acc: 0.4250 - recall_16: 0.0074 - precision_16: 0.3125 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 314/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2930 - acc: 0.4221 - recall_16: 0.0074 - precision_16: 0.3125\n",
            "Epoch 314: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2930 - acc: 0.4221 - recall_16: 0.0074 - precision_16: 0.3125 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 315/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3020 - acc: 0.4246 - recall_16: 0.0077 - precision_16: 0.2778\n",
            "Epoch 315: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3031 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.2778 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 316/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2976 - acc: 0.4200 - recall_16: 0.0062 - precision_16: 0.2222\n",
            "Epoch 316: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.2982 - acc: 0.4206 - recall_16: 0.0059 - precision_16: 0.2105 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 317/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2940 - acc: 0.4283 - recall_16: 0.0133 - precision_16: 0.5333\n",
            "Epoch 317: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2916 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.5000 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 318/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2946 - acc: 0.4246 - recall_16: 0.0092 - precision_16: 0.5455\n",
            "Epoch 318: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2963 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.5455 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 319/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.3044 - acc: 0.4167 - recall_16: 0.0083 - precision_16: 0.3125\n",
            "Epoch 319: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2964 - acc: 0.4221 - recall_16: 0.0074 - precision_16: 0.2941 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 320/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2992 - acc: 0.4262 - recall_16: 0.0154 - precision_16: 0.4545\n",
            "Epoch 320: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3019 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.4167 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 321/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3012 - acc: 0.4246 - recall_16: 0.0046 - precision_16: 0.1875\n",
            "Epoch 321: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2988 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.1579 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 322/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2986 - acc: 0.4246 - recall_16: 0.0062 - precision_16: 0.2667\n",
            "Epoch 322: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3003 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.2667 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 323/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2899 - acc: 0.4221 - recall_16: 0.0088 - precision_16: 0.4000\n",
            "Epoch 323: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2899 - acc: 0.4221 - recall_16: 0.0088 - precision_16: 0.4000 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 324/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2834 - acc: 0.4231 - recall_16: 0.0108 - precision_16: 0.5000\n",
            "Epoch 324: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2869 - acc: 0.4221 - recall_16: 0.0118 - precision_16: 0.5333 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 325/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3002 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.4000\n",
            "Epoch 325: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3002 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.4000 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 326/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3005 - acc: 0.4185 - recall_16: 0.0123 - precision_16: 0.3810\n",
            "Epoch 326: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2981 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.3636 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 327/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2947 - acc: 0.4246 - recall_16: 0.0062 - precision_16: 0.2857\n",
            "Epoch 327: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2964 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.2353 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 328/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2984 - acc: 0.4200 - recall_16: 0.0138 - precision_16: 0.4500\n",
            "Epoch 328: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2954 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.4762 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 329/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2750 - acc: 0.4367 - recall_16: 0.0100 - precision_16: 0.4000\n",
            "Epoch 329: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2923 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4375 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 330/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2944 - acc: 0.4221 - recall_16: 0.0206 - precision_16: 0.7368\n",
            "Epoch 330: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2944 - acc: 0.4221 - recall_16: 0.0206 - precision_16: 0.7368 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 331/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3037 - acc: 0.4185 - recall_16: 0.0108 - precision_16: 0.3684\n",
            "Epoch 331: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3012 - acc: 0.4206 - recall_16: 0.0103 - precision_16: 0.3500 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 332/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2963 - acc: 0.4250 - recall_16: 0.0118 - precision_16: 0.4444\n",
            "Epoch 332: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2963 - acc: 0.4250 - recall_16: 0.0118 - precision_16: 0.4444 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 333/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2884 - acc: 0.4277 - recall_16: 0.0077 - precision_16: 0.6250\n",
            "Epoch 333: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2947 - acc: 0.4250 - recall_16: 0.0074 - precision_16: 0.6250 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 334/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2912 - acc: 0.4221 - recall_16: 0.0088 - precision_16: 0.3750\n",
            "Epoch 334: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2912 - acc: 0.4221 - recall_16: 0.0088 - precision_16: 0.3750 - val_loss: 1.3068 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 335/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3064 - acc: 0.4169 - recall_16: 0.0154 - precision_16: 0.6250\n",
            "Epoch 335: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2999 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.5882 - val_loss: 1.3067 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 336/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2994 - acc: 0.4185 - recall_16: 0.0154 - precision_16: 0.6667\n",
            "Epoch 336: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2938 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.6667 - val_loss: 1.3066 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 337/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3024 - acc: 0.4185 - recall_16: 0.0154 - precision_16: 0.4545\n",
            "Epoch 337: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2976 - acc: 0.4221 - recall_16: 0.0162 - precision_16: 0.4783 - val_loss: 1.3065 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 338/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2939 - acc: 0.4292 - recall_16: 0.0154 - precision_16: 0.4545\n",
            "Epoch 338: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2960 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.4545 - val_loss: 1.3068 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 339/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2904 - acc: 0.4292 - recall_16: 0.0092 - precision_16: 0.3750\n",
            "Epoch 339: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2945 - acc: 0.4265 - recall_16: 0.0118 - precision_16: 0.4211 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 340/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2802 - acc: 0.4320 - recall_16: 0.0120 - precision_16: 0.4615\n",
            "Epoch 340: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2927 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.3750 - val_loss: 1.3070 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 341/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2920 - acc: 0.4292 - recall_16: 0.0169 - precision_16: 0.6471\n",
            "Epoch 341: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2954 - acc: 0.4206 - recall_16: 0.0162 - precision_16: 0.6471 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 342/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3048 - acc: 0.4215 - recall_16: 0.0092 - precision_16: 0.3158\n",
            "Epoch 342: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3030 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.3333 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 343/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2941 - acc: 0.4246 - recall_16: 0.0185 - precision_16: 0.4800\n",
            "Epoch 343: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2961 - acc: 0.4221 - recall_16: 0.0176 - precision_16: 0.4444 - val_loss: 1.3072 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 344/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2975 - acc: 0.4262 - recall_16: 0.0062 - precision_16: 0.3333\n",
            "Epoch 344: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2995 - acc: 0.4221 - recall_16: 0.0059 - precision_16: 0.3333 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 345/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2888 - acc: 0.4260 - recall_16: 0.0180 - precision_16: 0.6000\n",
            "Epoch 345: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2903 - acc: 0.4235 - recall_16: 0.0162 - precision_16: 0.5500 - val_loss: 1.3071 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 346/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2884 - acc: 0.4333 - recall_16: 0.0133 - precision_16: 0.5714\n",
            "Epoch 346: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2961 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.6000 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 347/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2916 - acc: 0.4265 - recall_16: 0.0162 - precision_16: 0.5000\n",
            "Epoch 347: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2916 - acc: 0.4265 - recall_16: 0.0162 - precision_16: 0.5000 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 348/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3003 - acc: 0.4215 - recall_16: 0.0154 - precision_16: 0.5556\n",
            "Epoch 348: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2992 - acc: 0.4250 - recall_16: 0.0147 - precision_16: 0.5556 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 349/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3020 - acc: 0.4206 - recall_16: 0.0147 - precision_16: 0.5000\n",
            "Epoch 349: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3020 - acc: 0.4206 - recall_16: 0.0147 - precision_16: 0.5000 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 350/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3042 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4667\n",
            "Epoch 350: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3042 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4667 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 351/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2994 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.4211\n",
            "Epoch 351: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2994 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.4211 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 352/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3000 - acc: 0.4206 - recall_16: 0.0044 - precision_16: 0.2308\n",
            "Epoch 352: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3000 - acc: 0.4206 - recall_16: 0.0044 - precision_16: 0.2308 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 353/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2977 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.2353\n",
            "Epoch 353: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2977 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.2353 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 354/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3018 - acc: 0.4185 - recall_16: 0.0154 - precision_16: 0.4762\n",
            "Epoch 354: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2974 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.4762 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 355/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2950 - acc: 0.4200 - recall_16: 0.0077 - precision_16: 0.3846\n",
            "Epoch 355: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2942 - acc: 0.4221 - recall_16: 0.0074 - precision_16: 0.3846 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 356/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3034 - acc: 0.4200 - recall_16: 0.0077 - precision_16: 0.2778\n",
            "Epoch 356: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2985 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.2778 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 357/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2913 - acc: 0.4262 - recall_16: 0.0046 - precision_16: 0.2500\n",
            "Epoch 357: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2961 - acc: 0.4221 - recall_16: 0.0044 - precision_16: 0.2500 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 358/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.3043 - acc: 0.4160 - recall_16: 0.0020 - precision_16: 0.1000\n",
            "Epoch 358: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2976 - acc: 0.4221 - recall_16: 0.0044 - precision_16: 0.1667 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 359/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2992 - acc: 0.4265 - recall_16: 0.0132 - precision_16: 0.5294\n",
            "Epoch 359: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2992 - acc: 0.4265 - recall_16: 0.0132 - precision_16: 0.5294 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 360/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2967 - acc: 0.4277 - recall_16: 0.0031 - precision_16: 0.2000\n",
            "Epoch 360: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3010 - acc: 0.4250 - recall_16: 0.0044 - precision_16: 0.2500 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 361/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2951 - acc: 0.4300 - recall_16: 0.0067 - precision_16: 0.5000\n",
            "Epoch 361: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3018 - acc: 0.4250 - recall_16: 0.0059 - precision_16: 0.4444 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 362/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2960 - acc: 0.4267 - recall_16: 0.0100 - precision_16: 0.4615\n",
            "Epoch 362: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2941 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.4615 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 363/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3002 - acc: 0.4250 - recall_16: 0.0103 - precision_16: 0.5833\n",
            "Epoch 363: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3002 - acc: 0.4250 - recall_16: 0.0103 - precision_16: 0.5833 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 364/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3013 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.5833\n",
            "Epoch 364: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3013 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.5833 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 365/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2975 - acc: 0.4221 - recall_16: 0.0132 - precision_16: 0.5000\n",
            "Epoch 365: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2975 - acc: 0.4221 - recall_16: 0.0132 - precision_16: 0.5000 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 366/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2964 - acc: 0.4200 - recall_16: 0.0040 - precision_16: 0.4000\n",
            "Epoch 366: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2965 - acc: 0.4235 - recall_16: 0.0029 - precision_16: 0.3333 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 367/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2934 - acc: 0.4262 - recall_16: 0.0169 - precision_16: 0.4583\n",
            "Epoch 367: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2985 - acc: 0.4221 - recall_16: 0.0162 - precision_16: 0.4400 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 368/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2991 - acc: 0.4277 - recall_16: 0.0108 - precision_16: 0.4667\n",
            "Epoch 368: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3003 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4667 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 369/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3023 - acc: 0.4200 - recall_16: 0.0046 - precision_16: 0.2727\n",
            "Epoch 369: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3015 - acc: 0.4221 - recall_16: 0.0044 - precision_16: 0.2727 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 370/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2964 - acc: 0.4169 - recall_16: 0.0108 - precision_16: 0.3500\n",
            "Epoch 370: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2923 - acc: 0.4250 - recall_16: 0.0103 - precision_16: 0.3500 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 371/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2958 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.2500\n",
            "Epoch 371: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2958 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.2500 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 372/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2916 - acc: 0.4231 - recall_16: 0.0062 - precision_16: 0.3077\n",
            "Epoch 372: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2953 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.3077 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 373/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2983 - acc: 0.4215 - recall_16: 0.0015 - precision_16: 0.1667\n",
            "Epoch 373: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2981 - acc: 0.4235 - recall_16: 0.0015 - precision_16: 0.1667 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 374/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2981 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.4167\n",
            "Epoch 374: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2981 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.4167 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 375/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2944 - acc: 0.4231 - recall_16: 0.0138 - precision_16: 0.4737\n",
            "Epoch 375: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2953 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.4737 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 376/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2977 - acc: 0.4215 - recall_16: 0.0062 - precision_16: 0.3636\n",
            "Epoch 376: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2960 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.3636 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 377/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2893 - acc: 0.4308 - recall_16: 0.0123 - precision_16: 0.5333\n",
            "Epoch 377: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2948 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.5333 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 378/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3009 - acc: 0.4215 - recall_16: 0.0062 - precision_16: 0.3333\n",
            "Epoch 378: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2988 - acc: 0.4221 - recall_16: 0.0059 - precision_16: 0.3333 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 379/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3047 - acc: 0.4262 - recall_16: 0.0031 - precision_16: 0.2500\n",
            "Epoch 379: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3028 - acc: 0.4250 - recall_16: 0.0029 - precision_16: 0.2500 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 380/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2938 - acc: 0.4277 - recall_16: 0.0108 - precision_16: 0.4375\n",
            "Epoch 380: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2958 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4375 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 381/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2944 - acc: 0.4340 - recall_16: 0.0080 - precision_16: 0.5000\n",
            "Epoch 381: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2993 - acc: 0.4250 - recall_16: 0.0088 - precision_16: 0.5000 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 382/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3017 - acc: 0.4231 - recall_16: 0.0077 - precision_16: 0.2941\n",
            "Epoch 382: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2979 - acc: 0.4250 - recall_16: 0.0074 - precision_16: 0.2778 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 383/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2898 - acc: 0.4231 - recall_16: 0.0108 - precision_16: 0.8750\n",
            "Epoch 383: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2928 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.7778 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 384/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2957 - acc: 0.4283 - recall_16: 0.0083 - precision_16: 0.4545\n",
            "Epoch 384: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2898 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.4545 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 385/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3043 - acc: 0.4262 - recall_16: 0.0092 - precision_16: 0.4615\n",
            "Epoch 385: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3047 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.4615 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 386/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2701 - acc: 0.4420 - recall_16: 0.0120 - precision_16: 0.5455\n",
            "Epoch 386: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2969 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.5000 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 387/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2897 - acc: 0.4262 - recall_16: 0.0077 - precision_16: 0.5556\n",
            "Epoch 387: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2936 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.5556 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 388/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3015 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.2727\n",
            "Epoch 388: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3015 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.2727 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 389/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2998 - acc: 0.4185 - recall_16: 0.0046 - precision_16: 0.4286\n",
            "Epoch 389: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2973 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.4286 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 390/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2924 - acc: 0.4262 - recall_16: 0.0108 - precision_16: 0.6364\n",
            "Epoch 390: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2934 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.6364 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 391/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3007 - acc: 0.4246 - recall_16: 0.0046 - precision_16: 0.2500\n",
            "Epoch 391: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3020 - acc: 0.4206 - recall_16: 0.0044 - precision_16: 0.2500 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 392/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2960 - acc: 0.4262 - recall_16: 0.0138 - precision_16: 0.5625\n",
            "Epoch 392: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2967 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.5625 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 393/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2950 - acc: 0.4169 - recall_16: 0.0108 - precision_16: 0.4667\n",
            "Epoch 393: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.2930 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.4667 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 394/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2934 - acc: 0.4231 - recall_16: 0.0092 - precision_16: 0.4286\n",
            "Epoch 394: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2973 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.4000 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 395/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2949 - acc: 0.4221 - recall_16: 0.0118 - precision_16: 0.6154\n",
            "Epoch 395: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2949 - acc: 0.4221 - recall_16: 0.0118 - precision_16: 0.6154 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 396/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2943 - acc: 0.4215 - recall_16: 0.0077 - precision_16: 0.5556\n",
            "Epoch 396: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2918 - acc: 0.4221 - recall_16: 0.0088 - precision_16: 0.5455 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 397/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2921 - acc: 0.4246 - recall_16: 0.0123 - precision_16: 0.5000\n",
            "Epoch 397: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2917 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.5000 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 398/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2927 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4118\n",
            "Epoch 398: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2927 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4118 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 399/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3014 - acc: 0.4215 - recall_16: 0.0046 - precision_16: 0.3333\n",
            "Epoch 399: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2985 - acc: 0.4221 - recall_16: 0.0044 - precision_16: 0.3000 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 400/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2924 - acc: 0.4292 - recall_16: 0.0108 - precision_16: 0.4118\n",
            "Epoch 400: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2978 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4118 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 401/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2913 - acc: 0.4309 - recall_16: 0.0109 - precision_16: 0.6000\n",
            "Epoch 401: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2931 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.5385 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 402/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2979 - acc: 0.4246 - recall_16: 0.0062 - precision_16: 0.3333\n",
            "Epoch 402: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2989 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.3333 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 403/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.3011 - acc: 0.4200 - recall_16: 0.0033 - precision_16: 0.2500\n",
            "Epoch 403: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2971 - acc: 0.4235 - recall_16: 0.0029 - precision_16: 0.2222 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 404/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3042 - acc: 0.4169 - recall_16: 0.0062 - precision_16: 0.4444\n",
            "Epoch 404: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2970 - acc: 0.4250 - recall_16: 0.0059 - precision_16: 0.4444 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 405/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3002 - acc: 0.4169 - recall_16: 0.0123 - precision_16: 0.6667\n",
            "Epoch 405: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2950 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.6154 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 406/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2979 - acc: 0.4246 - recall_16: 0.0077 - precision_16: 0.3846\n",
            "Epoch 406: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2950 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.3846 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 407/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2920 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.5714\n",
            "Epoch 407: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2920 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.5714 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 408/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2956 - acc: 0.4215 - recall_16: 0.0092 - precision_16: 0.3333\n",
            "Epoch 408: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2955 - acc: 0.4250 - recall_16: 0.0088 - precision_16: 0.3333 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 409/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2935 - acc: 0.4200 - recall_16: 0.0108 - precision_16: 0.5000\n",
            "Epoch 409: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2898 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.5333 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 410/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2915 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.6667\n",
            "Epoch 410: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2915 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.6667 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 411/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3012 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.3810\n",
            "Epoch 411: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.3012 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.3810 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 412/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2998 - acc: 0.4185 - recall_16: 0.0092 - precision_16: 0.5000\n",
            "Epoch 412: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2979 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.5385 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 413/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2994 - acc: 0.4185 - recall_16: 0.0185 - precision_16: 0.5455\n",
            "Epoch 413: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2964 - acc: 0.4235 - recall_16: 0.0176 - precision_16: 0.5455 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 414/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2936 - acc: 0.4145 - recall_16: 0.0109 - precision_16: 0.6667\n",
            "Epoch 414: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 14ms/step - loss: 1.2909 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.7000 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 415/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2947 - acc: 0.4200 - recall_16: 0.0123 - precision_16: 0.5333\n",
            "Epoch 415: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2937 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.5333 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 416/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2972 - acc: 0.4246 - recall_16: 0.0015 - precision_16: 0.1000        \n",
            "Epoch 416: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2986 - acc: 0.4221 - recall_16: 0.0029 - precision_16: 0.1818 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 417/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2993 - acc: 0.4200 - recall_16: 0.0077 - precision_16: 0.4167\n",
            "Epoch 417: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2956 - acc: 0.4250 - recall_16: 0.0074 - precision_16: 0.3846 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 418/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2992 - acc: 0.4262 - recall_16: 0.0123 - precision_16: 0.4706\n",
            "Epoch 418: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2980 - acc: 0.4250 - recall_16: 0.0118 - precision_16: 0.4706 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 419/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2966 - acc: 0.4200 - recall_16: 0.0067 - precision_16: 0.3077\n",
            "Epoch 419: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2938 - acc: 0.4221 - recall_16: 0.0059 - precision_16: 0.3077 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 420/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2930 - acc: 0.4185 - recall_16: 0.0062 - precision_16: 0.4000\n",
            "Epoch 420: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2942 - acc: 0.4250 - recall_16: 0.0059 - precision_16: 0.3636 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 421/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2979 - acc: 0.4262 - recall_16: 0.0046 - precision_16: 0.2727\n",
            "Epoch 421: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2975 - acc: 0.4250 - recall_16: 0.0044 - precision_16: 0.2727 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 422/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2988 - acc: 0.4262 - recall_16: 0.0138 - precision_16: 0.5625\n",
            "Epoch 422: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.2996 - acc: 0.4235 - recall_16: 0.0147 - precision_16: 0.5556 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 423/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2936 - acc: 0.4292 - recall_16: 0.0031 - precision_16: 0.2222        \n",
            "Epoch 423: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2951 - acc: 0.4221 - recall_16: 0.0044 - precision_16: 0.3000 - val_loss: 1.3082 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 424/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2982 - acc: 0.4169 - recall_16: 0.0092 - precision_16: 0.5000\n",
            "Epoch 424: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2943 - acc: 0.4221 - recall_16: 0.0088 - precision_16: 0.5000 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 425/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3002 - acc: 0.4262 - recall_16: 0.0077 - precision_16: 0.6250\n",
            "Epoch 425: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3039 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.5556 - val_loss: 1.3082 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 426/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3006 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4375\n",
            "Epoch 426: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3006 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4375 - val_loss: 1.3084 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 427/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2958 - acc: 0.4169 - recall_16: 0.0046 - precision_16: 0.3333\n",
            "Epoch 427: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2951 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.3333 - val_loss: 1.3082 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 428/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2983 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4118\n",
            "Epoch 428: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2983 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.4118 - val_loss: 1.3082 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 429/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3002 - acc: 0.4169 - recall_16: 0.0062 - precision_16: 0.2667\n",
            "Epoch 429: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2975 - acc: 0.4221 - recall_16: 0.0059 - precision_16: 0.2667 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 430/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2957 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.6429\n",
            "Epoch 430: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2957 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.6429 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 431/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2968 - acc: 0.4246 - recall_16: 0.0062 - precision_16: 0.2353\n",
            "Epoch 431: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2962 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.2222 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 432/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3040 - acc: 0.4221 - recall_16: 0.0029 - precision_16: 0.1818        \n",
            "Epoch 432: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3040 - acc: 0.4221 - recall_16: 0.0029 - precision_16: 0.1818 - val_loss: 1.3082 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 433/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3018 - acc: 0.4215 - recall_16: 0.0077 - precision_16: 0.3125\n",
            "Epoch 433: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2980 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.2941 - val_loss: 1.3082 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 434/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3021 - acc: 0.4154 - recall_16: 0.0077 - precision_16: 0.5000\n",
            "Epoch 434: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2958 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.5000 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 435/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3059 - acc: 0.4200 - recall_16: 0.0138 - precision_16: 0.9000\n",
            "Epoch 435: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2997 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.9000 - val_loss: 1.3082 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 436/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2900 - acc: 0.4277 - recall_16: 0.0123 - precision_16: 0.6667\n",
            "Epoch 436: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2959 - acc: 0.4221 - recall_16: 0.0147 - precision_16: 0.7143 - val_loss: 1.3083 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 437/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2969 - acc: 0.4231 - recall_16: 0.0092 - precision_16: 0.5455\n",
            "Epoch 437: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2969 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.5455 - val_loss: 1.3084 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 438/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2952 - acc: 0.4215 - recall_16: 0.0062 - precision_16: 0.4000\n",
            "Epoch 438: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2937 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.4000 - val_loss: 1.3083 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 439/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2926 - acc: 0.4277 - recall_16: 0.0077 - precision_16: 0.3125\n",
            "Epoch 439: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2958 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.3125 - val_loss: 1.3083 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 440/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2896 - acc: 0.4231 - recall_16: 0.0092 - precision_16: 0.4286\n",
            "Epoch 440: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2914 - acc: 0.4221 - recall_16: 0.0103 - precision_16: 0.4667 - val_loss: 1.3083 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 441/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3082 - acc: 0.4231 - recall_16: 0.0062 - precision_16: 0.2667\n",
            "Epoch 441: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3055 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.2667 - val_loss: 1.3083 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 442/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2991 - acc: 0.4133 - recall_16: 0.0017 - precision_16: 0.1250\n",
            "Epoch 442: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.2996 - acc: 0.4221 - recall_16: 0.0029 - precision_16: 0.1818 - val_loss: 1.3083 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 443/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2932 - acc: 0.4215 - recall_16: 0.0046 - precision_16: 0.2500\n",
            "Epoch 443: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.2894 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.2500 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 444/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2962 - acc: 0.4217 - recall_16: 0.0050 - precision_16: 0.3000\n",
            "Epoch 444: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2950 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.2500 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 445/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2976 - acc: 0.4185 - recall_16: 0.0062 - precision_16: 0.5000\n",
            "Epoch 445: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2980 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.5000 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 446/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2959 - acc: 0.4308 - recall_16: 0.0092 - precision_16: 0.5455\n",
            "Epoch 446: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3002 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.5000 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 447/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2872 - acc: 0.4262 - recall_16: 0.0077 - precision_16: 0.7143\n",
            "Epoch 447: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2923 - acc: 0.4206 - recall_16: 0.0088 - precision_16: 0.7500 - val_loss: 1.3081 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 448/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.3125 - acc: 0.4033 - recall_16: 0.0033 - precision_16: 0.3333\n",
            "Epoch 448: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.3020 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.3750 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 449/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.3023 - acc: 0.4167 - recall_16: 0.0083 - precision_16: 0.2778\n",
            "Epoch 449: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.2986 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.2778 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 450/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.3004 - acc: 0.4233 - recall_16: 0.0067 - precision_16: 0.3333\n",
            "Epoch 450: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3022 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.3571 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 451/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2918 - acc: 0.4323 - recall_16: 0.0062 - precision_16: 0.2857\n",
            "Epoch 451: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2973 - acc: 0.4250 - recall_16: 0.0059 - precision_16: 0.2857 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 452/500\n",
            "11/14 [======================>.......] - ETA: 0s - loss: 1.2908 - acc: 0.4291 - recall_16: 0.0036 - precision_16: 0.3333\n",
            "Epoch 452: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 19ms/step - loss: 1.2964 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.5000 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 453/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2945 - acc: 0.4200 - recall_16: 0.0062 - precision_16: 0.4000\n",
            "Epoch 453: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2936 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.4167 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 454/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2930 - acc: 0.4231 - recall_16: 0.0092 - precision_16: 0.4615\n",
            "Epoch 454: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2906 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.4615 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 455/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3040 - acc: 0.4185 - recall_16: 0.0046 - precision_16: 0.4286\n",
            "Epoch 455: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2988 - acc: 0.4250 - recall_16: 0.0044 - precision_16: 0.3750 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 456/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2960 - acc: 0.4200 - recall_16: 0.0062 - precision_16: 0.2857\n",
            "Epoch 456: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 18ms/step - loss: 1.2941 - acc: 0.4250 - recall_16: 0.0059 - precision_16: 0.2667 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 457/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2958 - acc: 0.4221 - recall_16: 0.0029 - precision_16: 0.2857\n",
            "Epoch 457: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2958 - acc: 0.4221 - recall_16: 0.0029 - precision_16: 0.2857 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 458/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2995 - acc: 0.4235 - recall_16: 0.0029 - precision_16: 0.2000\n",
            "Epoch 458: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2995 - acc: 0.4235 - recall_16: 0.0029 - precision_16: 0.2000 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 459/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2973 - acc: 0.4231 - recall_16: 0.0077 - precision_16: 0.3571\n",
            "Epoch 459: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2993 - acc: 0.4221 - recall_16: 0.0074 - precision_16: 0.3571 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 460/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2936 - acc: 0.4215 - recall_16: 0.0092 - precision_16: 0.3750\n",
            "Epoch 460: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2929 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.3333 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 461/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2986 - acc: 0.4283 - recall_16: 0.0067 - precision_16: 0.6667\n",
            "Epoch 461: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2985 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.6667 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 462/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3051 - acc: 0.4138 - recall_16: 0.0108 - precision_16: 0.5385\n",
            "Epoch 462: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2973 - acc: 0.4235 - recall_16: 0.0103 - precision_16: 0.5000 - val_loss: 1.3074 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 463/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2853 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.4615\n",
            "Epoch 463: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2853 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.4615 - val_loss: 1.3073 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 464/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2959 - acc: 0.4262 - recall_16: 0.0062 - precision_16: 0.3333\n",
            "Epoch 464: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2976 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.3333 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 465/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2954 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.5000\n",
            "Epoch 465: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2954 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.5000 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 466/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2844 - acc: 0.4338 - recall_16: 0.0046 - precision_16: 0.2727\n",
            "Epoch 466: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2904 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.2500 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 467/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2983 - acc: 0.4215 - recall_16: 0.0031 - precision_16: 0.2000\n",
            "Epoch 467: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2973 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.2727 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 468/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2962 - acc: 0.4262 - recall_16: 0.0031 - precision_16: 0.2222\n",
            "Epoch 468: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2966 - acc: 0.4235 - recall_16: 0.0029 - precision_16: 0.2222 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 469/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2980 - acc: 0.4215 - recall_16: 0.0046 - precision_16: 0.3750\n",
            "Epoch 469: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2952 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.3750 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 470/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2930 - acc: 0.4246 - recall_16: 0.0077 - precision_16: 0.6250\n",
            "Epoch 470: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2948 - acc: 0.4221 - recall_16: 0.0088 - precision_16: 0.6667 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 471/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2928 - acc: 0.4221 - recall_16: 0.0029 - precision_16: 0.1333\n",
            "Epoch 471: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2928 - acc: 0.4221 - recall_16: 0.0029 - precision_16: 0.1333 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 472/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2903 - acc: 0.4250 - recall_16: 0.0050 - precision_16: 0.5000\n",
            "Epoch 472: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2911 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.4286 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 473/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2938 - acc: 0.4246 - recall_16: 0.0077 - precision_16: 0.2778\n",
            "Epoch 473: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2960 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.2778 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 474/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3059 - acc: 0.4185 - recall_16: 0.0062 - precision_16: 0.6667\n",
            "Epoch 474: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.3011 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.6667 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 475/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2893 - acc: 0.4292 - recall_16: 0.0108 - precision_16: 0.5000\n",
            "Epoch 475: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2907 - acc: 0.4250 - recall_16: 0.0103 - precision_16: 0.4667 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 476/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2996 - acc: 0.4231 - recall_16: 0.0015 - precision_16: 0.1667\n",
            "Epoch 476: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2995 - acc: 0.4235 - recall_16: 0.0015 - precision_16: 0.1429 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 477/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2935 - acc: 0.4235 - recall_16: 0.0029 - precision_16: 0.4000\n",
            "Epoch 477: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2935 - acc: 0.4235 - recall_16: 0.0029 - precision_16: 0.4000 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 478/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3012 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.3810\n",
            "Epoch 478: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.3012 - acc: 0.4235 - recall_16: 0.0118 - precision_16: 0.3810 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 479/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2970 - acc: 0.4215 - recall_16: 0.0108 - precision_16: 0.4667\n",
            "Epoch 479: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2932 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.5294 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 480/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2972 - acc: 0.4215 - recall_16: 0.0031 - precision_16: 0.2857\n",
            "Epoch 480: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2960 - acc: 0.4235 - recall_16: 0.0029 - precision_16: 0.2857 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 481/500\n",
            "10/14 [====================>.........] - ETA: 0s - loss: 1.2931 - acc: 0.4240 - recall_16: 0.0060 - precision_16: 0.6000\n",
            "Epoch 481: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 15ms/step - loss: 1.2968 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.6000 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 482/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2942 - acc: 0.4215 - recall_16: 0.0077 - precision_16: 0.3846\n",
            "Epoch 482: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2928 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.3333 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 483/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2900 - acc: 0.4215 - recall_16: 0.0031 - precision_16: 0.2222\n",
            "Epoch 483: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2884 - acc: 0.4235 - recall_16: 0.0029 - precision_16: 0.2222 - val_loss: 1.3076 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 484/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2938 - acc: 0.4246 - recall_16: 0.0077 - precision_16: 0.3571\n",
            "Epoch 484: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2933 - acc: 0.4235 - recall_16: 0.0074 - precision_16: 0.3571 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 485/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2930 - acc: 0.4215 - recall_16: 0.0092 - precision_16: 0.5000\n",
            "Epoch 485: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2912 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.5000 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 486/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2908 - acc: 0.4246 - recall_16: 0.0046 - precision_16: 0.3333\n",
            "Epoch 486: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2929 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.3000 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 487/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2975 - acc: 0.4231 - recall_16: 0.0015 - precision_16: 0.1250\n",
            "Epoch 487: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2964 - acc: 0.4221 - recall_16: 0.0029 - precision_16: 0.2000 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 488/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3033 - acc: 0.4123 - recall_16: 0.0138 - precision_16: 0.6429\n",
            "Epoch 488: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2994 - acc: 0.4206 - recall_16: 0.0132 - precision_16: 0.6429 - val_loss: 1.3077 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 489/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2878 - acc: 0.4338 - recall_16: 0.0092 - precision_16: 0.4286\n",
            "Epoch 489: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2914 - acc: 0.4250 - recall_16: 0.0088 - precision_16: 0.4286 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 490/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2941 - acc: 0.4277 - recall_16: 0.0031 - precision_16: 0.3333\n",
            "Epoch 490: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2955 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.4286 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 491/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2831 - acc: 0.4246 - recall_16: 0.0092 - precision_16: 0.4615\n",
            "Epoch 491: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2890 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.4615 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 492/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2920 - acc: 0.4246 - recall_16: 0.0138 - precision_16: 0.6000\n",
            "Epoch 492: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2923 - acc: 0.4235 - recall_16: 0.0132 - precision_16: 0.6000 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 493/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2997 - acc: 0.4277 - recall_16: 0.0031 - precision_16: 0.1818        \n",
            "Epoch 493: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3026 - acc: 0.4235 - recall_16: 0.0029 - precision_16: 0.1818 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 494/500\n",
            "12/14 [========================>.....] - ETA: 0s - loss: 1.2819 - acc: 0.4400 - recall_16: 0.0100 - precision_16: 0.6000\n",
            "Epoch 494: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2922 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.5455 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 495/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2910 - acc: 0.4231 - recall_16: 0.0092 - precision_16: 0.5000\n",
            "Epoch 495: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2922 - acc: 0.4235 - recall_16: 0.0088 - precision_16: 0.5000 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 496/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.2946 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.2727\n",
            "Epoch 496: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2946 - acc: 0.4235 - recall_16: 0.0044 - precision_16: 0.2727 - val_loss: 1.3079 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 497/500\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.3015 - acc: 0.4250 - recall_16: 0.0074 - precision_16: 0.3571\n",
            "Epoch 497: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.3015 - acc: 0.4250 - recall_16: 0.0074 - precision_16: 0.3571 - val_loss: 1.3080 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 498/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2994 - acc: 0.4138 - recall_16: 0.0062 - precision_16: 0.8000\n",
            "Epoch 498: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 17ms/step - loss: 1.2911 - acc: 0.4235 - recall_16: 0.0059 - precision_16: 0.8000 - val_loss: 1.3078 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 499/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.3068 - acc: 0.4077 - recall_16: 0.0031 - precision_16: 0.2000\n",
            "Epoch 499: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2954 - acc: 0.4221 - recall_16: 0.0029 - precision_16: 0.2000 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n",
            "Epoch 500/500\n",
            "13/14 [==========================>...] - ETA: 0s - loss: 1.2930 - acc: 0.4185 - recall_16: 0.0077 - precision_16: 0.5556\n",
            "Epoch 500: val_loss did not improve from 1.29263\n",
            "14/14 [==============================] - 0s 16ms/step - loss: 1.2927 - acc: 0.4221 - recall_16: 0.0074 - precision_16: 0.5556 - val_loss: 1.3075 - val_acc: 0.5000 - val_recall_16: 0.0000e+00 - val_precision_16: 0.0000e+00\n"
          ]
        }
      ],
      "source": [
        "best_weights_file = 'weights.best.hdf5'\n",
        "checkpoint = ModelCheckpoint(best_weights_file,monitor='val_loss',verbose=1,save_best_only=True,mode='min')\n",
        "\n",
        "callbacks = [checkpoint]\n",
        "x_delta = x_train[:,0,:,:]\n",
        "x_theta = x_train[:,1,:,:]\n",
        "x_alpha = x_train[:,2,:,:]\n",
        "x_beta = x_train[:,3,:,:]\n",
        "x_gamma = x_train[:,4,:,:]\n",
        "validation_data=([x_delta[:40],x_theta[:40],x_alpha[:40],x_beta[:40],x_gamma[:40]],y_train[:40])\n",
        "\n",
        "train_history = model.fit([x_delta[40:],x_theta[40:],x_alpha[40:],x_beta[40:],x_gamma[40:]],y_train[40:],\n",
        "          batch_size=50,epochs=500,callbacks=callbacks,\n",
        "          validation_data=validation_data,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AuEBqruqAbu",
        "outputId": "a4948b31-0509-492a-9deb-65ceaf8e2126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': [1.4561750888824463, 1.471984624862671, 1.4545177221298218, 1.4275217056274414, 1.4416192770004272, 1.4436637163162231, 1.4082036018371582, 1.412106990814209, 1.4200276136398315, 1.4349271059036255, 1.4179319143295288, 1.3839322328567505, 1.3985886573791504, 1.399111270904541, 1.3812782764434814, 1.397140622138977, 1.3951427936553955, 1.3928197622299194, 1.3617279529571533, 1.3731040954589844, 1.3778557777404785, 1.378414511680603, 1.385498285293579, 1.3572839498519897, 1.3659225702285767, 1.349683403968811, 1.3520982265472412, 1.3631237745285034, 1.3722203969955444, 1.3586499691009521, 1.344112753868103, 1.3408658504486084, 1.3604621887207031, 1.3612395524978638, 1.3501465320587158, 1.3296899795532227, 1.3568999767303467, 1.353869915008545, 1.3502564430236816, 1.3415789604187012, 1.327474594116211, 1.339965581893921, 1.35623300075531, 1.3478177785873413, 1.3334054946899414, 1.3361629247665405, 1.3426463603973389, 1.3393943309783936, 1.3533776998519897, 1.33596670627594, 1.3320497274398804, 1.3448357582092285, 1.3368749618530273, 1.327096700668335, 1.3131259679794312, 1.3287274837493896, 1.3325642347335815, 1.3239048719406128, 1.3509665727615356, 1.3217188119888306, 1.323095679283142, 1.3305219411849976, 1.315763235092163, 1.3369109630584717, 1.3309444189071655, 1.330252766609192, 1.3497083187103271, 1.3174806833267212, 1.3380080461502075, 1.341080904006958, 1.3361932039260864, 1.3136792182922363, 1.3399370908737183, 1.3452072143554688, 1.310524821281433, 1.331841230392456, 1.3349063396453857, 1.3362089395523071, 1.3161054849624634, 1.3465051651000977, 1.3292322158813477, 1.334864616394043, 1.3344858884811401, 1.3424018621444702, 1.3223044872283936, 1.3345012664794922, 1.308734655380249, 1.331035852432251, 1.3698698282241821, 1.34492027759552, 1.3275560140609741, 1.3314651250839233, 1.3492581844329834, 1.3360661268234253, 1.3520658016204834, 1.3207390308380127, 1.3437066078186035, 1.3480192422866821, 1.3259025812149048, 1.333849310874939, 1.3259119987487793, 1.3206452131271362, 1.3282551765441895, 1.3336856365203857, 1.3103611469268799, 1.3228517770767212, 1.3271592855453491, 1.324765920639038, 1.3247321844100952, 1.3017604351043701, 1.3254679441452026, 1.321945309638977, 1.3054234981536865, 1.3092206716537476, 1.3336125612258911, 1.3245760202407837, 1.3212300539016724, 1.3251533508300781, 1.3214046955108643, 1.3160678148269653, 1.3351811170578003, 1.3201998472213745, 1.3233455419540405, 1.3446757793426514, 1.3273142576217651, 1.3176718950271606, 1.329784631729126, 1.3310428857803345, 1.3308318853378296, 1.3093258142471313, 1.3291903734207153, 1.3424863815307617, 1.319590449333191, 1.322689414024353, 1.311533808708191, 1.3213715553283691, 1.3324346542358398, 1.3285622596740723, 1.3274141550064087, 1.3260924816131592, 1.3221243619918823, 1.3197118043899536, 1.3157397508621216, 1.3320136070251465, 1.3263951539993286, 1.3406836986541748, 1.3294391632080078, 1.3284388780593872, 1.3137155771255493, 1.3204290866851807, 1.3245484828948975, 1.322838544845581, 1.3348172903060913, 1.3073991537094116, 1.3108552694320679, 1.3194665908813477, 1.3320577144622803, 1.3258212804794312, 1.3182945251464844, 1.3268201351165771, 1.3188958168029785, 1.3332659006118774, 1.326296329498291, 1.3327999114990234, 1.3225041627883911, 1.3175950050354004, 1.334212064743042, 1.313077449798584, 1.3179250955581665, 1.3246326446533203, 1.3382071256637573, 1.325164794921875, 1.3222618103027344, 1.3309903144836426, 1.31057608127594, 1.3183515071868896, 1.339306116104126, 1.3250069618225098, 1.3301887512207031, 1.3227450847625732, 1.3247166872024536, 1.3124175071716309, 1.3441439867019653, 1.3202868700027466, 1.3149771690368652, 1.316917061805725, 1.3292359113693237, 1.3238462209701538, 1.3173648118972778, 1.319352626800537, 1.3243809938430786, 1.302632451057434, 1.3038244247436523, 1.3231362104415894, 1.3180783987045288, 1.318969964981079, 1.3367183208465576, 1.3220270872116089, 1.3295750617980957, 1.3226792812347412, 1.3407505750656128, 1.3220889568328857, 1.3239396810531616, 1.3222897052764893, 1.3126367330551147, 1.311238408088684, 1.3343867063522339, 1.3230935335159302, 1.3167579174041748, 1.3209673166275024, 1.3154393434524536, 1.305783748626709, 1.3196418285369873, 1.3226159811019897, 1.319972276687622, 1.3138105869293213, 1.3241852521896362, 1.34512197971344, 1.3000144958496094, 1.3138411045074463, 1.3286851644515991, 1.3349448442459106, 1.3263214826583862, 1.3070411682128906, 1.3064898252487183, 1.341557264328003, 1.3279043436050415, 1.3364017009735107, 1.318881630897522, 1.3256237506866455, 1.3095077276229858, 1.320859432220459, 1.3242552280426025, 1.308754801750183, 1.3285964727401733, 1.3245357275009155, 1.3281453847885132, 1.3169764280319214, 1.32046377658844, 1.3164371252059937, 1.331095814704895, 1.316941261291504, 1.3297737836837769, 1.3117023706436157, 1.3288555145263672, 1.3151516914367676, 1.3266361951828003, 1.3219428062438965, 1.3074785470962524, 1.3366347551345825, 1.3267654180526733, 1.336754560470581, 1.3166717290878296, 1.317674160003662, 1.2951393127441406, 1.3424158096313477, 1.3352652788162231, 1.3187980651855469, 1.3192249536514282, 1.3268405199050903, 1.3143508434295654, 1.3339544534683228, 1.312843680381775, 1.3272947072982788, 1.3266485929489136, 1.3201593160629272, 1.31203031539917, 1.3110876083374023, 1.3231334686279297, 1.3203927278518677, 1.319095253944397, 1.3216906785964966, 1.3206160068511963, 1.3426437377929688, 1.3272292613983154, 1.324273705482483, 1.3299579620361328, 1.3164830207824707, 1.3273075819015503, 1.3227872848510742, 1.3146617412567139, 1.3172037601470947, 1.3343474864959717, 1.3302315473556519, 1.3178093433380127, 1.3226847648620605, 1.323940396308899, 1.3229001760482788, 1.3255470991134644, 1.332259178161621, 1.3203991651535034, 1.320285677909851, 1.3334380388259888, 1.3162360191345215, 1.336844563484192, 1.311715841293335, 1.3000047206878662, 1.3192412853240967, 1.3084337711334229, 1.3039456605911255, 1.3278199434280396, 1.3072515726089478, 1.3054544925689697, 1.3084856271743774, 1.322706937789917, 1.342937707901001, 1.3069313764572144, 1.3078124523162842, 1.328763484954834, 1.329880952835083, 1.3230832815170288, 1.312342882156372, 1.3215479850769043, 1.3189138174057007, 1.3243013620376587, 1.3175934553146362, 1.326253056526184, 1.3191635608673096, 1.3201394081115723, 1.3259868621826172, 1.301584005355835, 1.3187423944473267, 1.3203094005584717, 1.3234344720840454, 1.3392143249511719, 1.3257477283477783, 1.3215842247009277, 1.3037670850753784, 1.3172242641448975, 1.3234721422195435, 1.3159269094467163, 1.31668221950531, 1.3000876903533936, 1.3264790773391724, 1.3229190111160278, 1.3174513578414917, 1.3280137777328491, 1.3210583925247192, 1.3194994926452637, 1.3116697072982788, 1.3350905179977417, 1.3259549140930176, 1.3139126300811768, 1.3186960220336914, 1.3169455528259277, 1.31196129322052, 1.306356430053711, 1.3067249059677124, 1.309116244316101, 1.3090927600860596, 1.3168046474456787, 1.3223776817321777, 1.335835576057434, 1.3329116106033325, 1.3252919912338257, 1.321811556816101, 1.3225058317184448, 1.3073979616165161, 1.3225681781768799, 1.3135218620300293, 1.3152735233306885, 1.3162345886230469, 1.3097233772277832, 1.3225593566894531, 1.319374442100525, 1.309084177017212, 1.3281017541885376, 1.3068954944610596, 1.3146684169769287, 1.3088518381118774, 1.3208688497543335, 1.3113270998001099, 1.3271386623382568, 1.3276629447937012, 1.3161836862564087, 1.321398377418518, 1.326602578163147, 1.321473479270935, 1.2867742776870728, 1.3026375770568848, 1.3276139497756958, 1.3166347742080688, 1.3096342086791992, 1.3271337747573853, 1.3168768882751465, 1.3076717853546143, 1.3137390613555908, 1.307033896446228, 1.3303512334823608, 1.328385353088379, 1.3164466619491577, 1.3107309341430664, 1.3078763484954834, 1.313590168952942, 1.3087449073791504, 1.3249565362930298, 1.320009708404541, 1.3100637197494507, 1.3170697689056396, 1.3248474597930908, 1.321932077407837, 1.3023481369018555, 1.3143932819366455, 1.318652868270874, 1.3231045007705688, 1.3114252090454102, 1.3238375186920166, 1.3124885559082031, 1.3047248125076294, 1.307586431503296, 1.3067924976348877, 1.2962957620620728, 1.3150190114974976, 1.3072186708450317, 1.3206219673156738, 1.320115089416504, 1.3252278566360474, 1.3127800226211548, 1.308793067932129, 1.3232247829437256, 1.3107959032058716, 1.3016223907470703, 1.305222511291504, 1.3192095756530762, 1.3053181171417236, 1.3202763795852661, 1.316960334777832, 1.3200886249542236, 1.306689739227295, 1.3131991624832153, 1.3258908987045288, 1.3220020532608032, 1.3309744596481323, 1.323886752128601, 1.3120216131210327, 1.3291549682617188, 1.3328523635864258, 1.310209035873413, 1.3110225200653076, 1.312205195426941, 1.324384331703186, 1.3158963918685913, 1.326061725616455, 1.3290526866912842, 1.3013288974761963, 1.3076577186584473, 1.33242666721344, 1.3128167390823364, 1.3163188695907593, 1.3145487308502197, 1.3242229223251343, 1.321685791015625, 1.31452214717865, 1.3020069599151611, 1.320600152015686, 1.3144997358322144, 1.3428456783294678, 1.318026065826416, 1.3205111026763916, 1.32198166847229, 1.320370078086853, 1.3038829565048218, 1.3253535032272339, 1.3064998388290405, 1.3144086599349976, 1.2946463823318481, 1.3104740381240845, 1.3190805912017822, 1.3242449760437012, 1.3084315061569214, 1.3297418355941772, 1.323730230331421, 1.315346598625183, 1.2979556322097778, 1.3122124671936035, 1.307218074798584, 1.3205655813217163, 1.317058801651001, 1.3139855861663818, 1.3032513856887817, 1.3224514722824097, 1.3279634714126587, 1.3110198974609375, 1.3072471618652344, 1.3032587766647339, 1.3267252445220947, 1.3156936168670654, 1.3229172229766846, 1.3098070621490479, 1.3134269714355469, 1.3175578117370605, 1.3015477657318115, 1.30385422706604, 1.3106529712677002, 1.3189313411712646, 1.3273404836654663, 1.289894700050354, 1.3112518787384033, 1.3055644035339355, 1.3060669898986816], 'acc': [0.22499999403953552, 0.22794117033481598, 0.23088234663009644, 0.2338235229253769, 0.24558822810649872, 0.2441176474094391, 0.28382351994514465, 0.28823530673980713, 0.2558823525905609, 0.2397058755159378, 0.24852940440177917, 0.2955882251262665, 0.2970588207244873, 0.27352941036224365, 0.2926470637321472, 0.30882352590560913, 0.2970588207244873, 0.29411765933036804, 0.3323529362678528, 0.32499998807907104, 0.32058823108673096, 0.33088234066963196, 0.31911763548851013, 0.32647058367729187, 0.3382352888584137, 0.36176469922065735, 0.36764705181121826, 0.31029412150382996, 0.3338235318660736, 0.3441176414489746, 0.3397058844566345, 0.34705883264541626, 0.35147058963775635, 0.36617645621299744, 0.3602941036224365, 0.3897058963775635, 0.3397058844566345, 0.37794119119644165, 0.3735294044017792, 0.3764705955982208, 0.375, 0.38235294818878174, 0.36764705181121826, 0.3735294044017792, 0.3897058963775635, 0.375, 0.3897058963775635, 0.37205880880355835, 0.385294109582901, 0.3867647051811218, 0.37794119119644165, 0.385294109582901, 0.38382354378700256, 0.375, 0.4058823585510254, 0.3794117569923401, 0.3867647051811218, 0.3911764621734619, 0.385294109582901, 0.3794117569923401, 0.38823530077934265, 0.4014706015586853, 0.38382354378700256, 0.3911764621734619, 0.3808823525905609, 0.4000000059604645, 0.37205880880355835, 0.38823530077934265, 0.3867647051811218, 0.3955882489681244, 0.4058823585510254, 0.4205882251262665, 0.3867647051811218, 0.3808823525905609, 0.4117647111415863, 0.3970588147640228, 0.3808823525905609, 0.385294109582901, 0.4014706015586853, 0.3867647051811218, 0.37794119119644165, 0.38382354378700256, 0.40294116735458374, 0.3911764621734619, 0.41470587253570557, 0.385294109582901, 0.42500001192092896, 0.375, 0.3808823525905609, 0.38382354378700256, 0.38235294818878174, 0.37205880880355835, 0.37794119119644165, 0.38382354378700256, 0.37794119119644165, 0.4220588207244873, 0.38235294818878174, 0.3705882430076599, 0.3955882489681244, 0.39264705777168274, 0.40882351994514465, 0.40882351994514465, 0.385294109582901, 0.3897058963775635, 0.4073529541492462, 0.3970588147640228, 0.37794119119644165, 0.39411765336990356, 0.40294116735458374, 0.4000000059604645, 0.3808823525905609, 0.40441176295280457, 0.40294116735458374, 0.41323530673980713, 0.3764705955982208, 0.4058823585510254, 0.385294109582901, 0.4102941155433655, 0.3911764621734619, 0.41470587253570557, 0.40882351994514465, 0.4161764681339264, 0.3897058963775635, 0.3735294044017792, 0.40882351994514465, 0.39264705777168274, 0.39264705777168274, 0.3808823525905609, 0.4014706015586853, 0.3897058963775635, 0.39852941036224365, 0.3764705955982208, 0.39411765336990356, 0.40441176295280457, 0.3867647051811218, 0.4102941155433655, 0.37794119119644165, 0.3970588147640228, 0.4014706015586853, 0.41323530673980713, 0.39411765336990356, 0.3911764621734619, 0.40294116735458374, 0.39411765336990356, 0.38235294818878174, 0.39411765336990356, 0.38823530077934265, 0.38823530077934265, 0.40882351994514465, 0.4102941155433655, 0.3705882430076599, 0.39264705777168274, 0.3808823525905609, 0.40882351994514465, 0.4176470637321472, 0.41470587253570557, 0.3911764621734619, 0.3955882489681244, 0.4117647111415863, 0.39411765336990356, 0.39264705777168274, 0.3970588147640228, 0.385294109582901, 0.3735294044017792, 0.39411765336990356, 0.39411765336990356, 0.3955882489681244, 0.4058823585510254, 0.3955882489681244, 0.3970588147640228, 0.3867647051811218, 0.3970588147640228, 0.4073529541492462, 0.4014706015586853, 0.39852941036224365, 0.4117647111415863, 0.385294109582901, 0.37794119119644165, 0.3897058963775635, 0.40441176295280457, 0.39852941036224365, 0.4102941155433655, 0.3735294044017792, 0.3970588147640228, 0.40882351994514465, 0.40294116735458374, 0.3955882489681244, 0.3897058963775635, 0.40294116735458374, 0.39411765336990356, 0.38823530077934265, 0.42352941632270813, 0.4117647111415863, 0.4102941155433655, 0.3867647051811218, 0.39852941036224365, 0.39411765336990356, 0.40294116735458374, 0.385294109582901, 0.41470587253570557, 0.385294109582901, 0.4000000059604645, 0.39411765336990356, 0.3735294044017792, 0.4000000059604645, 0.41470587253570557, 0.4102941155433655, 0.3970588147640228, 0.3970588147640228, 0.39411765336990356, 0.39852941036224365, 0.41323530673980713, 0.39852941036224365, 0.4014706015586853, 0.4014706015586853, 0.39264705777168274, 0.3955882489681244, 0.3794117569923401, 0.42500001192092896, 0.41323530673980713, 0.4014706015586853, 0.4058823585510254, 0.3970588147640228, 0.4073529541492462, 0.41911765933036804, 0.3897058963775635, 0.3955882489681244, 0.4220588207244873, 0.385294109582901, 0.4000000059604645, 0.39852941036224365, 0.4058823585510254, 0.3911764621734619, 0.41470587253570557, 0.4058823585510254, 0.4014706015586853, 0.3955882489681244, 0.39264705777168274, 0.40441176295280457, 0.4161764681339264, 0.3897058963775635, 0.39411765336990356, 0.4058823585510254, 0.4000000059604645, 0.41470587253570557, 0.41470587253570557, 0.4014706015586853, 0.3955882489681244, 0.4014706015586853, 0.3911764621734619, 0.4058823585510254, 0.4014706015586853, 0.4161764681339264, 0.3955882489681244, 0.3970588147640228, 0.39852941036224365, 0.4117647111415863, 0.3970588147640228, 0.4000000059604645, 0.38823530077934265, 0.4073529541492462, 0.38823530077934265, 0.40294116735458374, 0.3955882489681244, 0.40441176295280457, 0.40882351994514465, 0.4073529541492462, 0.4220588207244873, 0.3955882489681244, 0.4014706015586853, 0.3955882489681244, 0.40882351994514465, 0.3955882489681244, 0.37205880880355835, 0.4161764681339264, 0.39411765336990356, 0.39411765336990356, 0.4220588207244873, 0.39411765336990356, 0.4000000059604645, 0.4014706015586853, 0.41911765933036804, 0.3867647051811218, 0.4102941155433655, 0.39411765336990356, 0.40441176295280457, 0.38823530077934265, 0.40294116735458374, 0.39264705777168274, 0.39852941036224365, 0.3911764621734619, 0.4161764681339264, 0.39411765336990356, 0.4014706015586853, 0.40441176295280457, 0.4117647111415863, 0.4220588207244873, 0.3911764621734619, 0.4102941155433655, 0.4102941155433655, 0.4176470637321472, 0.39264705777168274, 0.41470587253570557, 0.4058823585510254, 0.40294116735458374, 0.3867647051811218, 0.39411765336990356, 0.4117647111415863, 0.39411765336990356, 0.4058823585510254, 0.40882351994514465, 0.40441176295280457, 0.40294116735458374, 0.4102941155433655, 0.38823530077934265, 0.40882351994514465, 0.4117647111415863, 0.4117647111415863, 0.4014706015586853, 0.41323530673980713, 0.4058823585510254, 0.3970588147640228, 0.39852941036224365, 0.4014706015586853, 0.4000000059604645, 0.4058823585510254, 0.40441176295280457, 0.42500001192092896, 0.3911764621734619, 0.4014706015586853, 0.4073529541492462, 0.4058823585510254, 0.4161764681339264, 0.39852941036224365, 0.40882351994514465, 0.3955882489681244, 0.4000000059604645, 0.39264705777168274, 0.39411765336990356, 0.4073529541492462, 0.4073529541492462, 0.4161764681339264, 0.4000000059604645, 0.40441176295280457, 0.3955882489681244, 0.40882351994514465, 0.4000000059604645, 0.40441176295280457, 0.4058823585510254, 0.41911765933036804, 0.40441176295280457, 0.40882351994514465, 0.3955882489681244, 0.4058823585510254, 0.41323530673980713, 0.40294116735458374, 0.40294116735458374, 0.42352941632270813, 0.4073529541492462, 0.4264705777168274, 0.40882351994514465, 0.4014706015586853, 0.4058823585510254, 0.4014706015586853, 0.40294116735458374, 0.39411765336990356, 0.4073529541492462, 0.4014706015586853, 0.4058823585510254, 0.40294116735458374, 0.40294116735458374, 0.40441176295280457, 0.4000000059604645, 0.39411765336990356, 0.40294116735458374, 0.40882351994514465, 0.39411765336990356, 0.3970588147640228, 0.43088236451148987, 0.40882351994514465, 0.39264705777168274, 0.40294116735458374, 0.41470587253570557, 0.3911764621734619, 0.4000000059604645, 0.4161764681339264, 0.41911765933036804, 0.4102941155433655, 0.3955882489681244, 0.3911764621734619, 0.4014706015586853, 0.40882351994514465, 0.41470587253570557, 0.42352941632270813, 0.4000000059604645, 0.40294116735458374, 0.3897058963775635, 0.40882351994514465, 0.40441176295280457, 0.4117647111415863, 0.3867647051811218, 0.4058823585510254, 0.40441176295280457, 0.41911765933036804, 0.4014706015586853, 0.40882351994514465, 0.4102941155433655, 0.4058823585510254, 0.4205882251262665, 0.4058823585510254, 0.4073529541492462, 0.41911765933036804, 0.4014706015586853, 0.4073529541492462, 0.4014706015586853, 0.40294116735458374, 0.41470587253570557, 0.4073529541492462, 0.4058823585510254, 0.4014706015586853, 0.4014706015586853, 0.4102941155433655, 0.4073529541492462, 0.40441176295280457, 0.40882351994514465, 0.40882351994514465, 0.41323530673980713, 0.40294116735458374, 0.4000000059604645, 0.40441176295280457, 0.4102941155433655, 0.4058823585510254, 0.40441176295280457, 0.40441176295280457, 0.40882351994514465, 0.3970588147640228, 0.41323530673980713, 0.40882351994514465, 0.4102941155433655, 0.41323530673980713, 0.39852941036224365, 0.4117647111415863, 0.39264705777168274, 0.4014706015586853, 0.4117647111415863, 0.4176470637321472, 0.4000000059604645, 0.39852941036224365, 0.40441176295280457, 0.41470587253570557, 0.4000000059604645, 0.41470587253570557, 0.4058823585510254, 0.4161764681339264, 0.3970588147640228, 0.4220588207244873, 0.39411765336990356, 0.39411765336990356, 0.4073529541492462, 0.4000000059604645, 0.41323530673980713, 0.4264705777168274, 0.4102941155433655, 0.4073529541492462, 0.40882351994514465, 0.41911765933036804, 0.41323530673980713, 0.39852941036224365, 0.4000000059604645, 0.4000000059604645, 0.40882351994514465, 0.40441176295280457, 0.3955882489681244, 0.41470587253570557, 0.4176470637321472, 0.4161764681339264, 0.4117647111415863, 0.4117647111415863, 0.4073529541492462, 0.40441176295280457, 0.4000000059604645, 0.39852941036224365, 0.4058823585510254, 0.4117647111415863, 0.4058823585510254, 0.3970588147640228, 0.41470587253570557, 0.4220588207244873, 0.39852941036224365, 0.39852941036224365, 0.41470587253570557, 0.4014706015586853, 0.40441176295280457, 0.4264705777168274, 0.4073529541492462, 0.40294116735458374, 0.43088236451148987, 0.3970588147640228, 0.4058823585510254, 0.40294116735458374], 'recall_15': [0.0, 0.0, 0.0029411765281111, 0.00147058826405555, 0.00147058826405555, 0.0, 0.0, 0.004411764908581972, 0.00147058826405555, 0.00147058826405555, 0.0029411765281111, 0.0029411765281111, 0.00147058826405555, 0.0058823530562222, 0.008823529817163944, 0.004411764908581972, 0.008823529817163944, 0.0058823530562222, 0.010294117964804173, 0.0029411765281111, 0.008823529817163944, 0.0, 0.0058823530562222, 0.013235294260084629, 0.013235294260084629, 0.0117647061124444, 0.010294117964804173, 0.0117647061124444, 0.014705882407724857, 0.01617647148668766, 0.01764705963432789, 0.0235294122248888, 0.030882352963089943, 0.020588235929608345, 0.020588235929608345, 0.022058824077248573, 0.022058824077248573, 0.022058824077248573, 0.019117647781968117, 0.02500000037252903, 0.03529411926865578, 0.022058824077248573, 0.036764707416296005, 0.022058824077248573, 0.03529411926865578, 0.026470588520169258, 0.036764707416296005, 0.0235294122248888, 0.03529411926865578, 0.026470588520169258, 0.03529411926865578, 0.038235295563936234, 0.04264706000685692, 0.04411764815449715, 0.04852941259741783, 0.045588236302137375, 0.03970588371157646, 0.04117647185921669, 0.036764707416296005, 0.04852941259741783, 0.0470588244497776, 0.05147058889269829, 0.05000000074505806, 0.05147058889269829, 0.04852941259741783, 0.0470588244497776, 0.026470588520169258, 0.0573529414832592, 0.03529411926865578, 0.06029411777853966, 0.05147058889269829, 0.0573529414832592, 0.05147058889269829, 0.04411764815449715, 0.07352941483259201, 0.061764705926179886, 0.0470588244497776, 0.04411764815449715, 0.05147058889269829, 0.05588235333561897, 0.0573529414832592, 0.06617647409439087, 0.06470588594675064, 0.06029411777853966, 0.06470588594675064, 0.0470588244497776, 0.08676470816135406, 0.07500000298023224, 0.04852941259741783, 0.04852941259741783, 0.08529412001371384, 0.05147058889269829, 0.04852941259741783, 0.05588235333561897, 0.06323529779911041, 0.05882352963089943, 0.05588235333561897, 0.054411765187978745, 0.0676470622420311, 0.05147058889269829, 0.07058823853731155, 0.0676470622420311, 0.05147058889269829, 0.06911765038967133, 0.06617647409439087, 0.07647059112787247, 0.061764705926179886, 0.05588235333561897, 0.06323529779911041, 0.08529412001371384, 0.06470588594675064, 0.07500000298023224, 0.08235294371843338, 0.08529412001371384, 0.06323529779911041, 0.07352941483259201, 0.06911765038967133, 0.05882352963089943, 0.06323529779911041, 0.07058823853731155, 0.04852941259741783, 0.09117647260427475, 0.07205882668495178, 0.07500000298023224, 0.05882352963089943, 0.0573529414832592, 0.07941176742315292, 0.06470588594675064, 0.06470588594675064, 0.07205882668495178, 0.06470588594675064, 0.045588236302137375, 0.061764705926179886, 0.06470588594675064, 0.06617647409439087, 0.05882352963089943, 0.05882352963089943, 0.07500000298023224, 0.06617647409439087, 0.06617647409439087, 0.06323529779911041, 0.05588235333561897, 0.07500000298023224, 0.06470588594675064, 0.07205882668495178, 0.06470588594675064, 0.06029411777853966, 0.06617647409439087, 0.07205882668495178, 0.0470588244497776, 0.07058823853731155, 0.07352941483259201, 0.06617647409439087, 0.08088235557079315, 0.07058823853731155, 0.052941177040338516, 0.06323529779911041, 0.07205882668495178, 0.07058823853731155, 0.06323529779911041, 0.07058823853731155, 0.07647059112787247, 0.07500000298023224, 0.0573529414832592, 0.06470588594675064, 0.07500000298023224, 0.05147058889269829, 0.07205882668495178, 0.06617647409439087, 0.05588235333561897, 0.06470588594675064, 0.06323529779911041, 0.07500000298023224, 0.07352941483259201, 0.061764705926179886, 0.05882352963089943, 0.06617647409439087, 0.06323529779911041, 0.07941176742315292, 0.06323529779911041, 0.061764705926179886, 0.05588235333561897, 0.07205882668495178, 0.06617647409439087, 0.06617647409439087, 0.0676470622420311, 0.06911765038967133, 0.0573529414832592, 0.05588235333561897, 0.06470588594675064, 0.08970588445663452, 0.07500000298023224, 0.07058823853731155, 0.06029411777853966, 0.06617647409439087, 0.0882352963089943, 0.06323529779911041, 0.0573529414832592, 0.0676470622420311, 0.06911765038967133, 0.05147058889269829, 0.07058823853731155, 0.07941176742315292, 0.054411765187978745, 0.07205882668495178, 0.0676470622420311, 0.061764705926179886, 0.06029411777853966, 0.0573529414832592, 0.07352941483259201, 0.07058823853731155, 0.06617647409439087, 0.07205882668495178, 0.0676470622420311, 0.06470588594675064, 0.08235294371843338, 0.06029411777853966, 0.06029411777853966, 0.0573529414832592, 0.06617647409439087, 0.06617647409439087, 0.054411765187978745, 0.06323529779911041, 0.0470588244497776, 0.06029411777853966, 0.07647059112787247, 0.06323529779911041, 0.06617647409439087, 0.0676470622420311, 0.06323529779911041, 0.07205882668495178, 0.08088235557079315, 0.05147058889269829, 0.07058823853731155, 0.061764705926179886, 0.0573529414832592, 0.061764705926179886, 0.05000000074505806, 0.052941177040338516, 0.0573529414832592, 0.054411765187978745, 0.08382353186607361, 0.06617647409439087, 0.06617647409439087, 0.06470588594675064, 0.07352941483259201, 0.04117647185921669, 0.06029411777853966, 0.07500000298023224, 0.05147058889269829, 0.06470588594675064, 0.05588235333561897, 0.06911765038967133, 0.07205882668495178, 0.06470588594675064, 0.061764705926179886, 0.06617647409439087, 0.0676470622420311, 0.0676470622420311, 0.05882352963089943, 0.04264706000685692, 0.06323529779911041, 0.07500000298023224, 0.07205882668495178, 0.04852941259741783, 0.045588236302137375, 0.0779411792755127, 0.0676470622420311, 0.05882352963089943, 0.045588236302137375, 0.05882352963089943, 0.06617647409439087, 0.07058823853731155, 0.061764705926179886, 0.0676470622420311, 0.05882352963089943, 0.07647059112787247, 0.05882352963089943, 0.06029411777853966, 0.04411764815449715, 0.07352941483259201, 0.04411764815449715, 0.07058823853731155, 0.04264706000685692, 0.05147058889269829, 0.04117647185921669, 0.07205882668495178, 0.054411765187978745, 0.045588236302137375, 0.06029411777853966, 0.07352941483259201, 0.045588236302137375, 0.05147058889269829, 0.054411765187978745, 0.04411764815449715, 0.06470588594675064, 0.06323529779911041, 0.06029411777853966, 0.061764705926179886, 0.07647059112787247, 0.05588235333561897, 0.07205882668495178, 0.06470588594675064, 0.0676470622420311, 0.0573529414832592, 0.045588236302137375, 0.06911765038967133, 0.07647059112787247, 0.06029411777853966, 0.06029411777853966, 0.052941177040338516, 0.05588235333561897, 0.06323529779911041, 0.054411765187978745, 0.054411765187978745, 0.05000000074505806, 0.05147058889269829, 0.0470588244497776, 0.05147058889269829, 0.05882352963089943, 0.0573529414832592, 0.06911765038967133, 0.0573529414832592, 0.052941177040338516, 0.045588236302137375, 0.05882352963089943, 0.06470588594675064, 0.06029411777853966, 0.061764705926179886, 0.052941177040338516, 0.06323529779911041, 0.06617647409439087, 0.06470588594675064, 0.045588236302137375, 0.054411765187978745, 0.06029411777853966, 0.04852941259741783, 0.07500000298023224, 0.05147058889269829, 0.07205882668495178, 0.05000000074505806, 0.054411765187978745, 0.0573529414832592, 0.052941177040338516, 0.05588235333561897, 0.05882352963089943, 0.061764705926179886, 0.0573529414832592, 0.06029411777853966, 0.05882352963089943, 0.06323529779911041, 0.04411764815449715, 0.04264706000685692, 0.04117647185921669, 0.052941177040338516, 0.045588236302137375, 0.045588236302137375, 0.06029411777853966, 0.038235295563936234, 0.04117647185921669, 0.0676470622420311, 0.06029411777853966, 0.06470588594675064, 0.061764705926179886, 0.0470588244497776, 0.04852941259741783, 0.04264706000685692, 0.06617647409439087, 0.06911765038967133, 0.04852941259741783, 0.06323529779911041, 0.06029411777853966, 0.05147058889269829, 0.0470588244497776, 0.0470588244497776, 0.06911765038967133, 0.04411764815449715, 0.04411764815449715, 0.0779411792755127, 0.05882352963089943, 0.0676470622420311, 0.0470588244497776, 0.0573529414832592, 0.05147058889269829, 0.05588235333561897, 0.0573529414832592, 0.0470588244497776, 0.0573529414832592, 0.06029411777853966, 0.03235294297337532, 0.054411765187978745, 0.045588236302137375, 0.054411765187978745, 0.0573529414832592, 0.04411764815449715, 0.05588235333561897, 0.054411765187978745, 0.06470588594675064, 0.04852941259741783, 0.06029411777853966, 0.054411765187978745, 0.06470588594675064, 0.05147058889269829, 0.05882352963089943, 0.0470588244497776, 0.03529411926865578, 0.04264706000685692, 0.05000000074505806, 0.03970588371157646, 0.061764705926179886, 0.052941177040338516, 0.07941176742315292, 0.05000000074505806, 0.06323529779911041, 0.0573529414832592, 0.061764705926179886, 0.045588236302137375, 0.05588235333561897, 0.05588235333561897, 0.0470588244497776, 0.04117647185921669, 0.0573529414832592, 0.061764705926179886, 0.06617647409439087, 0.045588236302137375, 0.04852941259741783, 0.0470588244497776, 0.06470588594675064, 0.054411765187978745, 0.04117647185921669, 0.03970588371157646, 0.045588236302137375, 0.045588236302137375, 0.0573529414832592, 0.05147058889269829, 0.04852941259741783, 0.04117647185921669, 0.04264706000685692, 0.06470588594675064, 0.05147058889269829, 0.04411764815449715, 0.045588236302137375, 0.052941177040338516, 0.04411764815449715, 0.05147058889269829, 0.061764705926179886, 0.03382353112101555, 0.05588235333561897, 0.05882352963089943, 0.06029411777853966, 0.04411764815449715, 0.04264706000685692, 0.045588236302137375, 0.0470588244497776, 0.054411765187978745, 0.061764705926179886, 0.04117647185921669, 0.045588236302137375, 0.052941177040338516, 0.054411765187978745, 0.04117647185921669, 0.054411765187978745, 0.0573529414832592, 0.05147058889269829, 0.0573529414832592, 0.0573529414832592, 0.061764705926179886, 0.03970588371157646, 0.0573529414832592, 0.05882352963089943, 0.05000000074505806, 0.04411764815449715, 0.04411764815449715, 0.0573529414832592, 0.06470588594675064, 0.05882352963089943, 0.05147058889269829, 0.04117647185921669, 0.04264706000685692, 0.04411764815449715, 0.04852941259741783, 0.05000000074505806, 0.04852941259741783, 0.054411765187978745, 0.04852941259741783, 0.04264706000685692, 0.03529411926865578, 0.04852941259741783, 0.05000000074505806, 0.05588235333561897, 0.045588236302137375, 0.04411764815449715, 0.05588235333561897, 0.05000000074505806, 0.04852941259741783, 0.03529411926865578, 0.0573529414832592, 0.05000000074505806, 0.0676470622420311, 0.04264706000685692], 'precision_15': [0.0, 0.0, 0.2857142984867096, 0.1666666716337204, 0.20000000298023224, 0.0, 0.0, 0.75, 0.1666666716337204, 0.125, 0.6666666865348816, 0.6666666865348816, 0.5, 0.2857142984867096, 0.6000000238418579, 0.375, 0.5, 0.4000000059604645, 0.6363636255264282, 0.2857142984867096, 0.4615384638309479, 0.0, 0.23529411852359772, 0.5625, 0.4285714328289032, 0.47058823704719543, 0.3333333432674408, 0.5714285969734192, 0.4000000059604645, 0.4583333432674408, 0.47999998927116394, 0.5161290168762207, 0.6000000238418579, 0.37837839126586914, 0.42424243688583374, 0.5555555820465088, 0.4545454680919647, 0.3658536672592163, 0.37142857909202576, 0.48571428656578064, 0.52173912525177, 0.3191489279270172, 0.46296295523643494, 0.3947368562221527, 0.47058823704719543, 0.4390243887901306, 0.48076921701431274, 0.3636363744735718, 0.4067796468734741, 0.29032257199287415, 0.42105263471603394, 0.3513513505458832, 0.4264705777168274, 0.4615384638309479, 0.4285714328289032, 0.484375, 0.3802816867828369, 0.4000000059604645, 0.3731343150138855, 0.43421053886413574, 0.4848484992980957, 0.4166666567325592, 0.53125, 0.3888888955116272, 0.3928571343421936, 0.380952388048172, 0.3103448152542114, 0.47560974955558777, 0.34285715222358704, 0.4141414165496826, 0.3932584226131439, 0.49367088079452515, 0.3888888955116272, 0.3571428656578064, 0.5102040767669678, 0.45652174949645996, 0.35164836049079895, 0.32967033982276917, 0.4166666567325592, 0.4470588266849518, 0.39393940567970276, 0.4591836631298065, 0.46315789222717285, 0.39047619700431824, 0.4536082446575165, 0.4000000059604645, 0.5267857313156128, 0.45945945382118225, 0.37931033968925476, 0.33673468232154846, 0.4793388545513153, 0.38461539149284363, 0.34375, 0.42696627974510193, 0.39814814925193787, 0.42105263471603394, 0.3799999952316284, 0.36274510622024536, 0.46000000834465027, 0.3888888955116272, 0.4285714328289032, 0.3801652789115906, 0.35353535413742065, 0.4563106894493103, 0.4166666567325592, 0.4406779706478119, 0.3925233781337738, 0.4175824224948883, 0.44329896569252014, 0.48739495873451233, 0.42718446254730225, 0.43589743971824646, 0.5185185074806213, 0.4296296238899231, 0.4343434274196625, 0.4385964870452881, 0.4653465449810028, 0.380952388048172, 0.37719297409057617, 0.40336135029792786, 0.3510638177394867, 0.484375, 0.41525423526763916, 0.4146341383457184, 0.380952388048172, 0.41489362716674805, 0.44628098607063293, 0.4536082446575165, 0.36974790692329407, 0.45370370149612427, 0.4313725531101227, 0.3100000023841858, 0.4117647111415863, 0.40740740299224854, 0.4545454680919647, 0.3883495032787323, 0.3669724762439728, 0.4553571343421936, 0.39823007583618164, 0.39823007583618164, 0.38738739490509033, 0.4367816150188446, 0.47663551568984985, 0.4112149477005005, 0.426086962223053, 0.4313725531101227, 0.4019607901573181, 0.478723406791687, 0.48514851927757263, 0.34408602118492126, 0.4403669834136963, 0.4000000059604645, 0.4245283007621765, 0.4365079402923584, 0.4403669834136963, 0.38297873735427856, 0.40186914801597595, 0.44545453786849976, 0.47058823704719543, 0.4134615361690521, 0.42105263471603394, 0.41600000858306885, 0.5, 0.3545454442501068, 0.3928571343421936, 0.44736841320991516, 0.3645833432674408, 0.4579439163208008, 0.4285714328289032, 0.3762376308441162, 0.38260868191719055, 0.4343434274196625, 0.4285714328289032, 0.45045045018196106, 0.41584157943725586, 0.39603960514068604, 0.4245283007621765, 0.37719297409057617, 0.47787609696388245, 0.3583333194255829, 0.37168142199516296, 0.42696627974510193, 0.42241379618644714, 0.40909090638160706, 0.4455445408821106, 0.42990654706954956, 0.46078431606292725, 0.3451327383518219, 0.37254902720451355, 0.4536082446575165, 0.4692307710647583, 0.45132744312286377, 0.4615384638309479, 0.4712643623352051, 0.44999998807907104, 0.5504587292671204, 0.373913049697876, 0.4588235318660736, 0.42592594027519226, 0.39830508828163147, 0.33980581164360046, 0.4403669834136963, 0.4736842215061188, 0.39361703395843506, 0.4711538553237915, 0.4423076808452606, 0.4329896867275238, 0.44086021184921265, 0.3644859790802002, 0.4587155878543854, 0.4660194218158722, 0.5056179761886597, 0.44545453786849976, 0.40707963705062866, 0.4399999976158142, 0.5, 0.3980582654476166, 0.37272727489471436, 0.4193548262119293, 0.4545454680919647, 0.4455445408821106, 0.3700000047683716, 0.4343434274196625, 0.40506330132484436, 0.43617022037506104, 0.4642857015132904, 0.4343434274196625, 0.4166666567325592, 0.3965517282485962, 0.4095238149166107, 0.426086962223053, 0.48672565817832947, 0.3499999940395355, 0.5052631497383118, 0.38181817531585693, 0.42391303181648254, 0.4117647111415863, 0.3863636255264282, 0.40449437499046326, 0.37142857909202576, 0.3854166567325592, 0.5135135054588318, 0.4285714328289032, 0.4736842215061188, 0.44897958636283875, 0.5263158082962036, 0.3076923191547394, 0.4270833432674408, 0.4722222089767456, 0.3333333432674408, 0.38596490025520325, 0.3689320385456085, 0.4392523467540741, 0.4375, 0.4680851101875305, 0.4421052634716034, 0.44117647409439087, 0.4035087823867798, 0.42592594027519226, 0.380952388048172, 0.3295454680919647, 0.3909091055393219, 0.4285714328289032, 0.44545453786849976, 0.37078651785850525, 0.3444444537162781, 0.5247524976730347, 0.43809524178504944, 0.47058823704719543, 0.31313130259513855, 0.39603960514068604, 0.3781512677669525, 0.4571428596973419, 0.3962264060974121, 0.41818180680274963, 0.4301075339317322, 0.49056604504585266, 0.39603960514068604, 0.4767441749572754, 0.3191489279270172, 0.44247788190841675, 0.3529411852359772, 0.49484536051750183, 0.3580246865749359, 0.3932584226131439, 0.3589743673801422, 0.4757281541824341, 0.45679011940956116, 0.3444444537162781, 0.38679245114326477, 0.4587155878543854, 0.3563218414783478, 0.3888888955116272, 0.3978494703769684, 0.32608696818351746, 0.42307692766189575, 0.4215686321258545, 0.42268040776252747, 0.4038461446762085, 0.5098039507865906, 0.42696627974510193, 0.4803921580314636, 0.4583333432674408, 0.5111111402511597, 0.4431818127632141, 0.3734939694404602, 0.4747474789619446, 0.5098039507865906, 0.4555555582046509, 0.46590909361839294, 0.38297873735427856, 0.4318181872367859, 0.483146071434021, 0.43529412150382996, 0.4021739065647125, 0.39080458879470825, 0.39772728085517883, 0.3333333432674408, 0.40697672963142395, 0.39603960514068604, 0.4193548262119293, 0.5053763389587402, 0.375, 0.4137931168079376, 0.3333333432674408, 0.380952388048172, 0.4536082446575165, 0.44086021184921265, 0.38532111048698425, 0.3870967626571655, 0.44329896569252014, 0.4639175236225128, 0.5057471394538879, 0.37804877758026123, 0.41111111640930176, 0.488095223903656, 0.37078651785850525, 0.43220338225364685, 0.43209877610206604, 0.5157894492149353, 0.3863636255264282, 0.3978494703769684, 0.45348837971687317, 0.4390243887901306, 0.38383838534355164, 0.4597701132297516, 0.5, 0.4333333373069763, 0.44086021184921265, 0.449438214302063, 0.4526315927505493, 0.37037035822868347, 0.353658527135849, 0.3255814015865326, 0.36734694242477417, 0.3827160596847534, 0.4025973975658417, 0.44086021184921265, 0.33766233921051025, 0.3636363744735718, 0.5, 0.44565218687057495, 0.42307692766189575, 0.4883720874786377, 0.41025641560554504, 0.38823530077934265, 0.3333333432674408, 0.5, 0.4699999988079071, 0.4583333432674408, 0.47777777910232544, 0.4270833432674408, 0.40697672963142395, 0.36781609058380127, 0.4000000059604645, 0.4895833432674408, 0.3571428656578064, 0.3614457845687866, 0.6022727489471436, 0.4166666567325592, 0.4423076808452606, 0.40506330132484436, 0.46987950801849365, 0.3365384638309479, 0.42696627974510193, 0.4333333373069763, 0.36781609058380127, 0.42391303181648254, 0.4183673560619354, 0.34375, 0.45121949911117554, 0.39743590354919434, 0.4157303273677826, 0.4431818127632141, 0.4166666567325592, 0.4691357910633087, 0.4868420958518982, 0.4444444477558136, 0.4583333432674408, 0.4270833432674408, 0.4021739065647125, 0.4583333432674408, 0.3804347813129425, 0.4395604431629181, 0.4000000059604645, 0.3287671208381653, 0.3918918967247009, 0.4000000059604645, 0.375, 0.47191011905670166, 0.38297873735427856, 0.5625, 0.39080458879470825, 0.4479166567325592, 0.4588235318660736, 0.5316455960273743, 0.37804877758026123, 0.5066666603088379, 0.4470588266849518, 0.395061731338501, 0.37837839126586914, 0.5064935088157654, 0.4516128897666931, 0.4455445408821106, 0.4025973975658417, 0.36666667461395264, 0.40506330132484436, 0.4583333432674408, 0.4868420958518982, 0.37837839126586914, 0.3253012001514435, 0.38749998807907104, 0.4025973975658417, 0.4588235318660736, 0.39772728085517883, 0.38823530077934265, 0.3499999940395355, 0.43939393758773804, 0.44897958636283875, 0.4166666567325592, 0.4166666567325592, 0.39743590354919434, 0.42352941632270813, 0.3658536672592163, 0.4166666567325592, 0.44680851697921753, 0.3333333432674408, 0.4367816150188446, 0.449438214302063, 0.4606741666793823, 0.4225352108478546, 0.3815789520740509, 0.42465752363204956, 0.41558441519737244, 0.4252873659133911, 0.4883720874786377, 0.3499999940395355, 0.41333332657814026, 0.47999998927116394, 0.4457831382751465, 0.3733333349227905, 0.5522388219833374, 0.4588235318660736, 0.4794520437717438, 0.45348837971687317, 0.49367088079452515, 0.45652174949645996, 0.421875, 0.4482758641242981, 0.42105263471603394, 0.39534884691238403, 0.37037035822868347, 0.375, 0.5270270109176636, 0.44897958636283875, 0.4761904776096344, 0.4430379867553711, 0.3499999940395355, 0.3815789520740509, 0.44117647409439087, 0.39759036898612976, 0.4533333480358124, 0.4714285731315613, 0.4457831382751465, 0.44594594836235046, 0.3580246865749359, 0.3243243098258972, 0.4285714328289032, 0.465753436088562, 0.45783132314682007, 0.41333332657814026, 0.46875, 0.45783132314682007, 0.3863636255264282, 0.38372093439102173, 0.29629629850387573, 0.4642857015132904, 0.4722222089767456, 0.4842105209827423, 0.3815789520740509], 'val_loss': [1.4115806818008423, 1.4068663120269775, 1.4025194644927979, 1.398442029953003, 1.3942443132400513, 1.390303611755371, 1.3865344524383545, 1.3831173181533813, 1.3796919584274292, 1.3764848709106445, 1.3733444213867188, 1.3706313371658325, 1.3679230213165283, 1.3654162883758545, 1.3629209995269775, 1.3607347011566162, 1.3584657907485962, 1.3563581705093384, 1.3541395664215088, 1.3523213863372803, 1.3504359722137451, 1.3485920429229736, 1.3469061851501465, 1.3452436923980713, 1.3435981273651123, 1.3422602415084839, 1.3408700227737427, 1.3396008014678955, 1.3380680084228516, 1.336713433265686, 1.3354767560958862, 1.334388017654419, 1.3332407474517822, 1.3321287631988525, 1.3311444520950317, 1.3300385475158691, 1.3290380239486694, 1.3282527923583984, 1.3273718357086182, 1.3263710737228394, 1.3256152868270874, 1.3249928951263428, 1.3241984844207764, 1.3235636949539185, 1.3229196071624756, 1.3221566677093506, 1.321576476097107, 1.3208736181259155, 1.3200631141662598, 1.3194329738616943, 1.3188228607177734, 1.3183120489120483, 1.317917823791504, 1.3175160884857178, 1.3170981407165527, 1.316564679145813, 1.3162378072738647, 1.315879464149475, 1.3154594898223877, 1.3151123523712158, 1.314795732498169, 1.314606785774231, 1.3142708539962769, 1.3140249252319336, 1.3137412071228027, 1.313402533531189, 1.3131153583526611, 1.3129017353057861, 1.3124794960021973, 1.3124796152114868, 1.3123352527618408, 1.3122129440307617, 1.312023401260376, 1.3119474649429321, 1.3118054866790771, 1.31171452999115, 1.311597466468811, 1.311570405960083, 1.3115216493606567, 1.3113701343536377, 1.3113231658935547, 1.3111114501953125, 1.311079978942871, 1.311004638671875, 1.3109467029571533, 1.3107906579971313, 1.3107101917266846, 1.310743808746338, 1.3105729818344116, 1.3105571269989014, 1.3106157779693604, 1.3104288578033447, 1.3103868961334229, 1.3102442026138306, 1.310192346572876, 1.3102233409881592, 1.3102600574493408, 1.310158610343933, 1.3100296258926392, 1.3099666833877563, 1.3099029064178467, 1.309981346130371, 1.3098598718643188, 1.3096857070922852, 1.3097044229507446, 1.3097267150878906, 1.309691071510315, 1.3095788955688477, 1.3094574213027954, 1.309450387954712, 1.3094727993011475, 1.309477686882019, 1.309328556060791, 1.3093875646591187, 1.3093669414520264, 1.3093907833099365, 1.3093386888504028, 1.309202790260315, 1.3091685771942139, 1.3092138767242432, 1.3093312978744507, 1.3092100620269775, 1.309157133102417, 1.3091609477996826, 1.3091508150100708, 1.3091466426849365, 1.3091567754745483, 1.3091626167297363, 1.309314489364624, 1.309396505355835, 1.309382677078247, 1.3094559907913208, 1.3093557357788086, 1.309417963027954, 1.309450387954712, 1.3094468116760254, 1.309497594833374, 1.309463620185852, 1.3094183206558228, 1.3094043731689453, 1.3093230724334717, 1.309200644493103, 1.3091169595718384, 1.3092302083969116, 1.3092867136001587, 1.3092565536499023, 1.3092241287231445, 1.3092247247695923, 1.3092007637023926, 1.3092305660247803, 1.3092267513275146, 1.3093477487564087, 1.3092201948165894, 1.3092314004898071, 1.3092477321624756, 1.3091932535171509, 1.3092024326324463, 1.3092893362045288, 1.3092758655548096, 1.3092052936553955, 1.3091785907745361, 1.309234380722046, 1.309258222579956, 1.3091797828674316, 1.3091213703155518, 1.3092114925384521, 1.309211015701294, 1.3091894388198853, 1.3091081380844116, 1.3090440034866333, 1.3089725971221924, 1.3090251684188843, 1.3090635538101196, 1.3090368509292603, 1.3089789152145386, 1.3089851140975952, 1.3089745044708252, 1.3089138269424438, 1.309088110923767, 1.3090225458145142, 1.308966875076294, 1.3088458776474, 1.3089210987091064, 1.3089330196380615, 1.3088338375091553, 1.3088691234588623, 1.3087873458862305, 1.3088843822479248, 1.3088279962539673, 1.308795690536499, 1.3089873790740967, 1.3090654611587524, 1.3090240955352783, 1.3090126514434814, 1.3090802431106567, 1.3091320991516113, 1.3091542720794678, 1.3091354370117188, 1.3092834949493408, 1.3094450235366821, 1.3094581365585327, 1.309411644935608, 1.3093971014022827, 1.3094255924224854, 1.3093931674957275, 1.3094589710235596, 1.309462547302246, 1.3093277215957642, 1.3092269897460938, 1.3092172145843506, 1.3092947006225586, 1.3092167377471924, 1.3092105388641357, 1.3092012405395508, 1.3091245889663696, 1.309107780456543, 1.3091684579849243, 1.3091892004013062, 1.3092244863510132, 1.3091322183609009, 1.3092750310897827, 1.309335470199585, 1.3093782663345337, 1.3093783855438232, 1.3092682361602783, 1.3092411756515503, 1.3092330694198608, 1.3092429637908936, 1.3093448877334595, 1.3095126152038574, 1.3095405101776123, 1.309662938117981, 1.3096554279327393, 1.309543490409851, 1.3095159530639648, 1.3094685077667236, 1.309446930885315, 1.3094393014907837, 1.3094362020492554, 1.309403419494629, 1.309491515159607, 1.3094428777694702, 1.309548258781433, 1.309607744216919, 1.3096482753753662, 1.3096942901611328, 1.3095388412475586, 1.3095299005508423, 1.3095266819000244, 1.3095195293426514, 1.309570550918579, 1.3095401525497437, 1.309562087059021, 1.309556245803833, 1.3094395399093628, 1.309423565864563, 1.3094711303710938, 1.309528112411499, 1.3095600605010986, 1.3095874786376953, 1.3096625804901123, 1.3097132444381714, 1.3098621368408203, 1.3099159002304077, 1.309916615486145, 1.3099180459976196, 1.3098912239074707, 1.3099972009658813, 1.3099915981292725, 1.3100796937942505, 1.3101122379302979, 1.310201644897461, 1.310243844985962, 1.310281753540039, 1.3103606700897217, 1.310364007949829, 1.310342788696289, 1.3104360103607178, 1.310393214225769, 1.3104283809661865, 1.3104288578033447, 1.310455083847046, 1.3104488849639893, 1.31047523021698, 1.3104095458984375, 1.310375452041626, 1.3103374242782593, 1.3102970123291016, 1.3103053569793701, 1.310426115989685, 1.3105099201202393, 1.3104957342147827, 1.3105442523956299, 1.3105812072753906, 1.3105971813201904, 1.3105980157852173, 1.3106186389923096, 1.310630440711975, 1.3106797933578491, 1.3106733560562134, 1.3106657266616821, 1.3105846643447876, 1.3105589151382446, 1.3104753494262695, 1.310453176498413, 1.3104970455169678, 1.3104934692382812, 1.3105075359344482, 1.3105484247207642, 1.3105037212371826, 1.3105072975158691, 1.3104448318481445, 1.3104854822158813, 1.3105463981628418, 1.3105690479278564, 1.3105825185775757, 1.310565710067749, 1.3105300664901733, 1.310524582862854, 1.3105762004852295, 1.3106577396392822, 1.3106452226638794, 1.3107097148895264, 1.31075119972229, 1.3106980323791504, 1.3107531070709229, 1.3108067512512207, 1.3108174800872803, 1.3107635974884033, 1.3107459545135498, 1.3107510805130005, 1.3107322454452515, 1.3108125925064087, 1.3108131885528564, 1.3108723163604736, 1.3109153509140015, 1.3109029531478882, 1.310931921005249, 1.3108742237091064, 1.310898780822754, 1.3108837604522705, 1.3109725713729858, 1.310965657234192, 1.3109633922576904, 1.3109838962554932, 1.3109931945800781, 1.311007022857666, 1.3110156059265137, 1.3109829425811768, 1.31094229221344, 1.3109146356582642, 1.3108892440795898, 1.3108727931976318, 1.3108974695205688, 1.3109915256500244, 1.3110198974609375, 1.310949683189392, 1.3109644651412964, 1.3109238147735596, 1.3109227418899536, 1.3109991550445557, 1.3110098838806152, 1.3110398054122925, 1.3110339641571045, 1.3109195232391357, 1.3109108209609985, 1.3109500408172607, 1.3109838962554932, 1.3109891414642334, 1.3110063076019287, 1.3110666275024414, 1.3111579418182373, 1.3111613988876343, 1.3112003803253174, 1.3112223148345947, 1.3113000392913818, 1.31130850315094, 1.3113161325454712, 1.3113043308258057, 1.3113181591033936, 1.311331033706665, 1.3112467527389526, 1.311325192451477, 1.3113877773284912, 1.3114051818847656, 1.311448097229004, 1.3114253282546997, 1.311462163925171, 1.3114429712295532, 1.3114471435546875, 1.3114488124847412, 1.3113868236541748, 1.3114467859268188, 1.3114023208618164, 1.3112905025482178, 1.3113383054733276, 1.3112905025482178, 1.3113733530044556, 1.311418890953064, 1.311384916305542, 1.3114126920700073, 1.31150221824646, 1.311468482017517, 1.3115156888961792, 1.3114748001098633, 1.311440110206604, 1.3115075826644897, 1.3115041255950928, 1.3114302158355713, 1.311440110206604, 1.3114542961120605, 1.31148362159729, 1.3114588260650635, 1.3115019798278809, 1.3114922046661377, 1.3115094900131226, 1.3114339113235474, 1.3114076852798462, 1.3114044666290283, 1.311450481414795, 1.3114382028579712, 1.3114159107208252, 1.3114560842514038, 1.3114835023880005, 1.3114721775054932, 1.3114941120147705, 1.3114945888519287, 1.3115098476409912, 1.311547875404358, 1.3115520477294922, 1.3116555213928223, 1.3116912841796875, 1.311708688735962, 1.3117557764053345, 1.3117514848709106, 1.3117297887802124, 1.3117401599884033, 1.311767339706421, 1.3118352890014648, 1.3117831945419312, 1.3118234872817993, 1.3117094039916992, 1.3117311000823975, 1.3117367029190063, 1.3117046356201172, 1.3118054866790771, 1.3117802143096924, 1.3116791248321533, 1.311694860458374, 1.3117023706436157, 1.3117307424545288, 1.311715841293335, 1.3116546869277954, 1.3116053342819214, 1.3116428852081299, 1.3115867376327515, 1.3116004467010498, 1.311621904373169, 1.3115230798721313, 1.3115947246551514, 1.3116461038589478, 1.3115886449813843, 1.3116519451141357, 1.3116521835327148, 1.3117024898529053, 1.311713457107544, 1.3116449117660522, 1.3116223812103271, 1.3116525411605835, 1.3117516040802002, 1.3118000030517578, 1.3117411136627197, 1.311741590499878, 1.311655044555664, 1.311733603477478, 1.3118664026260376, 1.3119001388549805, 1.3119251728057861, 1.3117955923080444, 1.3118007183074951, 1.3118864297866821, 1.3118677139282227, 1.3117879629135132, 1.3117982149124146, 1.31179940700531, 1.3117835521697998, 1.311767339706421, 1.311805009841919, 1.311804175376892, 1.311787486076355, 1.311798334121704, 1.311726450920105, 1.311652660369873, 1.311706304550171, 1.3117021322250366, 1.31178879737854, 1.3117576837539673, 1.3118005990982056, 1.311855673789978, 1.311785340309143], 'val_acc': [0.17499999701976776, 0.17499999701976776, 0.17499999701976776, 0.17499999701976776, 0.17499999701976776, 0.17499999701976776, 0.17499999701976776, 0.17499999701976776, 0.17499999701976776, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5], 'val_recall_15': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'val_precision_15': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n"
          ]
        }
      ],
      "source": [
        "print(train_history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "pLjALP5ycmSe",
        "outputId": "bac1e3ac-33e1-4d36-e24b-9849607567bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Validation Loss')"
            ]
          },
          "metadata": {},
          "execution_count": 108
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(train_history.history['val_loss'])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "rX-pc4S4d-br",
        "outputId": "2df2b3da-db8b-429e-ac16-cebd868dee21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Validation Accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 109
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(train_history.history['val_acc'])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Pbvs_fZ7ZEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}