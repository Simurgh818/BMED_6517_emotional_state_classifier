{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simurgh818/BMED_6517_emotional_state_classifier/blob/main/DEAP_CNN_DataAugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1: Clone repo and get dataset"
      ],
      "metadata": {
        "id": "hBIzO3XHML7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKKDN0tIXKTt",
        "outputId": "b88fbf0c-a4e8-41bb-eeae-29c383a55636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-04 16:29:47--  https://drive.google.com/drive/folders/1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.24.102, 74.125.24.100, 74.125.24.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.24.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing’\n",
            "\n",
            "1_9n-kRKkpnCC2wVovO     [  <=>               ] 221.04K   761KB/s    in 0.3s    \n",
            "\n",
            "2022-12-04 16:29:47 (761 KB/s) - ‘1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing’ saved [226340]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importing the preprocessed dataset files\n",
        "!wget https://drive.google.com/drive/folders/1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTiKd_g15Kl0",
        "outputId": "d693f6ab-3b8c-4a6f-e674-4743e78c9d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "     0K .......... .......... .......... .......... .......... 11.2M\n",
            "    50K .......... .......... .......... .......... .......... 8.72M\n",
            "   100K .......... .......... .......... .......... .......... 22.5M\n",
            "   150K .......... .......... ..                               11.7M=0.01sCloning into 'BMED_6517_emotional_state_classifier'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 139 (delta 6), reused 12 (delta 1), pack-reused 114\u001b[K\n",
            "Receiving objects: 100% (139/139), 78.37 MiB | 16.39 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "#importing our git repo\n",
        "import os\n",
        "if not os.path.exists('/content/BMED_6517_emotional_state_classifier'):\n",
        "  !wget https://github.com/Simurgh818/BMED_6517_emotional_state_classifier/blob/main/requirements.txt -q --show-progress --progress=dot\n",
        "  !git clone https://github.com/Simurgh818/BMED_6517_emotional_state_classifier.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPTiWGmT7qwu",
        "outputId": "49cb5d3b-bb87-46ba-9a96-dbf72baa2838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(880, 5, 32, 32)\n",
            "(880, 5, 512)\n",
            "(880, 32, 6)\n",
            "(880,)\n",
            "(880,)\n",
            "(880,)\n"
          ]
        }
      ],
      "source": [
        "# import data from numpy arrays\n",
        "import numpy as np\n",
        "\n",
        "loaded_features = np.load('/content/BMED_6517_emotional_state_classifier/results/npy/EEG_features.npy', allow_pickle=True)\n",
        "\n",
        "connectivityMatrix = loaded_features.item().get('connectivity_matrix')\n",
        "connectivityLinear = loaded_features.item().get('connectivity_linear')\n",
        "wavelet = loaded_features.item().get('waveletEntropy')\n",
        "Valence = loaded_features.item().get('Valence')\n",
        "Arousal = loaded_features.item().get('Arousal')\n",
        "Classes = loaded_features.item().get('Classes')\n",
        "\n",
        "print(connectivityMatrix.shape)\n",
        "print(connectivityLinear.shape)\n",
        "print(wavelet.shape)\n",
        "print(Valence.shape)\n",
        "print(Arousal.shape)\n",
        "print(Classes.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0N8fbUuUbqYp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Importing Deep Learning Libraries\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
        "from keras.models import Model,Sequential\n",
        "from keras.optimizers import Adam,SGD,RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8YfwL18POR_d"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l2teHjzaOeHr"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "#dataset_labels = np.load('content/gdrive/MyDrive/Colab Notebooks/Copy of labels_1_22.npy', mmap_mode='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ofakMc12O3EG"
      },
      "outputs": [],
      "source": [
        "#dataset_name1 = 'Copy of bipolar_feats.npy'\n",
        "\n",
        "#dataset_bipolarfts = np.load(dataset_name1, encoding='bytes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YYNle4ynO_uM"
      },
      "outputs": [],
      "source": [
        "#dataset_name2 = 'Copy of labels_1_22.npy'\n",
        "\n",
        "#dataset_labels = np.load(dataset_name2, encoding='bytes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q3o_RfZLPRoI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phTMsRTyi6T8",
        "outputId": "8789b845-17bd-4602-9943-ed22bce724ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(880,)\n",
            "[0. 1. 2. 3.]\n"
          ]
        }
      ],
      "source": [
        "print(np.shape(Classes))\n",
        "print(np.unique(Classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2: class balancing"
      ],
      "metadata": {
        "id": "O78d6tyWMFE6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bStEfQ5h4t1",
        "outputId": "85aa9ac7-3175-41dd-9fae-0b0ab8e4db42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[172]\n"
          ]
        }
      ],
      "source": [
        "# Class balance check:\n",
        "labels = ['Classes']\n",
        "zeros = [Classes[Classes == 0].shape[0]]\n",
        "ones = [Classes[Classes == 1].shape[0]]\n",
        "twos = [Classes[Classes == 2].shape[0]]\n",
        "threes = [Classes[Classes == 3].shape[0]]\n",
        "\n",
        "x = np.arange(1)  # the label locations\n",
        "width = 0.25  # the width of the bars\n",
        "\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "eq2q6c7NniMF",
        "outputId": "13a72625-96b8-4aeb-b337-acdfb870d7ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - 1.5*width, zeros, width, label='zeros')\n",
        "rects2 = ax.bar(x - width/2, ones, width, label='ones')\n",
        "rects3 = ax.bar(x + width/2, twos, width, label='twos')\n",
        "rects4 = ax.bar(x + 1.5*width, threes, width, label='threes')\n",
        "\n",
        "ax.set_title('The numbers of each class')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACiGDaBJh1YO",
        "outputId": "9a2fd690-7cb0-4433-84a2-a571beb56a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(688,) \n",
            " 128*4 is:  512\n",
            "(688, 5, 32, 32)\n"
          ]
        }
      ],
      "source": [
        "# Rebalance classes based on class 1 = 172: Take the first 172 from class 0,2 and 3.\n",
        "zero_mask = (Classes == 0)\n",
        "one_mask = (Classes == 1)\n",
        "two_mask = (Classes == 2)\n",
        "three_mask = (Classes == 3)\n",
        "size = 172\n",
        "\n",
        "Classes_bl = np.concatenate([Classes[zero_mask][:size],Classes[one_mask][:size],\n",
        "                            Classes[two_mask][:size],Classes[three_mask][:size]])\n",
        "print(np.shape(Classes_bl),'\\n 128*4 is: ', 128*4)\n",
        "\n",
        "connectivityMatrix_bl = np.concatenate([connectivityMatrix[zero_mask][:size],connectivityMatrix[one_mask][:size],\n",
        "                            connectivityMatrix[two_mask][:size],connectivityMatrix[three_mask][:size]])\n",
        "print(np.shape(connectivityMatrix_bl))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Classes_bl[Classes_bl == 0].shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UntIH3oLMy1",
        "outputId": "0470ded9-9d7d-4db0-a54c-2845d248d59c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVukc0CcuTdR",
        "outputId": "7d47fe67-c378-4a22-b7ca-c7a94ec68189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valence shape is:  (688,)\n",
            "Arousal shape is:  (688,)\n"
          ]
        }
      ],
      "source": [
        "# balancing valence and arrousal\n",
        "Valence_bl = np.concatenate([Valence[zero_mask][:size],Valence[one_mask][:size],\n",
        "                            Valence[two_mask][:size],Valence[three_mask][:size]])\n",
        "print('Valence shape is: ', np.shape(Valence_bl))\n",
        "\n",
        "Arousal_bl = np.concatenate([Arousal[zero_mask][:size],Arousal[one_mask][:size],\n",
        "                            Arousal[two_mask][:size],Arousal[three_mask][:size]])\n",
        "print('Arousal shape is: ', np.shape(Arousal_bl))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zeros_bl = [Classes_bl[Classes_bl == 0].shape[0]]\n",
        "ones_bl = [Classes_bl[Classes_bl == 1].shape[0]]\n",
        "twos_bl = [Classes_bl[Classes_bl == 2].shape[0]]\n",
        "threes_bl = [Classes_bl[Classes_bl == 3].shape[0]]\n",
        "x = np.arange(1)  # the label locations\n",
        "fig2, ax2 = plt.subplots()\n",
        "rects1 = ax2.bar(x - 1.5*width, zeros_bl, width, label='zeros_bl')\n",
        "rects2 = ax2.bar(x - width/2, ones_bl, width, label='ones_bl')\n",
        "rects3 = ax2.bar(x + width/2, twos_bl, width, label='twos_bl')\n",
        "rects4 = ax2.bar(x + 1.5*width, threes_bl, width, label='threes_bl')\n",
        "\n",
        "ax2.set_title('The numbers of each class')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(labels)\n",
        "ax2.legend()\n",
        "\n",
        "fig2.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "dfzl5-1-J_Uf",
        "outputId": "789bd049-8a46-428e-f10c-5cef5901ba99"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3: CNN Model"
      ],
      "metadata": {
        "id": "R4t43f_NNH_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyU4fNdmCL-l",
        "outputId": "34f9c3cf-49f6-4b7b-b70e-5843ccffbd22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 2. 3.]\n",
            "(688,)\n",
            "[0. 1.]\n",
            "(688, 4)\n",
            "[116. 164. 176. 232.]\n"
          ]
        }
      ],
      "source": [
        "# Algorithm 1 for Convolutional Neural Model :\n",
        "##Require: Training EEG Dataset nntrX, Training Valence/Arousal Values nntrY, Testing subject’s EEG\n",
        "#Dataset nnteX, Testing Valence/Arousal Values nnteY\n",
        "# cnn = model(trainX, trainY )\n",
        "\n",
        "#x = dataset_bipolarfts\n",
        "x = connectivityMatrix_bl\n",
        "#y = dataset_labels[1:881]\n",
        "y = np.vstack([Valence_bl,Arousal_bl]).T\n",
        "#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
        "y[np.where(y<4.5)] = 0\n",
        "y[np.where(y>=4.5)] = 1\n",
        "y_4_class = y[:,0]*2+y[:,1]\n",
        "print(np.unique(y_4_class))\n",
        "print(y_4_class.shape)\n",
        "y_one_hot = np.zeros((y.shape[0],4))\n",
        "y_one_hot[np.where(y_4_class==0),0] = 1\n",
        "y_one_hot[np.where(y_4_class==1),1] = 1\n",
        "y_one_hot[np.where(y_4_class==2),2] = 1\n",
        "y_one_hot[np.where(y_4_class==3),3] = 1\n",
        "print(np.unique(y_one_hot))\n",
        "print(y_one_hot.shape)\n",
        "print(np.sum(y_one_hot,axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dgqnDGha6IE",
        "outputId": "4eed15d6-c8b3-4cc1-b43c-dbc5c9a96906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(160, 5, 32, 32)\n",
            "(528, 5, 32, 32)\n",
            "(528, 4)\n"
          ]
        }
      ],
      "source": [
        "val_size = 4\n",
        "y_test = y_one_hot[:val_size*40,:]\n",
        "x_test = x[:val_size*40,:,:,:]\n",
        "y_train = y_one_hot[val_size*40:,:]\n",
        "x_train = x[val_size*40:,:,:,:]\n",
        "print(x_test.shape)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "1zFYJl5heF2R"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation, concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.metrics import CategoricalAccuracy,CategoricalCrossentropy,Precision,Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "gc1zs7Lfex41"
      },
      "outputs": [],
      "source": [
        "from keras.layers.activation.relu import ReLU\n",
        "def make_CNN_layers(input_mat):\n",
        "  # layer 1\n",
        "  model = Conv2D(16,(3,3),padding='same',input_shape=input_shape)(input_mat)\n",
        "  model = ReLU()(model)\n",
        "  model = MaxPooling2D((2,2),padding='same')(model)\n",
        "  model = Dropout(0.2)(model)\n",
        "\n",
        "  # layer 2\n",
        "  model = Conv2D(32,(3,3),padding='same')(model)\n",
        "  model = ReLU()(model)\n",
        "  model = MaxPooling2D((2,2),padding='same')(model)\n",
        "  model = Dropout(0.2)(model)\n",
        "\n",
        "  #layer 3\n",
        "  model = Conv2D(64,(3,3),padding='same')(model)\n",
        "  model = ReLU()(model)\n",
        "  model = MaxPooling2D((2,2),padding='same')(model)\n",
        "  model = Dropout(0.5)(model)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "djeO0LSKdSAQ"
      },
      "outputs": [],
      "source": [
        "input_shape = x_test[0,0].shape\n",
        "input_shape = (32,32,1)\n",
        "delta_input = Input(shape = input_shape)\n",
        "delta_model = make_CNN_layers(delta_input)\n",
        "\n",
        "theta_input = Input(shape = input_shape)\n",
        "theta_model = make_CNN_layers(theta_input)\n",
        "\n",
        "alpha_input = Input(shape = input_shape)\n",
        "alpha_model = make_CNN_layers(alpha_input)\n",
        "\n",
        "beta_input = Input(shape = input_shape)\n",
        "beta_model = make_CNN_layers(beta_input)\n",
        "\n",
        "gamma_input = Input(shape = input_shape)\n",
        "gamma_model = make_CNN_layers(gamma_input)\n",
        "\n",
        "conv = concatenate([delta_model,theta_model,alpha_model,beta_model,gamma_model])\n",
        "\n",
        "conv = Flatten()(conv)\n",
        "\n",
        "dense = Dense(512)(conv)\n",
        "dense = ReLU()(dense)\n",
        "dense = Dropout(0.3)(dense)\n",
        "\n",
        "output = Dense(4,activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=[delta_input,theta_input,alpha_input,beta_input,gamma_input],\n",
        "              outputs=[output])\n",
        "\n",
        "opt = optimizers.SGD(learning_rate=1e-2, momentum=0)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['acc',Recall(),Precision()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sspGx1ekjYhR",
        "outputId": "274a8e06-b0e4-4c4f-ca1a-b4b6a392e84c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_12\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_61 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_62 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_63 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_64 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_65 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_180 (Conv2D)            (None, 32, 32, 16)   160         ['input_61[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_183 (Conv2D)            (None, 32, 32, 16)   160         ['input_62[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_186 (Conv2D)            (None, 32, 32, 16)   160         ['input_63[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_189 (Conv2D)            (None, 32, 32, 16)   160         ['input_64[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_192 (Conv2D)            (None, 32, 32, 16)   160         ['input_65[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_192 (ReLU)               (None, 32, 32, 16)   0           ['conv2d_180[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_195 (ReLU)               (None, 32, 32, 16)   0           ['conv2d_183[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_198 (ReLU)               (None, 32, 32, 16)   0           ['conv2d_186[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_201 (ReLU)               (None, 32, 32, 16)   0           ['conv2d_189[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_204 (ReLU)               (None, 32, 32, 16)   0           ['conv2d_192[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_180 (MaxPooling2  (None, 16, 16, 16)  0           ['re_lu_192[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_183 (MaxPooling2  (None, 16, 16, 16)  0           ['re_lu_195[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_186 (MaxPooling2  (None, 16, 16, 16)  0           ['re_lu_198[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_189 (MaxPooling2  (None, 16, 16, 16)  0           ['re_lu_201[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_192 (MaxPooling2  (None, 16, 16, 16)  0           ['re_lu_204[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_192 (Dropout)          (None, 16, 16, 16)   0           ['max_pooling2d_180[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_195 (Dropout)          (None, 16, 16, 16)   0           ['max_pooling2d_183[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_198 (Dropout)          (None, 16, 16, 16)   0           ['max_pooling2d_186[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_201 (Dropout)          (None, 16, 16, 16)   0           ['max_pooling2d_189[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_204 (Dropout)          (None, 16, 16, 16)   0           ['max_pooling2d_192[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_181 (Conv2D)            (None, 16, 16, 32)   4640        ['dropout_192[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_184 (Conv2D)            (None, 16, 16, 32)   4640        ['dropout_195[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_187 (Conv2D)            (None, 16, 16, 32)   4640        ['dropout_198[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_190 (Conv2D)            (None, 16, 16, 32)   4640        ['dropout_201[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_193 (Conv2D)            (None, 16, 16, 32)   4640        ['dropout_204[0][0]']            \n",
            "                                                                                                  \n",
            " re_lu_193 (ReLU)               (None, 16, 16, 32)   0           ['conv2d_181[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_196 (ReLU)               (None, 16, 16, 32)   0           ['conv2d_184[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_199 (ReLU)               (None, 16, 16, 32)   0           ['conv2d_187[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_202 (ReLU)               (None, 16, 16, 32)   0           ['conv2d_190[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_205 (ReLU)               (None, 16, 16, 32)   0           ['conv2d_193[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_181 (MaxPooling2  (None, 8, 8, 32)    0           ['re_lu_193[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_184 (MaxPooling2  (None, 8, 8, 32)    0           ['re_lu_196[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_187 (MaxPooling2  (None, 8, 8, 32)    0           ['re_lu_199[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_190 (MaxPooling2  (None, 8, 8, 32)    0           ['re_lu_202[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_193 (MaxPooling2  (None, 8, 8, 32)    0           ['re_lu_205[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_193 (Dropout)          (None, 8, 8, 32)     0           ['max_pooling2d_181[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_196 (Dropout)          (None, 8, 8, 32)     0           ['max_pooling2d_184[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_199 (Dropout)          (None, 8, 8, 32)     0           ['max_pooling2d_187[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_202 (Dropout)          (None, 8, 8, 32)     0           ['max_pooling2d_190[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_205 (Dropout)          (None, 8, 8, 32)     0           ['max_pooling2d_193[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_182 (Conv2D)            (None, 8, 8, 64)     18496       ['dropout_193[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_185 (Conv2D)            (None, 8, 8, 64)     18496       ['dropout_196[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_188 (Conv2D)            (None, 8, 8, 64)     18496       ['dropout_199[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_191 (Conv2D)            (None, 8, 8, 64)     18496       ['dropout_202[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_194 (Conv2D)            (None, 8, 8, 64)     18496       ['dropout_205[0][0]']            \n",
            "                                                                                                  \n",
            " re_lu_194 (ReLU)               (None, 8, 8, 64)     0           ['conv2d_182[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_197 (ReLU)               (None, 8, 8, 64)     0           ['conv2d_185[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_200 (ReLU)               (None, 8, 8, 64)     0           ['conv2d_188[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_203 (ReLU)               (None, 8, 8, 64)     0           ['conv2d_191[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_206 (ReLU)               (None, 8, 8, 64)     0           ['conv2d_194[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_182 (MaxPooling2  (None, 4, 4, 64)    0           ['re_lu_194[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_185 (MaxPooling2  (None, 4, 4, 64)    0           ['re_lu_197[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_188 (MaxPooling2  (None, 4, 4, 64)    0           ['re_lu_200[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_191 (MaxPooling2  (None, 4, 4, 64)    0           ['re_lu_203[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " max_pooling2d_194 (MaxPooling2  (None, 4, 4, 64)    0           ['re_lu_206[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout_194 (Dropout)          (None, 4, 4, 64)     0           ['max_pooling2d_182[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_197 (Dropout)          (None, 4, 4, 64)     0           ['max_pooling2d_185[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_200 (Dropout)          (None, 4, 4, 64)     0           ['max_pooling2d_188[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_203 (Dropout)          (None, 4, 4, 64)     0           ['max_pooling2d_191[0][0]']      \n",
            "                                                                                                  \n",
            " dropout_206 (Dropout)          (None, 4, 4, 64)     0           ['max_pooling2d_194[0][0]']      \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 4, 4, 320)    0           ['dropout_194[0][0]',            \n",
            "                                                                  'dropout_197[0][0]',            \n",
            "                                                                  'dropout_200[0][0]',            \n",
            "                                                                  'dropout_203[0][0]',            \n",
            "                                                                  'dropout_206[0][0]']            \n",
            "                                                                                                  \n",
            " flatten_12 (Flatten)           (None, 5120)         0           ['concatenate_12[0][0]']         \n",
            "                                                                                                  \n",
            " dense_24 (Dense)               (None, 512)          2621952     ['flatten_12[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_207 (ReLU)               (None, 512)          0           ['dense_24[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_207 (Dropout)          (None, 512)          0           ['re_lu_207[0][0]']              \n",
            "                                                                                                  \n",
            " dense_25 (Dense)               (None, 4)            2052        ['dropout_207[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,740,484\n",
            "Trainable params: 2,740,484\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_delta = x_train[:,0,:,:]\n",
        "x_theta = x_train[:,1,:,:]\n",
        "x_alpha = x_train[:,2,:,:]\n",
        "x_beta = x_train[:,3,:,:]\n",
        "x_gamma = x_train[:,4,:,:]\n",
        "validation_data=([x_delta[:40],x_theta[:40],x_alpha[:40],x_beta[:40],x_gamma[:40]],y_train[:40])"
      ],
      "metadata": {
        "id": "n0lyC-YhphlO"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.1: Data Augmentation"
      ],
      "metadata": {
        "id": "hKQRxNcZMXBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation: cropping\n",
        "x_delta = x_train[:,0,:,:]\n",
        "print(np.shape(x_delta))\n",
        "rnd_idx = int(np.round(np.random.rand(1)*100)[0])\n",
        "print(rnd_idx)\n",
        "x_delta_clip = x_train[rnd_idx:rnd_idx+100,0,:,:]\n",
        "print(np.shape(x_delta_clip))\n",
        "x_delta_aug = np.append(x_delta,x_delta_clip, axis=0)\n",
        "print(np.shape(x_delta_aug))\n",
        "\n",
        "x_theta = x_train[:,1,:,:]\n",
        "x_theta_clip = x_train[rnd_idx:rnd_idx+100,1,:,:]\n",
        "x_theta_aug = np.append(x_theta,x_theta_clip, axis=0)\n",
        "\n",
        "x_alpha = x_train[:,2,:,:]\n",
        "x_alpha_clip = x_train[rnd_idx:rnd_idx+100,2,:,:]\n",
        "x_alpha_aug = np.append(x_alpha,x_alpha_clip, axis=0)\n",
        "\n",
        "x_beta = x_train[:,3,:,:]\n",
        "x_beta_clip = x_train[rnd_idx:rnd_idx+100,3,:,:]\n",
        "x_beta_aug = np.append(x_beta,x_beta_clip, axis=0)\n",
        "\n",
        "x_gamma = x_train[:,4,:,:]\n",
        "x_gamma_clip = x_train[rnd_idx:rnd_idx+100,4,:,:]\n",
        "x_gamma_aug = np.append(x_gamma,x_gamma_clip, axis=0)\n",
        "\n",
        "y_clip = y_train[rnd_idx:rnd_idx+100]\n",
        "y_train_aug = np.append(y_train,y_clip, axis=0)\n",
        "print(np.shape(y_train_aug),'\\n', y_train_aug[-1,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaaW295Zjp0E",
        "outputId": "adbe07d9-ec78-40a7-e21b-65a839675c30"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(528, 32, 32)\n",
            "13\n",
            "(100, 32, 32)\n",
            "(628, 32, 32)\n",
            "(628, 4) \n",
            " [0. 0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "x_plt = np.arange(0,32*32)\n",
        "plt.subplot(1,5,1)\n",
        "plt.plot(x_plt, x_delta_aug[0,:,:].ravel());\n",
        "plt.subplot(1,5,2)\n",
        "plt.plot(x_plt, x_theta_aug[0,:,:].ravel());\n",
        "plt.subplot(1,5,3)\n",
        "plt.plot(x_plt, x_alpha_aug[0,:,:].ravel());\n",
        "plt.subplot(1,5,4)\n",
        "plt.plot(x_plt, x_beta_aug[0,:,:].ravel());\n",
        "plt.subplot(1,5,5)\n",
        "plt.plot(x_plt, x_gamma_aug[0,:,:].ravel());"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "K46-VTOENUYS",
        "outputId": "15ec76e2-dcc7-44f6-919c-a03928cdaa24"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Augmentation: flipping\n",
        "\n",
        "x_delta = x_train[:,0,:,:]\n",
        "print(np.shape(x_delta))\n",
        "rnd_idx2 = int(np.round(np.random.rand(1)*100)[0])\n",
        "print(rnd_idx2)\n",
        "x_delta_clip2 = x_train[rnd_idx2:rnd_idx2+100,0,:,:]\n",
        "print(np.shape(x_delta_clip2))\n",
        "x_theta_clip2 = x_train[rnd_idx2:rnd_idx2+100,1,:,:]\n",
        "x_alpha_clip2 = x_train[rnd_idx2:rnd_idx2+100,2,:,:]\n",
        "x_beta_clip2 = x_train[rnd_idx2:rnd_idx2+100,3,:,:]\n",
        "x_gamma_clip2 = x_train[rnd_idx2:rnd_idx2+100,4,:,:]\n",
        "\n",
        "x_size = np.shape(x_delta_clip2)[0]\n",
        "x_delta_flip = np.empty((x_size, 32,32))\n",
        "x_theta_flip = np.empty((x_size, 32,32))\n",
        "x_alpha_flip = np.empty((x_size, 32,32))\n",
        "x_beta_flip = np.empty((x_size, 32,32))\n",
        "x_gamma_flip = np.empty((x_size, 32,32))\n",
        "\n",
        "for f in range(0, x_size):\n",
        "  x_delta_fl = np.flip(x_delta_clip2[f,:,:].ravel())\n",
        "  x_delta_flip[f,:,:] = np.reshape(x_delta_fl, (32,32))\n",
        "\n",
        "  x_theta_fl = np.flip(x_theta_clip2[f,:,:].ravel())\n",
        "  x_theta_flip[f,:,:] = np.reshape(x_theta_fl, (32,32))\n",
        "\n",
        "  x_alpha_fl = np.flip(x_alpha_clip2[f,:,:].ravel())\n",
        "  x_alpha_flip[f,:,:] = np.reshape(x_alpha_fl, (32,32))\n",
        "\n",
        "  x_beta_fl = np.flip(x_beta_clip2[f,:,:].ravel())\n",
        "  x_beta_flip[f,:,:] = np.reshape(x_beta_fl, (32,32))\n",
        "\n",
        "  x_gamma_fl = np.flip(x_gamma_clip2[f,:,:].ravel())\n",
        "  x_gamma_flip[f,:,:] = np.reshape(x_gamma_fl, (32,32))\n",
        "\n",
        "print(np.shape(x_delta_flip))\n",
        "\n",
        "plt.figure(figsize=(20,4))\n",
        "x_plt = np.arange(0,32*32)\n",
        "plt.subplot(1,5,1)\n",
        "plt.plot(x_plt, x_delta_aug[0,:,:].ravel());\n",
        "plt.subplot(1,5,2)\n",
        "plt.plot(x_plt, x_delta_flip[0,:,:].ravel());\n",
        "\n",
        "x_delta_aug2 = np.append(x_delta_aug,x_delta_flip, axis=0)\n",
        "print(np.shape(x_delta_aug2))\n",
        "x_theta_aug2 = np.append(x_theta_aug,x_theta_flip, axis=0)\n",
        "x_alpha_aug2 = np.append(x_alpha_aug,x_alpha_flip, axis=0)\n",
        "x_beta_aug2 = np.append(x_beta_aug,x_beta_flip, axis=0)\n",
        "x_gamma_aug2 = np.append(x_gamma_aug,x_gamma_flip, axis=0)\n",
        "\n",
        "y_clip2 = y_train[rnd_idx2:rnd_idx2+100]\n",
        "\n",
        "y_train_aug2 = np.append(y_train_aug,y_clip2, axis=0)\n",
        "print(np.shape(y_train_aug2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "wVeha0tKPtDH",
        "outputId": "3032778b-94f8-4d97-d029-8353bc9e213e"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(528, 32, 32)\n",
            "30\n",
            "(100, 32, 32)\n",
            "(100, 32, 32)\n",
            "(728, 32, 32)\n",
            "(728, 4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FutIrekXjult",
        "outputId": "a1254555-9b1a-495f-ecaf-8215ff4f27de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.1745 - acc: 0.3922 - recall_12: 0.0500 - precision_12: 0.4638\n",
            "Epoch 1: val_loss improved from inf to 1.22515, saving model to weights.best.hdf5\n",
            "22/22 [==============================] - 3s 42ms/step - loss: 1.1688 - acc: 0.3953 - recall_12: 0.0610 - precision_12: 0.4941 - val_loss: 1.2251 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 2/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.1127 - acc: 0.4062 - recall_12: 0.0952 - precision_12: 0.3765\n",
            "Epoch 2: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.1126 - acc: 0.4070 - recall_12: 0.0945 - precision_12: 0.3714 - val_loss: 1.2613 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 3/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.1241 - acc: 0.3795 - recall_12: 0.1042 - precision_12: 0.3846\n",
            "Epoch 3: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.1225 - acc: 0.3794 - recall_12: 0.1032 - precision_12: 0.3880 - val_loss: 1.2886 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 4/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.1002 - acc: 0.3889 - recall_12: 0.0972 - precision_12: 0.4480\n",
            "Epoch 4: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0993 - acc: 0.3939 - recall_12: 0.1017 - precision_12: 0.4636 - val_loss: 1.3153 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 5/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0958 - acc: 0.4125 - recall_12: 0.0906 - precision_12: 0.4085\n",
            "Epoch 5: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0955 - acc: 0.4128 - recall_12: 0.0872 - precision_12: 0.4054 - val_loss: 1.2925 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 6/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.1007 - acc: 0.4084 - recall_12: 0.0945 - precision_12: 0.4333\n",
            "Epoch 6: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.1007 - acc: 0.4084 - recall_12: 0.0945 - precision_12: 0.4333 - val_loss: 1.3477 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 7/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.1001 - acc: 0.4084 - recall_12: 0.0770 - precision_12: 0.4454\n",
            "Epoch 7: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.1001 - acc: 0.4084 - recall_12: 0.0770 - precision_12: 0.4454 - val_loss: 1.3198 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 8/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0890 - acc: 0.3906 - recall_12: 0.0799 - precision_12: 0.3710\n",
            "Epoch 8: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0891 - acc: 0.3968 - recall_12: 0.0741 - precision_12: 0.3723 - val_loss: 1.3449 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 9/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0927 - acc: 0.4070 - recall_12: 0.0814 - precision_12: 0.4308\n",
            "Epoch 9: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0927 - acc: 0.4070 - recall_12: 0.0814 - precision_12: 0.4308 - val_loss: 1.3825 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 10/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0852 - acc: 0.4099 - recall_12: 0.0683 - precision_12: 0.4796\n",
            "Epoch 10: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0852 - acc: 0.4099 - recall_12: 0.0683 - precision_12: 0.4796 - val_loss: 1.3574 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 11/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0878 - acc: 0.4241 - recall_12: 0.0833 - precision_12: 0.4667\n",
            "Epoch 11: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0905 - acc: 0.4201 - recall_12: 0.0828 - precision_12: 0.4597 - val_loss: 1.3804 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 12/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.1001 - acc: 0.3735 - recall_12: 0.0521 - precision_12: 0.3977\n",
            "Epoch 12: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0999 - acc: 0.3750 - recall_12: 0.0538 - precision_12: 0.3936 - val_loss: 1.3731 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 13/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0832 - acc: 0.4241 - recall_12: 0.0387 - precision_12: 0.4062\n",
            "Epoch 13: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0831 - acc: 0.4244 - recall_12: 0.0392 - precision_12: 0.3971 - val_loss: 1.3301 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 14/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0850 - acc: 0.4077 - recall_12: 0.0729 - precision_12: 0.4261\n",
            "Epoch 14: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0843 - acc: 0.4055 - recall_12: 0.0741 - precision_12: 0.4250 - val_loss: 1.3734 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 15/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0779 - acc: 0.3958 - recall_12: 0.0565 - precision_12: 0.4524\n",
            "Epoch 15: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0745 - acc: 0.4026 - recall_12: 0.0581 - precision_12: 0.4598 - val_loss: 1.3397 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 16/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0809 - acc: 0.4360 - recall_12: 0.0756 - precision_12: 0.4298\n",
            "Epoch 16: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0809 - acc: 0.4360 - recall_12: 0.0756 - precision_12: 0.4298 - val_loss: 1.3611 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 17/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0853 - acc: 0.3953 - recall_12: 0.0741 - precision_12: 0.4286\n",
            "Epoch 17: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0853 - acc: 0.3953 - recall_12: 0.0741 - precision_12: 0.4286 - val_loss: 1.3960 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 18/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0935 - acc: 0.3924 - recall_12: 0.0363 - precision_12: 0.3425\n",
            "Epoch 18: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0935 - acc: 0.3924 - recall_12: 0.0363 - precision_12: 0.3425 - val_loss: 1.3956 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 19/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0784 - acc: 0.4041 - recall_12: 0.0363 - precision_12: 0.3521\n",
            "Epoch 19: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0784 - acc: 0.4041 - recall_12: 0.0363 - precision_12: 0.3521 - val_loss: 1.4069 - val_acc: 0.7000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 20/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0766 - acc: 0.4070 - recall_12: 0.0523 - precision_12: 0.5000\n",
            "Epoch 20: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0766 - acc: 0.4070 - recall_12: 0.0523 - precision_12: 0.5000 - val_loss: 1.4429 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 21/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0893 - acc: 0.4046 - recall_12: 0.0132 - precision_12: 0.2759\n",
            "Epoch 21: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.0872 - acc: 0.4099 - recall_12: 0.0160 - precision_12: 0.2821 - val_loss: 1.3626 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 22/500\n",
            "17/22 [======================>.......] - ETA: 0s - loss: 1.0822 - acc: 0.4136 - recall_12: 0.0919 - precision_12: 0.4630\n",
            "Epoch 22: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0788 - acc: 0.4099 - recall_12: 0.0741 - precision_12: 0.4595 - val_loss: 1.3886 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 23/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0755 - acc: 0.4273 - recall_12: 0.0407 - precision_12: 0.4375\n",
            "Epoch 23: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0755 - acc: 0.4273 - recall_12: 0.0407 - precision_12: 0.4375 - val_loss: 1.3882 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 24/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0757 - acc: 0.4302 - recall_12: 0.0785 - precision_12: 0.5143\n",
            "Epoch 24: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0757 - acc: 0.4302 - recall_12: 0.0785 - precision_12: 0.5143 - val_loss: 1.4388 - val_acc: 0.1000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 25/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0623 - acc: 0.4201 - recall_12: 0.0573 - precision_12: 0.4521\n",
            "Epoch 25: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0722 - acc: 0.4142 - recall_12: 0.0480 - precision_12: 0.4400 - val_loss: 1.4634 - val_acc: 0.1000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 26/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0781 - acc: 0.4219 - recall_12: 0.0226 - precision_12: 0.3939\n",
            "Epoch 26: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0802 - acc: 0.4128 - recall_12: 0.0189 - precision_12: 0.3421 - val_loss: 1.3944 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 27/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0728 - acc: 0.4256 - recall_12: 0.0372 - precision_12: 0.4310\n",
            "Epoch 27: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0729 - acc: 0.4230 - recall_12: 0.0378 - precision_12: 0.4333 - val_loss: 1.3968 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 28/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0636 - acc: 0.4253 - recall_12: 0.0347 - precision_12: 0.4255\n",
            "Epoch 28: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0690 - acc: 0.4215 - recall_12: 0.0407 - precision_12: 0.4444 - val_loss: 1.4157 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 29/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0781 - acc: 0.4132 - recall_12: 0.0260 - precision_12: 0.4167\n",
            "Epoch 29: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.0797 - acc: 0.4142 - recall_12: 0.0233 - precision_12: 0.4000 - val_loss: 1.4041 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 30/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0676 - acc: 0.4149 - recall_12: 0.0365 - precision_12: 0.5000\n",
            "Epoch 30: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0740 - acc: 0.4055 - recall_12: 0.0422 - precision_12: 0.4462 - val_loss: 1.4208 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 31/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0761 - acc: 0.4099 - recall_12: 0.0305 - precision_12: 0.4667\n",
            "Epoch 31: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0761 - acc: 0.4099 - recall_12: 0.0305 - precision_12: 0.4667 - val_loss: 1.4067 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 32/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0750 - acc: 0.4182 - recall_12: 0.0312 - precision_12: 0.4286\n",
            "Epoch 32: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0756 - acc: 0.4215 - recall_12: 0.0320 - precision_12: 0.4314 - val_loss: 1.4089 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 33/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0688 - acc: 0.4062 - recall_12: 0.0298 - precision_12: 0.4444\n",
            "Epoch 33: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0701 - acc: 0.4070 - recall_12: 0.0291 - precision_12: 0.4348 - val_loss: 1.4157 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 34/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0781 - acc: 0.4142 - recall_12: 0.0174 - precision_12: 0.3243\n",
            "Epoch 34: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0781 - acc: 0.4142 - recall_12: 0.0174 - precision_12: 0.3243 - val_loss: 1.3916 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 35/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0793 - acc: 0.4041 - recall_12: 0.0334 - precision_12: 0.3538\n",
            "Epoch 35: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0793 - acc: 0.4041 - recall_12: 0.0334 - precision_12: 0.3538 - val_loss: 1.4033 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 36/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0767 - acc: 0.4137 - recall_12: 0.0357 - precision_12: 0.5000\n",
            "Epoch 36: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0743 - acc: 0.4201 - recall_12: 0.0349 - precision_12: 0.5000 - val_loss: 1.4050 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 37/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0759 - acc: 0.4132 - recall_12: 0.0295 - precision_12: 0.4250\n",
            "Epoch 37: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.0754 - acc: 0.4099 - recall_12: 0.0305 - precision_12: 0.4200 - val_loss: 1.4225 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 38/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0712 - acc: 0.4236 - recall_12: 0.0469 - precision_12: 0.5510\n",
            "Epoch 38: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0690 - acc: 0.4317 - recall_12: 0.0392 - precision_12: 0.5510 - val_loss: 1.4325 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 39/500\n",
            "17/22 [======================>.......] - ETA: 0s - loss: 1.0699 - acc: 0.4265 - recall_12: 0.0312 - precision_12: 0.5000\n",
            "Epoch 39: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0695 - acc: 0.4230 - recall_12: 0.0291 - precision_12: 0.5000 - val_loss: 1.4349 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 40/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0778 - acc: 0.4215 - recall_12: 0.0218 - precision_12: 0.4839\n",
            "Epoch 40: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0778 - acc: 0.4215 - recall_12: 0.0218 - precision_12: 0.4839 - val_loss: 1.4283 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 41/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0707 - acc: 0.4115 - recall_12: 0.0312 - precision_12: 0.5806\n",
            "Epoch 41: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0692 - acc: 0.4201 - recall_12: 0.0363 - precision_12: 0.6410 - val_loss: 1.4148 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 42/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0760 - acc: 0.4172 - recall_12: 0.0523 - precision_12: 0.5000\n",
            "Epoch 42: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0760 - acc: 0.4172 - recall_12: 0.0523 - precision_12: 0.5000 - val_loss: 1.4264 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 43/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0714 - acc: 0.4241 - recall_12: 0.0402 - precision_12: 0.6136\n",
            "Epoch 43: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0701 - acc: 0.4244 - recall_12: 0.0392 - precision_12: 0.6136 - val_loss: 1.4194 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 44/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0746 - acc: 0.4286 - recall_12: 0.0417 - precision_12: 0.4746\n",
            "Epoch 44: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0756 - acc: 0.4259 - recall_12: 0.0407 - precision_12: 0.4667 - val_loss: 1.4302 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 45/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0625 - acc: 0.4497 - recall_12: 0.0191 - precision_12: 0.4400\n",
            "Epoch 45: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.0734 - acc: 0.4375 - recall_12: 0.0247 - precision_12: 0.4595 - val_loss: 1.4251 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 46/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0686 - acc: 0.4256 - recall_12: 0.0134 - precision_12: 0.4500\n",
            "Epoch 46: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0695 - acc: 0.4230 - recall_12: 0.0131 - precision_12: 0.4091 - val_loss: 1.4237 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 47/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0696 - acc: 0.4211 - recall_12: 0.0298 - precision_12: 0.5000\n",
            "Epoch 47: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0706 - acc: 0.4215 - recall_12: 0.0291 - precision_12: 0.5000 - val_loss: 1.4139 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 48/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0615 - acc: 0.4259 - recall_12: 0.0901 - precision_12: 0.5299\n",
            "Epoch 48: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0615 - acc: 0.4259 - recall_12: 0.0901 - precision_12: 0.5299 - val_loss: 1.4721 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 49/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0648 - acc: 0.4410 - recall_12: 0.0174 - precision_12: 0.4167\n",
            "Epoch 49: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0642 - acc: 0.4331 - recall_12: 0.0247 - precision_12: 0.5000 - val_loss: 1.4488 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 50/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0755 - acc: 0.4184 - recall_12: 0.0191 - precision_12: 0.3438\n",
            "Epoch 50: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 11ms/step - loss: 1.0715 - acc: 0.4113 - recall_12: 0.0189 - precision_12: 0.3611 - val_loss: 1.4370 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 51/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0681 - acc: 0.4256 - recall_12: 0.0164 - precision_12: 0.3548\n",
            "Epoch 51: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0663 - acc: 0.4317 - recall_12: 0.0160 - precision_12: 0.3548 - val_loss: 1.4155 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 52/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0691 - acc: 0.4302 - recall_12: 0.0756 - precision_12: 0.4561\n",
            "Epoch 52: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0691 - acc: 0.4302 - recall_12: 0.0756 - precision_12: 0.4561 - val_loss: 1.4703 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 53/500\n",
            "17/22 [======================>.......] - ETA: 0s - loss: 1.0777 - acc: 0.4081 - recall_12: 0.0202 - precision_12: 0.3333\n",
            "Epoch 53: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0744 - acc: 0.4201 - recall_12: 0.0203 - precision_12: 0.3889 - val_loss: 1.4655 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 54/500\n",
            "17/22 [======================>.......] - ETA: 0s - loss: 1.0775 - acc: 0.4265 - recall_12: 0.0074 - precision_12: 0.2857\n",
            "Epoch 54: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0698 - acc: 0.4331 - recall_12: 0.0131 - precision_12: 0.4737 - val_loss: 1.4343 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 55/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0672 - acc: 0.4109 - recall_12: 0.0219 - precision_12: 0.3590\n",
            "Epoch 55: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0695 - acc: 0.4113 - recall_12: 0.0203 - precision_12: 0.3500 - val_loss: 1.4431 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 56/500\n",
            "17/22 [======================>.......] - ETA: 0s - loss: 1.0769 - acc: 0.3897 - recall_12: 0.0129 - precision_12: 0.5000\n",
            "Epoch 56: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0749 - acc: 0.4084 - recall_12: 0.0131 - precision_12: 0.3750 - val_loss: 1.4186 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 57/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0661 - acc: 0.4271 - recall_12: 0.0253 - precision_12: 0.4474\n",
            "Epoch 57: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0660 - acc: 0.4259 - recall_12: 0.0247 - precision_12: 0.4474 - val_loss: 1.4275 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 58/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0729 - acc: 0.4286 - recall_12: 0.0208 - precision_12: 0.4828\n",
            "Epoch 58: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0714 - acc: 0.4302 - recall_12: 0.0203 - precision_12: 0.4667 - val_loss: 1.4148 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 59/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0763 - acc: 0.4201 - recall_12: 0.0417 - precision_12: 0.4364\n",
            "Epoch 59: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0699 - acc: 0.4230 - recall_12: 0.0363 - precision_12: 0.4386 - val_loss: 1.4526 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 60/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0760 - acc: 0.4172 - recall_12: 0.0276 - precision_12: 0.4222\n",
            "Epoch 60: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0760 - acc: 0.4172 - recall_12: 0.0276 - precision_12: 0.4222 - val_loss: 1.4536 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 61/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0754 - acc: 0.4256 - recall_12: 0.0208 - precision_12: 0.4516\n",
            "Epoch 61: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0750 - acc: 0.4215 - recall_12: 0.0203 - precision_12: 0.4516 - val_loss: 1.4635 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 62/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0610 - acc: 0.4340 - recall_12: 0.0260 - precision_12: 0.6000\n",
            "Epoch 62: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0673 - acc: 0.4230 - recall_12: 0.0262 - precision_12: 0.6207 - val_loss: 1.4557 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 63/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0687 - acc: 0.4317 - recall_12: 0.0145 - precision_12: 0.6667\n",
            "Epoch 63: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0687 - acc: 0.4317 - recall_12: 0.0145 - precision_12: 0.6667 - val_loss: 1.4058 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 64/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0719 - acc: 0.4375 - recall_12: 0.0387 - precision_12: 0.3611\n",
            "Epoch 64: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0736 - acc: 0.4331 - recall_12: 0.0378 - precision_12: 0.3562 - val_loss: 1.4549 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 65/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0746 - acc: 0.4302 - recall_12: 0.0291 - precision_12: 0.5882\n",
            "Epoch 65: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0746 - acc: 0.4302 - recall_12: 0.0291 - precision_12: 0.5882 - val_loss: 1.4488 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 66/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0690 - acc: 0.4286 - recall_12: 0.0134 - precision_12: 0.4286\n",
            "Epoch 66: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0666 - acc: 0.4331 - recall_12: 0.0131 - precision_12: 0.4286 - val_loss: 1.4435 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 67/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0630 - acc: 0.4226 - recall_12: 0.0223 - precision_12: 0.4688\n",
            "Epoch 67: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0642 - acc: 0.4215 - recall_12: 0.0218 - precision_12: 0.4688 - val_loss: 1.4556 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 68/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0681 - acc: 0.4215 - recall_12: 0.0160 - precision_12: 0.6111\n",
            "Epoch 68: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0681 - acc: 0.4215 - recall_12: 0.0160 - precision_12: 0.6111 - val_loss: 1.4416 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 69/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0646 - acc: 0.4226 - recall_12: 0.0193 - precision_12: 0.5000\n",
            "Epoch 69: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0683 - acc: 0.4215 - recall_12: 0.0203 - precision_12: 0.5185 - val_loss: 1.4394 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 70/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0739 - acc: 0.4152 - recall_12: 0.0149 - precision_12: 0.4000\n",
            "Epoch 70: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0732 - acc: 0.4157 - recall_12: 0.0145 - precision_12: 0.4000 - val_loss: 1.4198 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 71/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0695 - acc: 0.4315 - recall_12: 0.0342 - precision_12: 0.3833\n",
            "Epoch 71: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0719 - acc: 0.4273 - recall_12: 0.0334 - precision_12: 0.3833 - val_loss: 1.4537 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 72/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0730 - acc: 0.4288 - recall_12: 0.0073 - precision_12: 0.2632\n",
            "Epoch 72: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0730 - acc: 0.4288 - recall_12: 0.0073 - precision_12: 0.2632 - val_loss: 1.4311 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 73/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0650 - acc: 0.4259 - recall_12: 0.0276 - precision_12: 0.4130\n",
            "Epoch 73: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0650 - acc: 0.4259 - recall_12: 0.0276 - precision_12: 0.4130 - val_loss: 1.4591 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 74/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0685 - acc: 0.4360 - recall_12: 0.0134 - precision_12: 0.4737\n",
            "Epoch 74: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0705 - acc: 0.4302 - recall_12: 0.0131 - precision_12: 0.4737 - val_loss: 1.4600 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 75/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0581 - acc: 0.4408 - recall_12: 0.0197 - precision_12: 0.8571\n",
            "Epoch 75: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0607 - acc: 0.4375 - recall_12: 0.0174 - precision_12: 0.7500 - val_loss: 1.4693 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 76/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0698 - acc: 0.4137 - recall_12: 0.0060 - precision_12: 0.4444\n",
            "Epoch 76: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0723 - acc: 0.4113 - recall_12: 0.0058 - precision_12: 0.4444 - val_loss: 1.4545 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 77/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0746 - acc: 0.4048 - recall_12: 0.0119 - precision_12: 0.3636\n",
            "Epoch 77: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0720 - acc: 0.4084 - recall_12: 0.0116 - precision_12: 0.3636 - val_loss: 1.4284 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 78/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0691 - acc: 0.4215 - recall_12: 0.0276 - precision_12: 0.5135\n",
            "Epoch 78: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0691 - acc: 0.4215 - recall_12: 0.0276 - precision_12: 0.5135 - val_loss: 1.4578 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 79/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0659 - acc: 0.4345 - recall_12: 0.0104 - precision_12: 0.3333\n",
            "Epoch 79: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0654 - acc: 0.4331 - recall_12: 0.0102 - precision_12: 0.3333 - val_loss: 1.4571 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 80/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0677 - acc: 0.4286 - recall_12: 0.0193 - precision_12: 0.4815\n",
            "Epoch 80: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0687 - acc: 0.4288 - recall_12: 0.0189 - precision_12: 0.4815 - val_loss: 1.4611 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 81/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0627 - acc: 0.4259 - recall_12: 0.0116 - precision_12: 0.5000\n",
            "Epoch 81: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0627 - acc: 0.4259 - recall_12: 0.0116 - precision_12: 0.5000 - val_loss: 1.4605 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 82/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0694 - acc: 0.4345 - recall_12: 0.0119 - precision_12: 0.3810\n",
            "Epoch 82: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0697 - acc: 0.4317 - recall_12: 0.0116 - precision_12: 0.3810 - val_loss: 1.4711 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 83/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0651 - acc: 0.4167 - recall_12: 0.0312 - precision_12: 0.5294\n",
            "Epoch 83: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0653 - acc: 0.4142 - recall_12: 0.0262 - precision_12: 0.5294 - val_loss: 1.4659 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 84/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0638 - acc: 0.4302 - recall_12: 0.0189 - precision_12: 0.4483\n",
            "Epoch 84: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0638 - acc: 0.4302 - recall_12: 0.0189 - precision_12: 0.4483 - val_loss: 1.4750 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 85/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0680 - acc: 0.4211 - recall_12: 0.0045 - precision_12: 0.3000\n",
            "Epoch 85: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0671 - acc: 0.4215 - recall_12: 0.0044 - precision_12: 0.3000 - val_loss: 1.4589 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 86/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0719 - acc: 0.4107 - recall_12: 0.0149 - precision_12: 0.4167\n",
            "Epoch 86: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0726 - acc: 0.4099 - recall_12: 0.0145 - precision_12: 0.4000 - val_loss: 1.4633 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 87/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0715 - acc: 0.4186 - recall_12: 0.0160 - precision_12: 0.4583\n",
            "Epoch 87: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0715 - acc: 0.4186 - recall_12: 0.0160 - precision_12: 0.4583 - val_loss: 1.4536 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 88/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0641 - acc: 0.4358 - recall_12: 0.0208 - precision_12: 0.4444\n",
            "Epoch 88: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0689 - acc: 0.4302 - recall_12: 0.0174 - precision_12: 0.4444 - val_loss: 1.4580 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 89/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0669 - acc: 0.4301 - recall_12: 0.0253 - precision_12: 0.6071\n",
            "Epoch 89: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0666 - acc: 0.4331 - recall_12: 0.0247 - precision_12: 0.6071 - val_loss: 1.4757 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 90/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0657 - acc: 0.4141 - recall_12: 0.0172 - precision_12: 0.5789\n",
            "Epoch 90: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0665 - acc: 0.4113 - recall_12: 0.0160 - precision_12: 0.5789 - val_loss: 1.4607 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 91/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0696 - acc: 0.4390 - recall_12: 0.0149 - precision_12: 0.5263\n",
            "Epoch 91: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0661 - acc: 0.4433 - recall_12: 0.0145 - precision_12: 0.5263 - val_loss: 1.4423 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 92/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0638 - acc: 0.4273 - recall_12: 0.0305 - precision_12: 0.5000\n",
            "Epoch 92: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0638 - acc: 0.4273 - recall_12: 0.0305 - precision_12: 0.5000 - val_loss: 1.4723 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 93/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0654 - acc: 0.4453 - recall_12: 0.0078 - precision_12: 0.3571\n",
            "Epoch 93: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0627 - acc: 0.4477 - recall_12: 0.0087 - precision_12: 0.3750 - val_loss: 1.4534 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 94/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0701 - acc: 0.4062 - recall_12: 0.0078 - precision_12: 0.2632\n",
            "Epoch 94: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0661 - acc: 0.4172 - recall_12: 0.0102 - precision_12: 0.3182 - val_loss: 1.4485 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 95/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0620 - acc: 0.4405 - recall_12: 0.0565 - precision_12: 0.5067\n",
            "Epoch 95: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0637 - acc: 0.4360 - recall_12: 0.0552 - precision_12: 0.5067 - val_loss: 1.4767 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 96/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0724 - acc: 0.4260 - recall_12: 0.0115 - precision_12: 0.3500\n",
            "Epoch 96: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0719 - acc: 0.4273 - recall_12: 0.0116 - precision_12: 0.3200 - val_loss: 1.4776 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 97/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0598 - acc: 0.4315 - recall_12: 0.0164 - precision_12: 0.5000\n",
            "Epoch 97: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0639 - acc: 0.4244 - recall_12: 0.0160 - precision_12: 0.5000 - val_loss: 1.4875 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 98/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0666 - acc: 0.4211 - recall_12: 0.0060 - precision_12: 0.4000\n",
            "Epoch 98: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0660 - acc: 0.4230 - recall_12: 0.0058 - precision_12: 0.4000 - val_loss: 1.4510 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 99/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0698 - acc: 0.4330 - recall_12: 0.0208 - precision_12: 0.3684\n",
            "Epoch 99: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0698 - acc: 0.4346 - recall_12: 0.0218 - precision_12: 0.3846 - val_loss: 1.4439 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 100/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0636 - acc: 0.4286 - recall_12: 0.0372 - precision_12: 0.5952\n",
            "Epoch 100: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0660 - acc: 0.4259 - recall_12: 0.0363 - precision_12: 0.5952 - val_loss: 1.4703 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 101/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0690 - acc: 0.4435 - recall_12: 0.0149 - precision_12: 0.4545\n",
            "Epoch 101: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0712 - acc: 0.4360 - recall_12: 0.0145 - precision_12: 0.4545 - val_loss: 1.4812 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 102/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0607 - acc: 0.4276 - recall_12: 0.0066 - precision_12: 0.2857\n",
            "Epoch 102: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0645 - acc: 0.4201 - recall_12: 0.0087 - precision_12: 0.3750 - val_loss: 1.4696 - val_acc: 0.7500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 103/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0624 - acc: 0.4182 - recall_12: 0.0119 - precision_12: 0.5000\n",
            "Epoch 103: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0631 - acc: 0.4157 - recall_12: 0.0116 - precision_12: 0.5000 - val_loss: 1.4673 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 104/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0633 - acc: 0.4390 - recall_12: 0.0521 - precision_12: 0.5224\n",
            "Epoch 104: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0608 - acc: 0.4462 - recall_12: 0.0509 - precision_12: 0.5224 - val_loss: 1.4829 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 105/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0669 - acc: 0.4250 - recall_12: 0.0156 - precision_12: 0.5556\n",
            "Epoch 105: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0632 - acc: 0.4302 - recall_12: 0.0145 - precision_12: 0.5556 - val_loss: 1.4374 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 106/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0620 - acc: 0.4301 - recall_12: 0.0521 - precision_12: 0.5469\n",
            "Epoch 106: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0622 - acc: 0.4317 - recall_12: 0.0509 - precision_12: 0.5469 - val_loss: 1.4851 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 107/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0743 - acc: 0.4219 - recall_12: 0.0063 - precision_12: 0.4000\n",
            "Epoch 107: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0708 - acc: 0.4273 - recall_12: 0.0102 - precision_12: 0.4667 - val_loss: 1.4554 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 108/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0686 - acc: 0.4137 - recall_12: 0.0060 - precision_12: 0.3636\n",
            "Epoch 108: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0653 - acc: 0.4172 - recall_12: 0.0058 - precision_12: 0.3333 - val_loss: 1.4379 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 109/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0661 - acc: 0.4250 - recall_12: 0.0219 - precision_12: 0.4516\n",
            "Epoch 109: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0650 - acc: 0.4244 - recall_12: 0.0247 - precision_12: 0.4595 - val_loss: 1.4550 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 110/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0535 - acc: 0.4469 - recall_12: 0.0328 - precision_12: 0.6562\n",
            "Epoch 110: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0563 - acc: 0.4448 - recall_12: 0.0305 - precision_12: 0.6364 - val_loss: 1.4662 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 111/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0632 - acc: 0.4375 - recall_12: 0.0268 - precision_12: 0.6000\n",
            "Epoch 111: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0640 - acc: 0.4375 - recall_12: 0.0262 - precision_12: 0.6000 - val_loss: 1.4703 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 112/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0620 - acc: 0.4360 - recall_12: 0.0253 - precision_12: 0.4146\n",
            "Epoch 112: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0626 - acc: 0.4317 - recall_12: 0.0247 - precision_12: 0.4146 - val_loss: 1.5040 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 113/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0668 - acc: 0.4142 - recall_12: 0.0160 - precision_12: 0.4074\n",
            "Epoch 113: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0668 - acc: 0.4142 - recall_12: 0.0160 - precision_12: 0.4074 - val_loss: 1.4688 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 114/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0626 - acc: 0.4375 - recall_12: 0.0461 - precision_12: 0.5636\n",
            "Epoch 114: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0630 - acc: 0.4331 - recall_12: 0.0451 - precision_12: 0.5536 - val_loss: 1.4980 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 115/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0672 - acc: 0.4122 - recall_12: 0.0179 - precision_12: 0.4444\n",
            "Epoch 115: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0688 - acc: 0.4128 - recall_12: 0.0174 - precision_12: 0.4286 - val_loss: 1.4787 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 116/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0609 - acc: 0.4315 - recall_12: 0.0134 - precision_12: 0.4737\n",
            "Epoch 116: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0601 - acc: 0.4346 - recall_12: 0.0131 - precision_12: 0.4737 - val_loss: 1.4686 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 117/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0620 - acc: 0.4293 - recall_12: 0.0461 - precision_12: 0.5283\n",
            "Epoch 117: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0621 - acc: 0.4273 - recall_12: 0.0407 - precision_12: 0.5283 - val_loss: 1.4967 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 118/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0620 - acc: 0.4516 - recall_12: 0.0078 - precision_12: 0.6250\n",
            "Epoch 118: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0584 - acc: 0.4564 - recall_12: 0.0102 - precision_12: 0.5833 - val_loss: 1.4476 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 119/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0593 - acc: 0.4360 - recall_12: 0.0595 - precision_12: 0.5405\n",
            "Epoch 119: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0593 - acc: 0.4331 - recall_12: 0.0596 - precision_12: 0.5395 - val_loss: 1.4875 - val_acc: 0.7500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 120/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0661 - acc: 0.4234 - recall_12: 0.0203 - precision_12: 0.6842\n",
            "Epoch 120: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0684 - acc: 0.4230 - recall_12: 0.0189 - precision_12: 0.6500 - val_loss: 1.4826 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 121/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0692 - acc: 0.4256 - recall_12: 0.0179 - precision_12: 0.6000\n",
            "Epoch 121: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0666 - acc: 0.4288 - recall_12: 0.0174 - precision_12: 0.6000 - val_loss: 1.4755 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 122/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0553 - acc: 0.4547 - recall_12: 0.0391 - precision_12: 0.4902\n",
            "Epoch 122: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0612 - acc: 0.4433 - recall_12: 0.0378 - precision_12: 0.4906 - val_loss: 1.5022 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 123/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0592 - acc: 0.4172 - recall_12: 0.0141 - precision_12: 0.6000\n",
            "Epoch 123: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0598 - acc: 0.4172 - recall_12: 0.0145 - precision_12: 0.5556 - val_loss: 1.4921 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 124/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0747 - acc: 0.4141 - recall_12: 0.0094 - precision_12: 0.3000\n",
            "Epoch 124: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0687 - acc: 0.4230 - recall_12: 0.0102 - precision_12: 0.3182 - val_loss: 1.4683 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 125/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0641 - acc: 0.4226 - recall_12: 0.0283 - precision_12: 0.5429\n",
            "Epoch 125: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0613 - acc: 0.4244 - recall_12: 0.0276 - precision_12: 0.5429 - val_loss: 1.4868 - val_acc: 0.7500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 126/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0666 - acc: 0.4524 - recall_12: 0.0208 - precision_12: 0.4000\n",
            "Epoch 126: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 12ms/step - loss: 1.0654 - acc: 0.4535 - recall_12: 0.0203 - precision_12: 0.4000 - val_loss: 1.4887 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 127/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0637 - acc: 0.4344 - recall_12: 0.0125 - precision_12: 0.3810\n",
            "Epoch 127: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0652 - acc: 0.4390 - recall_12: 0.0189 - precision_12: 0.4333 - val_loss: 1.4643 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 128/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0568 - acc: 0.4405 - recall_12: 0.0476 - precision_12: 0.4848\n",
            "Epoch 128: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0585 - acc: 0.4375 - recall_12: 0.0465 - precision_12: 0.4776 - val_loss: 1.5277 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 129/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0651 - acc: 0.4453 - recall_12: 0.0047 - precision_12: 0.6000        \n",
            "Epoch 129: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0669 - acc: 0.4433 - recall_12: 0.0102 - precision_12: 0.7000 - val_loss: 1.4872 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 130/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0627 - acc: 0.4172 - recall_12: 0.0141 - precision_12: 0.4500\n",
            "Epoch 130: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0650 - acc: 0.4157 - recall_12: 0.0145 - precision_12: 0.4762 - val_loss: 1.4810 - val_acc: 0.7500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 131/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0635 - acc: 0.4219 - recall_12: 0.0172 - precision_12: 0.5238\n",
            "Epoch 131: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0614 - acc: 0.4288 - recall_12: 0.0189 - precision_12: 0.5417 - val_loss: 1.4820 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 132/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0635 - acc: 0.4315 - recall_12: 0.0283 - precision_12: 0.5278\n",
            "Epoch 132: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0640 - acc: 0.4331 - recall_12: 0.0291 - precision_12: 0.5405 - val_loss: 1.4626 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 133/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0583 - acc: 0.4408 - recall_12: 0.0559 - precision_12: 0.6071\n",
            "Epoch 133: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0584 - acc: 0.4404 - recall_12: 0.0552 - precision_12: 0.6230 - val_loss: 1.4932 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 134/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0534 - acc: 0.4484 - recall_12: 0.0422 - precision_12: 0.5192\n",
            "Epoch 134: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0568 - acc: 0.4419 - recall_12: 0.0407 - precision_12: 0.5283 - val_loss: 1.5055 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 135/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0608 - acc: 0.4509 - recall_12: 0.0268 - precision_12: 0.5294\n",
            "Epoch 135: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0612 - acc: 0.4491 - recall_12: 0.0262 - precision_12: 0.5294 - val_loss: 1.5329 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 136/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0591 - acc: 0.4196 - recall_12: 0.0238 - precision_12: 0.8000\n",
            "Epoch 136: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0564 - acc: 0.4230 - recall_12: 0.0233 - precision_12: 0.8000 - val_loss: 1.4826 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 137/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0631 - acc: 0.4271 - recall_12: 0.0208 - precision_12: 0.3871\n",
            "Epoch 137: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0580 - acc: 0.4360 - recall_12: 0.0247 - precision_12: 0.4359 - val_loss: 1.4993 - val_acc: 0.7500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 138/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0674 - acc: 0.4281 - recall_12: 0.0094 - precision_12: 0.4615\n",
            "Epoch 138: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0659 - acc: 0.4302 - recall_12: 0.0160 - precision_12: 0.5000 - val_loss: 1.4634 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 139/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0609 - acc: 0.4375 - recall_12: 0.0625 - precision_12: 0.4941\n",
            "Epoch 139: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0635 - acc: 0.4317 - recall_12: 0.0610 - precision_12: 0.4941 - val_loss: 1.5382 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 140/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0612 - acc: 0.4405 - recall_12: 0.0119 - precision_12: 0.7273\n",
            "Epoch 140: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0595 - acc: 0.4419 - recall_12: 0.0160 - precision_12: 0.7857 - val_loss: 1.4732 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 141/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0702 - acc: 0.4107 - recall_12: 0.0223 - precision_12: 0.3947\n",
            "Epoch 141: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0675 - acc: 0.4157 - recall_12: 0.0218 - precision_12: 0.3947 - val_loss: 1.4708 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 142/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0637 - acc: 0.4345 - recall_12: 0.0446 - precision_12: 0.6250\n",
            "Epoch 142: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0615 - acc: 0.4404 - recall_12: 0.0451 - precision_12: 0.6327 - val_loss: 1.4759 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 143/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0563 - acc: 0.4359 - recall_12: 0.0250 - precision_12: 0.5333\n",
            "Epoch 143: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 1.0555 - acc: 0.4390 - recall_12: 0.0262 - precision_12: 0.5625 - val_loss: 1.4931 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 144/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0626 - acc: 0.4391 - recall_12: 0.0281 - precision_12: 0.5143\n",
            "Epoch 144: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 19ms/step - loss: 1.0601 - acc: 0.4433 - recall_12: 0.0291 - precision_12: 0.5405 - val_loss: 1.5048 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 145/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0475 - acc: 0.4720 - recall_12: 0.0625 - precision_12: 0.5352\n",
            "Epoch 145: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0538 - acc: 0.4564 - recall_12: 0.0567 - precision_12: 0.5417 - val_loss: 1.5450 - val_acc: 0.5750 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 146/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0682 - acc: 0.4344 - recall_12: 0.0141 - precision_12: 0.4286\n",
            "Epoch 146: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0637 - acc: 0.4390 - recall_12: 0.0160 - precision_12: 0.4583 - val_loss: 1.4753 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 147/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0559 - acc: 0.4420 - recall_12: 0.0625 - precision_12: 0.5000\n",
            "Epoch 147: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0562 - acc: 0.4419 - recall_12: 0.0610 - precision_12: 0.5000 - val_loss: 1.4983 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 148/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0603 - acc: 0.4309 - recall_12: 0.0411 - precision_12: 0.5102\n",
            "Epoch 148: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0619 - acc: 0.4317 - recall_12: 0.0378 - precision_12: 0.5000 - val_loss: 1.4914 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 149/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0645 - acc: 0.4474 - recall_12: 0.0411 - precision_12: 0.5102\n",
            "Epoch 149: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0625 - acc: 0.4520 - recall_12: 0.0378 - precision_12: 0.5200 - val_loss: 1.5258 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 150/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0547 - acc: 0.4563 - recall_12: 0.0188 - precision_12: 0.4615\n",
            "Epoch 150: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0505 - acc: 0.4593 - recall_12: 0.0203 - precision_12: 0.5000 - val_loss: 1.4880 - val_acc: 0.6750 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 151/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0589 - acc: 0.4598 - recall_12: 0.0223 - precision_12: 0.4839\n",
            "Epoch 151: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0583 - acc: 0.4578 - recall_12: 0.0233 - precision_12: 0.4706 - val_loss: 1.4759 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 152/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0625 - acc: 0.4331 - recall_12: 0.0407 - precision_12: 0.5714\n",
            "Epoch 152: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0625 - acc: 0.4331 - recall_12: 0.0407 - precision_12: 0.5714 - val_loss: 1.4858 - val_acc: 0.6750 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 153/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0601 - acc: 0.4141 - recall_12: 0.0297 - precision_12: 0.6552\n",
            "Epoch 153: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0577 - acc: 0.4215 - recall_12: 0.0305 - precision_12: 0.6562 - val_loss: 1.4554 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 154/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0601 - acc: 0.4435 - recall_12: 0.0432 - precision_12: 0.5088\n",
            "Epoch 154: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0594 - acc: 0.4448 - recall_12: 0.0436 - precision_12: 0.5172 - val_loss: 1.4886 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 155/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0577 - acc: 0.4583 - recall_12: 0.0298 - precision_12: 0.6061\n",
            "Epoch 155: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0551 - acc: 0.4608 - recall_12: 0.0320 - precision_12: 0.6286 - val_loss: 1.4880 - val_acc: 0.7000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 156/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0606 - acc: 0.4405 - recall_12: 0.0283 - precision_12: 0.4524\n",
            "Epoch 156: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0623 - acc: 0.4390 - recall_12: 0.0305 - precision_12: 0.4773 - val_loss: 1.4851 - val_acc: 0.6750 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 157/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0504 - acc: 0.4309 - recall_12: 0.0411 - precision_12: 0.6098\n",
            "Epoch 157: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0555 - acc: 0.4259 - recall_12: 0.0422 - precision_12: 0.6304 - val_loss: 1.5075 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 158/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0682 - acc: 0.4330 - recall_12: 0.0208 - precision_12: 0.5600\n",
            "Epoch 158: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0653 - acc: 0.4390 - recall_12: 0.0247 - precision_12: 0.6071 - val_loss: 1.4508 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 159/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0541 - acc: 0.4469 - recall_12: 0.0797 - precision_12: 0.5426\n",
            "Epoch 159: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0580 - acc: 0.4404 - recall_12: 0.0756 - precision_12: 0.5474 - val_loss: 1.5006 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 160/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0593 - acc: 0.4554 - recall_12: 0.0402 - precision_12: 0.4500\n",
            "Epoch 160: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0607 - acc: 0.4520 - recall_12: 0.0392 - precision_12: 0.4500 - val_loss: 1.5099 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 161/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0615 - acc: 0.4539 - recall_12: 0.0446 - precision_12: 0.4545\n",
            "Epoch 161: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0625 - acc: 0.4506 - recall_12: 0.0436 - precision_12: 0.4545 - val_loss: 1.5263 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 162/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0531 - acc: 0.4464 - recall_12: 0.0685 - precision_12: 0.6133\n",
            "Epoch 162: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0543 - acc: 0.4419 - recall_12: 0.0669 - precision_12: 0.6053 - val_loss: 1.5694 - val_acc: 0.1000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 163/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0541 - acc: 0.4583 - recall_12: 0.0372 - precision_12: 0.6250\n",
            "Epoch 163: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0547 - acc: 0.4564 - recall_12: 0.0363 - precision_12: 0.6250 - val_loss: 1.5189 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 164/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0578 - acc: 0.4453 - recall_12: 0.0375 - precision_12: 0.5217\n",
            "Epoch 164: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0602 - acc: 0.4390 - recall_12: 0.0349 - precision_12: 0.5000 - val_loss: 1.5274 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 165/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0642 - acc: 0.4182 - recall_12: 0.0193 - precision_12: 0.5417\n",
            "Epoch 165: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0639 - acc: 0.4186 - recall_12: 0.0203 - precision_12: 0.5385 - val_loss: 1.5010 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 166/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0520 - acc: 0.4500 - recall_12: 0.0141 - precision_12: 0.6429\n",
            "Epoch 166: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0505 - acc: 0.4564 - recall_12: 0.0160 - precision_12: 0.6471 - val_loss: 1.4637 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 167/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0645 - acc: 0.4297 - recall_12: 0.0594 - precision_12: 0.5278\n",
            "Epoch 167: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0613 - acc: 0.4346 - recall_12: 0.0581 - precision_12: 0.5405 - val_loss: 1.4884 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 168/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0529 - acc: 0.4500 - recall_12: 0.0734 - precision_12: 0.5402\n",
            "Epoch 168: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0546 - acc: 0.4462 - recall_12: 0.0683 - precision_12: 0.5402 - val_loss: 1.5170 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 169/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0573 - acc: 0.4500 - recall_12: 0.0312 - precision_12: 0.5405\n",
            "Epoch 169: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0577 - acc: 0.4477 - recall_12: 0.0305 - precision_12: 0.5526 - val_loss: 1.5327 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 170/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0536 - acc: 0.4405 - recall_12: 0.0372 - precision_12: 0.5435\n",
            "Epoch 170: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0525 - acc: 0.4404 - recall_12: 0.0392 - precision_12: 0.5625 - val_loss: 1.4670 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 171/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0549 - acc: 0.4359 - recall_12: 0.0578 - precision_12: 0.5441\n",
            "Epoch 171: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0594 - acc: 0.4317 - recall_12: 0.0538 - precision_12: 0.5362 - val_loss: 1.4846 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 172/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0628 - acc: 0.4250 - recall_12: 0.0422 - precision_12: 0.5510\n",
            "Epoch 172: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0612 - acc: 0.4302 - recall_12: 0.0422 - precision_12: 0.5577 - val_loss: 1.4644 - val_acc: 0.6750 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 173/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0492 - acc: 0.4345 - recall_12: 0.0699 - precision_12: 0.5732\n",
            "Epoch 173: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0524 - acc: 0.4288 - recall_12: 0.0683 - precision_12: 0.5732 - val_loss: 1.5419 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 174/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0604 - acc: 0.4524 - recall_12: 0.0223 - precision_12: 0.6000\n",
            "Epoch 174: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0603 - acc: 0.4491 - recall_12: 0.0262 - precision_12: 0.6000 - val_loss: 1.4981 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 175/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0573 - acc: 0.4594 - recall_12: 0.0297 - precision_12: 0.4130\n",
            "Epoch 175: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0617 - acc: 0.4564 - recall_12: 0.0305 - precision_12: 0.4118 - val_loss: 1.4457 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 176/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0609 - acc: 0.4444 - recall_12: 0.0486 - precision_12: 0.5833\n",
            "Epoch 176: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0509 - acc: 0.4608 - recall_12: 0.0465 - precision_12: 0.6038 - val_loss: 1.4481 - val_acc: 0.7250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 177/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0558 - acc: 0.4422 - recall_12: 0.0672 - precision_12: 0.5181\n",
            "Epoch 177: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0545 - acc: 0.4433 - recall_12: 0.0683 - precision_12: 0.5109 - val_loss: 1.4795 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 178/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0552 - acc: 0.4360 - recall_12: 0.0804 - precision_12: 0.5684\n",
            "Epoch 178: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0549 - acc: 0.4346 - recall_12: 0.0785 - precision_12: 0.5684 - val_loss: 1.5467 - val_acc: 0.5750 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 179/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0562 - acc: 0.4469 - recall_12: 0.0437 - precision_12: 0.6667\n",
            "Epoch 179: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0582 - acc: 0.4477 - recall_12: 0.0407 - precision_12: 0.6667 - val_loss: 1.4986 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 180/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0520 - acc: 0.4598 - recall_12: 0.0357 - precision_12: 0.4800\n",
            "Epoch 180: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0498 - acc: 0.4608 - recall_12: 0.0363 - precision_12: 0.4902 - val_loss: 1.5024 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 181/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0598 - acc: 0.4125 - recall_12: 0.0469 - precision_12: 0.4918\n",
            "Epoch 181: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0600 - acc: 0.4201 - recall_12: 0.0465 - precision_12: 0.4848 - val_loss: 1.4810 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 182/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0547 - acc: 0.4609 - recall_12: 0.0453 - precision_12: 0.5370\n",
            "Epoch 182: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0522 - acc: 0.4637 - recall_12: 0.0436 - precision_12: 0.5455 - val_loss: 1.4796 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 183/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0545 - acc: 0.4453 - recall_12: 0.0750 - precision_12: 0.5783\n",
            "Epoch 183: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0569 - acc: 0.4419 - recall_12: 0.0698 - precision_12: 0.5647 - val_loss: 1.5527 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 184/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0565 - acc: 0.4297 - recall_12: 0.0250 - precision_12: 0.5333\n",
            "Epoch 184: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0543 - acc: 0.4346 - recall_12: 0.0262 - precision_12: 0.5294 - val_loss: 1.5061 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 185/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0569 - acc: 0.4523 - recall_12: 0.0510 - precision_12: 0.5636\n",
            "Epoch 185: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0573 - acc: 0.4535 - recall_12: 0.0523 - precision_12: 0.5806 - val_loss: 1.4845 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 186/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0630 - acc: 0.4516 - recall_12: 0.0625 - precision_12: 0.5556\n",
            "Epoch 186: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0604 - acc: 0.4506 - recall_12: 0.0596 - precision_12: 0.5325 - val_loss: 1.4845 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 187/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0462 - acc: 0.4628 - recall_12: 0.0565 - precision_12: 0.6230\n",
            "Epoch 187: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0466 - acc: 0.4622 - recall_12: 0.0552 - precision_12: 0.6129 - val_loss: 1.4939 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 188/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0517 - acc: 0.4598 - recall_12: 0.0461 - precision_12: 0.5849\n",
            "Epoch 188: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0509 - acc: 0.4578 - recall_12: 0.0451 - precision_12: 0.5849 - val_loss: 1.4992 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 189/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0572 - acc: 0.4516 - recall_12: 0.0531 - precision_12: 0.5075\n",
            "Epoch 189: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0577 - acc: 0.4578 - recall_12: 0.0538 - precision_12: 0.5068 - val_loss: 1.5181 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 190/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0666 - acc: 0.4406 - recall_12: 0.0453 - precision_12: 0.5800\n",
            "Epoch 190: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0599 - acc: 0.4462 - recall_12: 0.0581 - precision_12: 0.5882 - val_loss: 1.4337 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 191/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0525 - acc: 0.4509 - recall_12: 0.0908 - precision_12: 0.5648\n",
            "Epoch 191: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0546 - acc: 0.4448 - recall_12: 0.0887 - precision_12: 0.5545 - val_loss: 1.5137 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 192/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0499 - acc: 0.4554 - recall_12: 0.0536 - precision_12: 0.5714\n",
            "Epoch 192: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0506 - acc: 0.4549 - recall_12: 0.0523 - precision_12: 0.5714 - val_loss: 1.5036 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 193/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0454 - acc: 0.4792 - recall_12: 0.0595 - precision_12: 0.6667\n",
            "Epoch 193: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0458 - acc: 0.4782 - recall_12: 0.0596 - precision_12: 0.6721 - val_loss: 1.4907 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 194/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0561 - acc: 0.4547 - recall_12: 0.0547 - precision_12: 0.5147\n",
            "Epoch 194: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0565 - acc: 0.4520 - recall_12: 0.0581 - precision_12: 0.5333 - val_loss: 1.5012 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 195/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0498 - acc: 0.4875 - recall_12: 0.0312 - precision_12: 0.5128\n",
            "Epoch 195: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0482 - acc: 0.4869 - recall_12: 0.0334 - precision_12: 0.5227 - val_loss: 1.4211 - val_acc: 0.6750 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 196/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0506 - acc: 0.4572 - recall_12: 0.0888 - precision_12: 0.4779\n",
            "Epoch 196: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0563 - acc: 0.4520 - recall_12: 0.0858 - precision_12: 0.4758 - val_loss: 1.5338 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 197/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0492 - acc: 0.4688 - recall_12: 0.0580 - precision_12: 0.5270\n",
            "Epoch 197: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0514 - acc: 0.4666 - recall_12: 0.0581 - precision_12: 0.5263 - val_loss: 1.5264 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 198/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0498 - acc: 0.4622 - recall_12: 0.0526 - precision_12: 0.5079\n",
            "Epoch 198: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0499 - acc: 0.4651 - recall_12: 0.0567 - precision_12: 0.5132 - val_loss: 1.5553 - val_acc: 0.5750 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 199/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0545 - acc: 0.4479 - recall_12: 0.0642 - precision_12: 0.6066\n",
            "Epoch 199: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0540 - acc: 0.4491 - recall_12: 0.0770 - precision_12: 0.6163 - val_loss: 1.5148 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 200/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0480 - acc: 0.4390 - recall_12: 0.0610 - precision_12: 0.5616\n",
            "Epoch 200: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0488 - acc: 0.4404 - recall_12: 0.0610 - precision_12: 0.5600 - val_loss: 1.5608 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 201/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0472 - acc: 0.4500 - recall_12: 0.0688 - precision_12: 0.6197\n",
            "Epoch 201: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0480 - acc: 0.4491 - recall_12: 0.0669 - precision_12: 0.5974 - val_loss: 1.5236 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 202/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0517 - acc: 0.4844 - recall_12: 0.0547 - precision_12: 0.4930\n",
            "Epoch 202: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0525 - acc: 0.4753 - recall_12: 0.0523 - precision_12: 0.4932 - val_loss: 1.4760 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 203/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0452 - acc: 0.4688 - recall_12: 0.1000 - precision_12: 0.6531\n",
            "Epoch 203: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0443 - acc: 0.4666 - recall_12: 0.0959 - precision_12: 0.6535 - val_loss: 1.5255 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 204/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0483 - acc: 0.4655 - recall_12: 0.0839 - precision_12: 0.4811\n",
            "Epoch 204: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0512 - acc: 0.4578 - recall_12: 0.0785 - precision_12: 0.4821 - val_loss: 1.5314 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 205/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0501 - acc: 0.4547 - recall_12: 0.0828 - precision_12: 0.5579\n",
            "Epoch 205: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0526 - acc: 0.4477 - recall_12: 0.0843 - precision_12: 0.5743 - val_loss: 1.5277 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 206/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0549 - acc: 0.4391 - recall_12: 0.0766 - precision_12: 0.5904\n",
            "Epoch 206: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0524 - acc: 0.4477 - recall_12: 0.0741 - precision_12: 0.5795 - val_loss: 1.5514 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 207/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0402 - acc: 0.4881 - recall_12: 0.0997 - precision_12: 0.5877\n",
            "Epoch 207: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0419 - acc: 0.4855 - recall_12: 0.0974 - precision_12: 0.5726 - val_loss: 1.5918 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 208/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0508 - acc: 0.4516 - recall_12: 0.0875 - precision_12: 0.5545\n",
            "Epoch 208: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0516 - acc: 0.4491 - recall_12: 0.0872 - precision_12: 0.5505 - val_loss: 1.5846 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 209/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0461 - acc: 0.4717 - recall_12: 0.0804 - precision_12: 0.5000\n",
            "Epoch 209: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0472 - acc: 0.4709 - recall_12: 0.0828 - precision_12: 0.5135 - val_loss: 1.5880 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 210/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0426 - acc: 0.4891 - recall_12: 0.0781 - precision_12: 0.5319\n",
            "Epoch 210: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0423 - acc: 0.4898 - recall_12: 0.0756 - precision_12: 0.5253 - val_loss: 1.5821 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 211/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0480 - acc: 0.4613 - recall_12: 0.0685 - precision_12: 0.6133\n",
            "Epoch 211: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0476 - acc: 0.4593 - recall_12: 0.0727 - precision_12: 0.6173 - val_loss: 1.5047 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 212/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0480 - acc: 0.4539 - recall_12: 0.1086 - precision_12: 0.5893\n",
            "Epoch 212: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0460 - acc: 0.4520 - recall_12: 0.1061 - precision_12: 0.5984 - val_loss: 1.5469 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 213/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0490 - acc: 0.4688 - recall_12: 0.0685 - precision_12: 0.5227\n",
            "Epoch 213: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0460 - acc: 0.4709 - recall_12: 0.0727 - precision_12: 0.5376 - val_loss: 1.4610 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 214/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0456 - acc: 0.4578 - recall_12: 0.0984 - precision_12: 0.6058\n",
            "Epoch 214: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0421 - acc: 0.4608 - recall_12: 0.0974 - precision_12: 0.5929 - val_loss: 1.4389 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 215/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0408 - acc: 0.4703 - recall_12: 0.1156 - precision_12: 0.5968\n",
            "Epoch 215: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0419 - acc: 0.4724 - recall_12: 0.1294 - precision_12: 0.6138 - val_loss: 1.4238 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 216/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0497 - acc: 0.4704 - recall_12: 0.1102 - precision_12: 0.5537\n",
            "Epoch 216: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0498 - acc: 0.4666 - recall_12: 0.1250 - precision_12: 0.5733 - val_loss: 1.4796 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 217/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0528 - acc: 0.4625 - recall_12: 0.0906 - precision_12: 0.6105\n",
            "Epoch 217: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0453 - acc: 0.4709 - recall_12: 0.0945 - precision_12: 0.6311 - val_loss: 1.3989 - val_acc: 0.6500 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 218/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0472 - acc: 0.4594 - recall_12: 0.1594 - precision_12: 0.5698\n",
            "Epoch 218: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0481 - acc: 0.4608 - recall_12: 0.1526 - precision_12: 0.5738 - val_loss: 1.5431 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 219/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0418 - acc: 0.4673 - recall_12: 0.0804 - precision_12: 0.5870\n",
            "Epoch 219: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0396 - acc: 0.4709 - recall_12: 0.0828 - precision_12: 0.5938 - val_loss: 1.4691 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 220/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0394 - acc: 0.4655 - recall_12: 0.1283 - precision_12: 0.5865\n",
            "Epoch 220: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0444 - acc: 0.4622 - recall_12: 0.1265 - precision_12: 0.5762 - val_loss: 1.5360 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 221/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0492 - acc: 0.4803 - recall_12: 0.1003 - precision_12: 0.5304\n",
            "Epoch 221: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0472 - acc: 0.4797 - recall_12: 0.1047 - precision_12: 0.5373 - val_loss: 1.5694 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 222/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0458 - acc: 0.4844 - recall_12: 0.0906 - precision_12: 0.5800\n",
            "Epoch 222: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0383 - acc: 0.4942 - recall_12: 0.0959 - precision_12: 0.5946 - val_loss: 1.4856 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 223/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0338 - acc: 0.4703 - recall_12: 0.1547 - precision_12: 0.6226\n",
            "Epoch 223: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0348 - acc: 0.4767 - recall_12: 0.1497 - precision_12: 0.6280 - val_loss: 1.5501 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 224/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0462 - acc: 0.4589 - recall_12: 0.0954 - precision_12: 0.5421\n",
            "Epoch 224: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0421 - acc: 0.4593 - recall_12: 0.0901 - precision_12: 0.5439 - val_loss: 1.5629 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 225/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0449 - acc: 0.4732 - recall_12: 0.0982 - precision_12: 0.5323\n",
            "Epoch 225: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0435 - acc: 0.4753 - recall_12: 0.1003 - precision_12: 0.5391 - val_loss: 1.4504 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 226/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0435 - acc: 0.4747 - recall_12: 0.1250 - precision_12: 0.5385\n",
            "Epoch 226: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0453 - acc: 0.4724 - recall_12: 0.1250 - precision_12: 0.5309 - val_loss: 1.5816 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 227/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0332 - acc: 0.4781 - recall_12: 0.1266 - precision_12: 0.6429\n",
            "Epoch 227: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0375 - acc: 0.4724 - recall_12: 0.1250 - precision_12: 0.6370 - val_loss: 1.5724 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 228/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0366 - acc: 0.4670 - recall_12: 0.1562 - precision_12: 0.5590\n",
            "Epoch 228: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0385 - acc: 0.4666 - recall_12: 0.1453 - precision_12: 0.5714 - val_loss: 1.6082 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 229/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0405 - acc: 0.4737 - recall_12: 0.0872 - precision_12: 0.5521\n",
            "Epoch 229: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0397 - acc: 0.4709 - recall_12: 0.1061 - precision_12: 0.5887 - val_loss: 1.4746 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 230/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0414 - acc: 0.4766 - recall_12: 0.1344 - precision_12: 0.5548\n",
            "Epoch 230: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0410 - acc: 0.4767 - recall_12: 0.1265 - precision_12: 0.5541 - val_loss: 1.4979 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 231/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0437 - acc: 0.4656 - recall_12: 0.1578 - precision_12: 0.5771\n",
            "Epoch 231: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0475 - acc: 0.4593 - recall_12: 0.1497 - precision_12: 0.5787 - val_loss: 1.5349 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 232/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0398 - acc: 0.4844 - recall_12: 0.1297 - precision_12: 0.5608\n",
            "Epoch 232: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0397 - acc: 0.4840 - recall_12: 0.1265 - precision_12: 0.5577 - val_loss: 1.5630 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 233/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0430 - acc: 0.4911 - recall_12: 0.1086 - precision_12: 0.5887\n",
            "Epoch 233: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0403 - acc: 0.4927 - recall_12: 0.1090 - precision_12: 0.5952 - val_loss: 1.4615 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 234/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0446 - acc: 0.4672 - recall_12: 0.1203 - precision_12: 0.5000\n",
            "Epoch 234: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0407 - acc: 0.4738 - recall_12: 0.1192 - precision_12: 0.5062 - val_loss: 1.5023 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 235/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0365 - acc: 0.4655 - recall_12: 0.1217 - precision_12: 0.5827\n",
            "Epoch 235: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0329 - acc: 0.4724 - recall_12: 0.1395 - precision_12: 0.5890 - val_loss: 1.4100 - val_acc: 0.6250 - val_recall_12: 0.0500 - val_precision_12: 0.6667\n",
            "Epoch 236/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0364 - acc: 0.4781 - recall_12: 0.2016 - precision_12: 0.5658\n",
            "Epoch 236: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0360 - acc: 0.4840 - recall_12: 0.1904 - precision_12: 0.5622 - val_loss: 1.6532 - val_acc: 0.5750 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 237/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0283 - acc: 0.4866 - recall_12: 0.1414 - precision_12: 0.6090\n",
            "Epoch 237: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0316 - acc: 0.4840 - recall_12: 0.1395 - precision_12: 0.6038 - val_loss: 1.6145 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 238/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0396 - acc: 0.4844 - recall_12: 0.1594 - precision_12: 0.5604\n",
            "Epoch 238: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0424 - acc: 0.4782 - recall_12: 0.1584 - precision_12: 0.5677 - val_loss: 1.5679 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 239/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0332 - acc: 0.4866 - recall_12: 0.1310 - precision_12: 0.5535\n",
            "Epoch 239: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0333 - acc: 0.4884 - recall_12: 0.1308 - precision_12: 0.5590 - val_loss: 1.5025 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 240/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0433 - acc: 0.4563 - recall_12: 0.1375 - precision_12: 0.5057\n",
            "Epoch 240: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0397 - acc: 0.4651 - recall_12: 0.1381 - precision_12: 0.5191 - val_loss: 1.4073 - val_acc: 0.6250 - val_recall_12: 0.1500 - val_precision_12: 0.8571\n",
            "Epoch 241/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0361 - acc: 0.4688 - recall_12: 0.1756 - precision_12: 0.5728\n",
            "Epoch 241: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0344 - acc: 0.4724 - recall_12: 0.1759 - precision_12: 0.5789 - val_loss: 1.5492 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 242/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0395 - acc: 0.4656 - recall_12: 0.1375 - precision_12: 0.5301\n",
            "Epoch 242: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0379 - acc: 0.4695 - recall_12: 0.1323 - precision_12: 0.5260 - val_loss: 1.5020 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 243/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0302 - acc: 0.4926 - recall_12: 0.1458 - precision_12: 0.5765\n",
            "Epoch 243: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0338 - acc: 0.4869 - recall_12: 0.1453 - precision_12: 0.5714 - val_loss: 1.5821 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 244/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0395 - acc: 0.4704 - recall_12: 0.1464 - precision_12: 0.5973\n",
            "Epoch 244: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0333 - acc: 0.4855 - recall_12: 0.1526 - precision_12: 0.6000 - val_loss: 1.4530 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 245/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0291 - acc: 0.4792 - recall_12: 0.2118 - precision_12: 0.5865\n",
            "Epoch 245: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0337 - acc: 0.4767 - recall_12: 0.1933 - precision_12: 0.5938 - val_loss: 1.6432 - val_acc: 0.6000 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 246/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0217 - acc: 0.5016 - recall_12: 0.1688 - precision_12: 0.6136\n",
            "Epoch 246: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0254 - acc: 0.4942 - recall_12: 0.1715 - precision_12: 0.6082 - val_loss: 1.5368 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 247/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0245 - acc: 0.4984 - recall_12: 0.1727 - precision_12: 0.6069\n",
            "Epoch 247: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0272 - acc: 0.4913 - recall_12: 0.1701 - precision_12: 0.5939 - val_loss: 1.5098 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 248/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0202 - acc: 0.4878 - recall_12: 0.1823 - precision_12: 0.5615\n",
            "Epoch 248: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0278 - acc: 0.4869 - recall_12: 0.1686 - precision_12: 0.5524 - val_loss: 1.5356 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 249/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0259 - acc: 0.5047 - recall_12: 0.1734 - precision_12: 0.5550\n",
            "Epoch 249: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0286 - acc: 0.4985 - recall_12: 0.1831 - precision_12: 0.5600 - val_loss: 1.5317 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 250/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0181 - acc: 0.4969 - recall_12: 0.1969 - precision_12: 0.6087\n",
            "Epoch 250: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0330 - acc: 0.4811 - recall_12: 0.1904 - precision_12: 0.5696 - val_loss: 1.6192 - val_acc: 0.5750 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 251/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0264 - acc: 0.4965 - recall_12: 0.1632 - precision_12: 0.5987\n",
            "Epoch 251: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0273 - acc: 0.4898 - recall_12: 0.1686 - precision_12: 0.6042 - val_loss: 1.4914 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 252/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0283 - acc: 0.4969 - recall_12: 0.1937 - precision_12: 0.5822\n",
            "Epoch 252: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0311 - acc: 0.4927 - recall_12: 0.1933 - precision_12: 0.5684 - val_loss: 1.6332 - val_acc: 0.6000 - val_recall_12: 0.0250 - val_precision_12: 0.5000\n",
            "Epoch 253/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0283 - acc: 0.5089 - recall_12: 0.1860 - precision_12: 0.5841\n",
            "Epoch 253: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0273 - acc: 0.5058 - recall_12: 0.1875 - precision_12: 0.5864 - val_loss: 1.5300 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 254/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0191 - acc: 0.4926 - recall_12: 0.1935 - precision_12: 0.5963\n",
            "Epoch 254: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0227 - acc: 0.4884 - recall_12: 0.1933 - precision_12: 0.5885 - val_loss: 1.5687 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 255/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0224 - acc: 0.4984 - recall_12: 0.1595 - precision_12: 0.6178\n",
            "Epoch 255: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0242 - acc: 0.4956 - recall_12: 0.1584 - precision_12: 0.6022 - val_loss: 1.4772 - val_acc: 0.6250 - val_recall_12: 0.1000 - val_precision_12: 0.8000\n",
            "Epoch 256/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0241 - acc: 0.4984 - recall_12: 0.2141 - precision_12: 0.6116\n",
            "Epoch 256: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0243 - acc: 0.4956 - recall_12: 0.2151 - precision_12: 0.6167 - val_loss: 1.5643 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 257/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0266 - acc: 0.4836 - recall_12: 0.1776 - precision_12: 0.6000\n",
            "Epoch 257: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0289 - acc: 0.4826 - recall_12: 0.1933 - precision_12: 0.6018 - val_loss: 1.4916 - val_acc: 0.6250 - val_recall_12: 0.1000 - val_precision_12: 0.8000\n",
            "Epoch 258/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0318 - acc: 0.4859 - recall_12: 0.1953 - precision_12: 0.5411\n",
            "Epoch 258: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0304 - acc: 0.4869 - recall_12: 0.1933 - precision_12: 0.5473 - val_loss: 1.5016 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 259/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0254 - acc: 0.4875 - recall_12: 0.1937 - precision_12: 0.6359\n",
            "Epoch 259: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0185 - acc: 0.5015 - recall_12: 0.2020 - precision_12: 0.6526 - val_loss: 1.3748 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 260/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0217 - acc: 0.4901 - recall_12: 0.2270 - precision_12: 0.5726\n",
            "Epoch 260: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0285 - acc: 0.4811 - recall_12: 0.2267 - precision_12: 0.5673 - val_loss: 1.5560 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 261/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0111 - acc: 0.5099 - recall_12: 0.1891 - precision_12: 0.6117\n",
            "Epoch 261: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0153 - acc: 0.5058 - recall_12: 0.1977 - precision_12: 0.5887 - val_loss: 1.5801 - val_acc: 0.6250 - val_recall_12: 0.0250 - val_precision_12: 0.2000\n",
            "Epoch 262/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0255 - acc: 0.5016 - recall_12: 0.2109 - precision_12: 0.5745\n",
            "Epoch 262: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0249 - acc: 0.4971 - recall_12: 0.2093 - precision_12: 0.5692 - val_loss: 1.4800 - val_acc: 0.6250 - val_recall_12: 0.1000 - val_precision_12: 0.8000\n",
            "Epoch 263/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0214 - acc: 0.4891 - recall_12: 0.2328 - precision_12: 0.6107\n",
            "Epoch 263: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0225 - acc: 0.4898 - recall_12: 0.2311 - precision_12: 0.6069 - val_loss: 1.5762 - val_acc: 0.6250 - val_recall_12: 0.0250 - val_precision_12: 0.5000\n",
            "Epoch 264/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0229 - acc: 0.4878 - recall_12: 0.1892 - precision_12: 0.5707\n",
            "Epoch 264: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0194 - acc: 0.4956 - recall_12: 0.2035 - precision_12: 0.5858 - val_loss: 1.5410 - val_acc: 0.6250 - val_recall_12: 0.0000e+00 - val_precision_12: 0.0000e+00\n",
            "Epoch 265/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0225 - acc: 0.5125 - recall_12: 0.2188 - precision_12: 0.6009\n",
            "Epoch 265: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0242 - acc: 0.5073 - recall_12: 0.2151 - precision_12: 0.5944 - val_loss: 1.6863 - val_acc: 0.4750 - val_recall_12: 0.0250 - val_precision_12: 0.1667\n",
            "Epoch 266/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0246 - acc: 0.4875 - recall_12: 0.2000 - precision_12: 0.5766\n",
            "Epoch 266: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0265 - acc: 0.4869 - recall_12: 0.1977 - precision_12: 0.5763 - val_loss: 1.6537 - val_acc: 0.6000 - val_recall_12: 0.0250 - val_precision_12: 0.2000\n",
            "Epoch 267/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0226 - acc: 0.4891 - recall_12: 0.1797 - precision_12: 0.5721\n",
            "Epoch 267: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0223 - acc: 0.4869 - recall_12: 0.1817 - precision_12: 0.5656 - val_loss: 1.4700 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.9375\n",
            "Epoch 268/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0185 - acc: 0.4969 - recall_12: 0.2375 - precision_12: 0.5568\n",
            "Epoch 268: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0224 - acc: 0.4884 - recall_12: 0.2326 - precision_12: 0.5594 - val_loss: 1.6096 - val_acc: 0.6250 - val_recall_12: 0.0250 - val_precision_12: 0.5000\n",
            "Epoch 269/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0191 - acc: 0.4797 - recall_12: 0.1953 - precision_12: 0.6757\n",
            "Epoch 269: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0148 - acc: 0.4869 - recall_12: 0.2049 - precision_12: 0.6746 - val_loss: 1.4974 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.9375\n",
            "Epoch 270/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0271 - acc: 0.4770 - recall_12: 0.2171 - precision_12: 0.5789\n",
            "Epoch 270: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0262 - acc: 0.4811 - recall_12: 0.2253 - precision_12: 0.5894 - val_loss: 1.4139 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 271/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0101 - acc: 0.5156 - recall_12: 0.2396 - precision_12: 0.5948\n",
            "Epoch 271: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0193 - acc: 0.5145 - recall_12: 0.2282 - precision_12: 0.5793 - val_loss: 1.5054 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.9375\n",
            "Epoch 272/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0168 - acc: 0.4927 - recall_12: 0.2297 - precision_12: 0.5962\n",
            "Epoch 272: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0168 - acc: 0.4927 - recall_12: 0.2297 - precision_12: 0.5962 - val_loss: 1.5862 - val_acc: 0.6250 - val_recall_12: 0.0250 - val_precision_12: 0.2000\n",
            "Epoch 273/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0092 - acc: 0.5109 - recall_12: 0.2156 - precision_12: 0.6079\n",
            "Epoch 273: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0115 - acc: 0.5102 - recall_12: 0.2137 - precision_12: 0.6025 - val_loss: 1.6630 - val_acc: 0.6250 - val_recall_12: 0.0250 - val_precision_12: 0.2000\n",
            "Epoch 274/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0259 - acc: 0.5047 - recall_12: 0.2203 - precision_12: 0.5709\n",
            "Epoch 274: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0203 - acc: 0.5073 - recall_12: 0.2238 - precision_12: 0.5878 - val_loss: 1.5256 - val_acc: 0.6250 - val_recall_12: 0.1250 - val_precision_12: 0.7143\n",
            "Epoch 275/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0258 - acc: 0.4840 - recall_12: 0.2209 - precision_12: 0.5914\n",
            "Epoch 275: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0258 - acc: 0.4840 - recall_12: 0.2209 - precision_12: 0.5914 - val_loss: 1.6088 - val_acc: 0.6250 - val_recall_12: 0.0250 - val_precision_12: 0.2000\n",
            "Epoch 276/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0226 - acc: 0.4938 - recall_12: 0.2047 - precision_12: 0.5901\n",
            "Epoch 276: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0209 - acc: 0.4971 - recall_12: 0.2108 - precision_12: 0.6017 - val_loss: 1.4743 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.8824\n",
            "Epoch 277/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0100 - acc: 0.5000 - recall_12: 0.2484 - precision_12: 0.6138\n",
            "Epoch 277: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0088 - acc: 0.5015 - recall_12: 0.2485 - precision_12: 0.6107 - val_loss: 1.5545 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.8824\n",
            "Epoch 278/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0145 - acc: 0.4885 - recall_12: 0.2270 - precision_12: 0.5798\n",
            "Epoch 278: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0127 - acc: 0.4942 - recall_12: 0.2326 - precision_12: 0.5797 - val_loss: 1.4734 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.7500\n",
            "Epoch 279/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0182 - acc: 0.5066 - recall_12: 0.2368 - precision_12: 0.5806\n",
            "Epoch 279: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0208 - acc: 0.5044 - recall_12: 0.2311 - precision_12: 0.5658 - val_loss: 1.4961 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.9375\n",
            "Epoch 280/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0215 - acc: 0.5208 - recall_12: 0.2413 - precision_12: 0.5582\n",
            "Epoch 280: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0215 - acc: 0.5160 - recall_12: 0.2282 - precision_12: 0.5607 - val_loss: 1.4891 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.9375\n",
            "Epoch 281/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0155 - acc: 0.4828 - recall_12: 0.2578 - precision_12: 0.6111\n",
            "Epoch 281: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0190 - acc: 0.4797 - recall_12: 0.2471 - precision_12: 0.6028 - val_loss: 1.4928 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.9375\n",
            "Epoch 282/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0097 - acc: 0.4940 - recall_12: 0.2247 - precision_12: 0.5968\n",
            "Epoch 282: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0085 - acc: 0.4942 - recall_12: 0.2282 - precision_12: 0.5970 - val_loss: 1.3933 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 283/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0127 - acc: 0.5044 - recall_12: 0.2631 - precision_12: 0.5764\n",
            "Epoch 283: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0127 - acc: 0.5044 - recall_12: 0.2631 - precision_12: 0.5764 - val_loss: 1.5657 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8889\n",
            "Epoch 284/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0089 - acc: 0.5122 - recall_12: 0.2222 - precision_12: 0.5872\n",
            "Epoch 284: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0087 - acc: 0.5073 - recall_12: 0.2311 - precision_12: 0.5911 - val_loss: 1.6310 - val_acc: 0.6250 - val_recall_12: 0.0250 - val_precision_12: 0.2000\n",
            "Epoch 285/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9998 - acc: 0.5115 - recall_12: 0.2286 - precision_12: 0.6261\n",
            "Epoch 285: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0051 - acc: 0.5073 - recall_12: 0.2267 - precision_12: 0.5932 - val_loss: 1.5968 - val_acc: 0.6250 - val_recall_12: 0.2000 - val_precision_12: 0.6154\n",
            "Epoch 286/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0272 - acc: 0.4918 - recall_12: 0.2385 - precision_12: 0.5642\n",
            "Epoch 286: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0175 - acc: 0.5029 - recall_12: 0.2413 - precision_12: 0.5887 - val_loss: 1.5207 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8889\n",
            "Epoch 287/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0103 - acc: 0.5015 - recall_12: 0.2396 - precision_12: 0.5812\n",
            "Epoch 287: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0117 - acc: 0.5000 - recall_12: 0.2398 - precision_12: 0.5830 - val_loss: 1.5371 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.9375\n",
            "Epoch 288/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0056 - acc: 0.4951 - recall_12: 0.2286 - precision_12: 0.6150\n",
            "Epoch 288: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0074 - acc: 0.4971 - recall_12: 0.2253 - precision_12: 0.6225 - val_loss: 1.6736 - val_acc: 0.6250 - val_recall_12: 0.0250 - val_precision_12: 0.2000\n",
            "Epoch 289/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0120 - acc: 0.4953 - recall_12: 0.2594 - precision_12: 0.6103\n",
            "Epoch 289: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0121 - acc: 0.5029 - recall_12: 0.2544 - precision_12: 0.6055 - val_loss: 1.5106 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.7895\n",
            "Epoch 290/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0159 - acc: 0.5073 - recall_12: 0.2369 - precision_12: 0.5739\n",
            "Epoch 290: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0159 - acc: 0.5073 - recall_12: 0.2369 - precision_12: 0.5739 - val_loss: 1.6174 - val_acc: 0.6250 - val_recall_12: 0.0250 - val_precision_12: 0.2500\n",
            "Epoch 291/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0219 - acc: 0.4861 - recall_12: 0.1944 - precision_12: 0.5714\n",
            "Epoch 291: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0169 - acc: 0.4942 - recall_12: 0.2064 - precision_12: 0.5892 - val_loss: 1.5218 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.8824\n",
            "Epoch 292/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0058 - acc: 0.5172 - recall_12: 0.2531 - precision_12: 0.5870\n",
            "Epoch 292: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0135 - acc: 0.5102 - recall_12: 0.2456 - precision_12: 0.5768 - val_loss: 1.5230 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8421\n",
            "Epoch 293/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9972 - acc: 0.5230 - recall_12: 0.2780 - precision_12: 0.5993\n",
            "Epoch 293: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0035 - acc: 0.5189 - recall_12: 0.2733 - precision_12: 0.6045 - val_loss: 1.6685 - val_acc: 0.6000 - val_recall_12: 0.0250 - val_precision_12: 0.2000\n",
            "Epoch 294/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0176 - acc: 0.4851 - recall_12: 0.2381 - precision_12: 0.5993\n",
            "Epoch 294: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0150 - acc: 0.4855 - recall_12: 0.2413 - precision_12: 0.6058 - val_loss: 1.4566 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 295/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0119 - acc: 0.5125 - recall_12: 0.2500 - precision_12: 0.5556\n",
            "Epoch 295: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0115 - acc: 0.5160 - recall_12: 0.2500 - precision_12: 0.5639 - val_loss: 1.6048 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 296/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0020 - acc: 0.5016 - recall_12: 0.2484 - precision_12: 0.6113\n",
            "Epoch 296: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0076 - acc: 0.5015 - recall_12: 0.2500 - precision_12: 0.6014 - val_loss: 1.6172 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.7500\n",
            "Epoch 297/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0047 - acc: 0.5066 - recall_12: 0.2615 - precision_12: 0.5933\n",
            "Epoch 297: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0020 - acc: 0.5116 - recall_12: 0.2645 - precision_12: 0.5987 - val_loss: 1.5528 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8421\n",
            "Epoch 298/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9955 - acc: 0.5033 - recall_12: 0.2796 - precision_12: 0.6028\n",
            "Epoch 298: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9983 - acc: 0.5000 - recall_12: 0.2660 - precision_12: 0.6040 - val_loss: 1.6034 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 299/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0003 - acc: 0.5073 - recall_12: 0.2602 - precision_12: 0.6109\n",
            "Epoch 299: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0003 - acc: 0.5073 - recall_12: 0.2602 - precision_12: 0.6109 - val_loss: 1.5901 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 300/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0084 - acc: 0.5078 - recall_12: 0.3000 - precision_12: 0.5854\n",
            "Epoch 300: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0110 - acc: 0.5044 - recall_12: 0.2922 - precision_12: 0.5894 - val_loss: 1.6746 - val_acc: 0.6000 - val_recall_12: 0.0250 - val_precision_12: 0.2000\n",
            "Epoch 301/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0030 - acc: 0.5148 - recall_12: 0.2319 - precision_12: 0.5732\n",
            "Epoch 301: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0050 - acc: 0.5145 - recall_12: 0.2340 - precision_12: 0.5833 - val_loss: 1.4640 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 302/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0139 - acc: 0.4906 - recall_12: 0.2750 - precision_12: 0.5886\n",
            "Epoch 302: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0102 - acc: 0.5000 - recall_12: 0.2762 - precision_12: 0.5975 - val_loss: 1.5125 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 303/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9961 - acc: 0.5148 - recall_12: 0.2566 - precision_12: 0.6047\n",
            "Epoch 303: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0015 - acc: 0.5131 - recall_12: 0.2544 - precision_12: 0.5932 - val_loss: 1.5511 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.6667\n",
            "Epoch 304/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0098 - acc: 0.4984 - recall_12: 0.2531 - precision_12: 0.5806\n",
            "Epoch 304: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0071 - acc: 0.5000 - recall_12: 0.2602 - precision_12: 0.5888 - val_loss: 1.5342 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 305/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0080 - acc: 0.5016 - recall_12: 0.2763 - precision_12: 0.5936\n",
            "Epoch 305: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0092 - acc: 0.5029 - recall_12: 0.2718 - precision_12: 0.5826 - val_loss: 1.5672 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.7500\n",
            "Epoch 306/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0015 - acc: 0.5188 - recall_12: 0.2609 - precision_12: 0.5922\n",
            "Epoch 306: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0014 - acc: 0.5160 - recall_12: 0.2573 - precision_12: 0.5861 - val_loss: 1.5488 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8000\n",
            "Epoch 307/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0002 - acc: 0.5125 - recall_12: 0.2719 - precision_12: 0.6084\n",
            "Epoch 307: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9961 - acc: 0.5203 - recall_12: 0.2747 - precision_12: 0.6156 - val_loss: 1.5863 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8000\n",
            "Epoch 308/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9984 - acc: 0.5247 - recall_12: 0.2582 - precision_12: 0.5730\n",
            "Epoch 308: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0038 - acc: 0.5189 - recall_12: 0.2558 - precision_12: 0.5659 - val_loss: 1.5812 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8421\n",
            "Epoch 309/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0086 - acc: 0.5029 - recall_12: 0.2849 - precision_12: 0.6106\n",
            "Epoch 309: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0086 - acc: 0.5029 - recall_12: 0.2849 - precision_12: 0.6106 - val_loss: 1.6301 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8421\n",
            "Epoch 310/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0056 - acc: 0.4985 - recall_12: 0.2587 - precision_12: 0.5836\n",
            "Epoch 310: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0056 - acc: 0.4985 - recall_12: 0.2587 - precision_12: 0.5836 - val_loss: 1.5055 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 311/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0112 - acc: 0.5074 - recall_12: 0.2634 - precision_12: 0.6020\n",
            "Epoch 311: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0094 - acc: 0.5087 - recall_12: 0.2660 - precision_12: 0.6020 - val_loss: 1.4873 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 312/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0050 - acc: 0.5145 - recall_12: 0.2645 - precision_12: 0.6190\n",
            "Epoch 312: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0050 - acc: 0.5145 - recall_12: 0.2645 - precision_12: 0.6190 - val_loss: 1.4992 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 313/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0030 - acc: 0.5214 - recall_12: 0.2632 - precision_12: 0.5882\n",
            "Epoch 313: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0035 - acc: 0.5131 - recall_12: 0.2674 - precision_12: 0.5786 - val_loss: 1.6032 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 314/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0139 - acc: 0.4969 - recall_12: 0.2734 - precision_12: 0.5776\n",
            "Epoch 314: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 1.0124 - acc: 0.4942 - recall_12: 0.2674 - precision_12: 0.5750 - val_loss: 1.5092 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.6667\n",
            "Epoch 315/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0141 - acc: 0.5139 - recall_12: 0.2517 - precision_12: 0.5870\n",
            "Epoch 315: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0107 - acc: 0.5189 - recall_12: 0.2587 - precision_12: 0.5973 - val_loss: 1.5855 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 316/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 1.0113 - acc: 0.5174 - recall_12: 0.2465 - precision_12: 0.5892\n",
            "Epoch 316: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9987 - acc: 0.5291 - recall_12: 0.2602 - precision_12: 0.5987 - val_loss: 1.6011 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 317/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 1.0082 - acc: 0.5030 - recall_12: 0.2753 - precision_12: 0.5692\n",
            "Epoch 317: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 1.0063 - acc: 0.5044 - recall_12: 0.2747 - precision_12: 0.5693 - val_loss: 1.5803 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 318/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0053 - acc: 0.5263 - recall_12: 0.2615 - precision_12: 0.5955\n",
            "Epoch 318: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0109 - acc: 0.5145 - recall_12: 0.2587 - precision_12: 0.5914 - val_loss: 1.5667 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8000\n",
            "Epoch 319/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9936 - acc: 0.5181 - recall_12: 0.2615 - precision_12: 0.5867\n",
            "Epoch 319: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9958 - acc: 0.5131 - recall_12: 0.2645 - precision_12: 0.5852 - val_loss: 1.5188 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6129\n",
            "Epoch 320/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9957 - acc: 0.5082 - recall_12: 0.2829 - precision_12: 0.6143\n",
            "Epoch 320: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9930 - acc: 0.5160 - recall_12: 0.2892 - precision_12: 0.6219 - val_loss: 1.6323 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 321/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9940 - acc: 0.5109 - recall_12: 0.2562 - precision_12: 0.5816\n",
            "Epoch 321: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9929 - acc: 0.5160 - recall_12: 0.2645 - precision_12: 0.5928 - val_loss: 1.5831 - val_acc: 0.6250 - val_recall_12: 0.4250 - val_precision_12: 0.6296\n",
            "Epoch 322/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0059 - acc: 0.5115 - recall_12: 0.2747 - precision_12: 0.5860\n",
            "Epoch 322: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0013 - acc: 0.5174 - recall_12: 0.2805 - precision_12: 0.5884 - val_loss: 1.5256 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 323/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9906 - acc: 0.5181 - recall_12: 0.2829 - precision_12: 0.5993\n",
            "Epoch 323: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9960 - acc: 0.5131 - recall_12: 0.2820 - precision_12: 0.5897 - val_loss: 1.5601 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 324/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0007 - acc: 0.5197 - recall_12: 0.2533 - precision_12: 0.5704\n",
            "Epoch 324: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0040 - acc: 0.5116 - recall_12: 0.2558 - precision_12: 0.5677 - val_loss: 1.5279 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 325/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0102 - acc: 0.5189 - recall_12: 0.2558 - precision_12: 0.5382\n",
            "Epoch 325: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0102 - acc: 0.5189 - recall_12: 0.2558 - precision_12: 0.5382 - val_loss: 1.6055 - val_acc: 0.6000 - val_recall_12: 0.4000 - val_precision_12: 0.8000\n",
            "Epoch 326/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0039 - acc: 0.4953 - recall_12: 0.2688 - precision_12: 0.6078\n",
            "Epoch 326: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0028 - acc: 0.4985 - recall_12: 0.2645 - precision_12: 0.6087 - val_loss: 1.4869 - val_acc: 0.6250 - val_recall_12: 0.4250 - val_precision_12: 0.6538\n",
            "Epoch 327/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9990 - acc: 0.5203 - recall_12: 0.2625 - precision_12: 0.6199\n",
            "Epoch 327: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0022 - acc: 0.5174 - recall_12: 0.2587 - precision_12: 0.6138 - val_loss: 1.5253 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 328/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0030 - acc: 0.5164 - recall_12: 0.2697 - precision_12: 0.5964\n",
            "Epoch 328: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0024 - acc: 0.5102 - recall_12: 0.2631 - precision_12: 0.5974 - val_loss: 1.5944 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 329/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9887 - acc: 0.5280 - recall_12: 0.3043 - precision_12: 0.6208\n",
            "Epoch 329: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9877 - acc: 0.5291 - recall_12: 0.3052 - precision_12: 0.6176 - val_loss: 1.6530 - val_acc: 0.6000 - val_recall_12: 0.4000 - val_precision_12: 0.7273\n",
            "Epoch 330/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0026 - acc: 0.5066 - recall_12: 0.2730 - precision_12: 0.5887\n",
            "Epoch 330: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9960 - acc: 0.5145 - recall_12: 0.2762 - precision_12: 0.5975 - val_loss: 1.5501 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6786\n",
            "Epoch 331/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 1.0027 - acc: 0.5203 - recall_12: 0.2834 - precision_12: 0.5927\n",
            "Epoch 331: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0027 - acc: 0.5203 - recall_12: 0.2834 - precision_12: 0.5927 - val_loss: 1.6186 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8000\n",
            "Epoch 332/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9956 - acc: 0.5276 - recall_12: 0.2442 - precision_12: 0.5895\n",
            "Epoch 332: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9956 - acc: 0.5276 - recall_12: 0.2442 - precision_12: 0.5895 - val_loss: 1.4824 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 333/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0012 - acc: 0.5066 - recall_12: 0.2862 - precision_12: 0.5800\n",
            "Epoch 333: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 1.0026 - acc: 0.5073 - recall_12: 0.2849 - precision_12: 0.5833 - val_loss: 1.5771 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 334/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9979 - acc: 0.5141 - recall_12: 0.2781 - precision_12: 0.6181\n",
            "Epoch 334: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0008 - acc: 0.5203 - recall_12: 0.2776 - precision_12: 0.6141 - val_loss: 1.5954 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 335/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9820 - acc: 0.5362 - recall_12: 0.2681 - precision_12: 0.6128\n",
            "Epoch 335: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9916 - acc: 0.5247 - recall_12: 0.2689 - precision_12: 0.5911 - val_loss: 1.6412 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.8000\n",
            "Epoch 336/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9965 - acc: 0.5160 - recall_12: 0.2660 - precision_12: 0.6040\n",
            "Epoch 336: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9965 - acc: 0.5160 - recall_12: 0.2660 - precision_12: 0.6040 - val_loss: 1.5326 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6129\n",
            "Epoch 337/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9706 - acc: 0.5417 - recall_12: 0.3125 - precision_12: 0.6360\n",
            "Epoch 337: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9890 - acc: 0.5203 - recall_12: 0.2980 - precision_12: 0.6119 - val_loss: 1.5915 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 338/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9918 - acc: 0.5230 - recall_12: 0.2664 - precision_12: 0.6113\n",
            "Epoch 338: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9994 - acc: 0.5174 - recall_12: 0.2703 - precision_12: 0.6019 - val_loss: 1.5078 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 339/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9965 - acc: 0.5164 - recall_12: 0.2977 - precision_12: 0.5934\n",
            "Epoch 339: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9945 - acc: 0.5116 - recall_12: 0.2936 - precision_12: 0.5976 - val_loss: 1.6221 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.8824\n",
            "Epoch 340/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0022 - acc: 0.5141 - recall_12: 0.2922 - precision_12: 0.5974\n",
            "Epoch 340: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 1.0012 - acc: 0.5203 - recall_12: 0.2892 - precision_12: 0.5976 - val_loss: 1.5994 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 341/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9939 - acc: 0.5276 - recall_12: 0.2660 - precision_12: 0.6100\n",
            "Epoch 341: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9939 - acc: 0.5276 - recall_12: 0.2660 - precision_12: 0.6100 - val_loss: 1.6062 - val_acc: 0.6250 - val_recall_12: 0.4250 - val_precision_12: 0.6800\n",
            "Epoch 342/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9928 - acc: 0.5148 - recall_12: 0.2730 - precision_12: 0.6081\n",
            "Epoch 342: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9942 - acc: 0.5189 - recall_12: 0.2747 - precision_12: 0.6019 - val_loss: 1.5244 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6129\n",
            "Epoch 343/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9953 - acc: 0.5172 - recall_12: 0.3078 - precision_12: 0.5988\n",
            "Epoch 343: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9902 - acc: 0.5189 - recall_12: 0.3096 - precision_12: 0.6068 - val_loss: 1.5916 - val_acc: 0.6250 - val_recall_12: 0.3750 - val_precision_12: 0.6818\n",
            "Epoch 344/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9927 - acc: 0.5087 - recall_12: 0.2812 - precision_12: 0.6113\n",
            "Epoch 344: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9949 - acc: 0.5131 - recall_12: 0.2762 - precision_12: 0.6070 - val_loss: 1.5743 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.7037\n",
            "Epoch 345/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9899 - acc: 0.5149 - recall_12: 0.2753 - precision_12: 0.6066\n",
            "Epoch 345: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 13ms/step - loss: 0.9877 - acc: 0.5174 - recall_12: 0.2747 - precision_12: 0.6117 - val_loss: 1.5206 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 346/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9958 - acc: 0.5087 - recall_12: 0.2820 - precision_12: 0.6178\n",
            "Epoch 346: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9958 - acc: 0.5087 - recall_12: 0.2820 - precision_12: 0.6178 - val_loss: 1.5478 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 347/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9881 - acc: 0.5115 - recall_12: 0.3026 - precision_12: 0.6411\n",
            "Epoch 347: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9897 - acc: 0.5116 - recall_12: 0.2936 - precision_12: 0.6273 - val_loss: 1.6566 - val_acc: 0.5750 - val_recall_12: 0.4000 - val_precision_12: 0.7273\n",
            "Epoch 348/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9922 - acc: 0.5280 - recall_12: 0.2780 - precision_12: 0.6036\n",
            "Epoch 348: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9898 - acc: 0.5247 - recall_12: 0.2791 - precision_12: 0.6154 - val_loss: 1.5535 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 349/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9997 - acc: 0.5174 - recall_12: 0.2965 - precision_12: 0.6108\n",
            "Epoch 349: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9997 - acc: 0.5174 - recall_12: 0.2965 - precision_12: 0.6108 - val_loss: 1.5345 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6786\n",
            "Epoch 350/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9940 - acc: 0.5164 - recall_12: 0.2878 - precision_12: 0.6076\n",
            "Epoch 350: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9956 - acc: 0.5189 - recall_12: 0.2805 - precision_12: 0.6069 - val_loss: 1.5944 - val_acc: 0.6000 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 351/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9885 - acc: 0.5172 - recall_12: 0.3125 - precision_12: 0.6135\n",
            "Epoch 351: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9886 - acc: 0.5174 - recall_12: 0.3096 - precision_12: 0.6192 - val_loss: 1.5945 - val_acc: 0.6250 - val_recall_12: 0.4250 - val_precision_12: 0.6800\n",
            "Epoch 352/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9884 - acc: 0.5160 - recall_12: 0.3009 - precision_12: 0.6273\n",
            "Epoch 352: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9884 - acc: 0.5160 - recall_12: 0.3009 - precision_12: 0.6273 - val_loss: 1.5370 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 353/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9898 - acc: 0.5134 - recall_12: 0.2768 - precision_12: 0.5759\n",
            "Epoch 353: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9882 - acc: 0.5131 - recall_12: 0.2791 - precision_12: 0.5766 - val_loss: 1.6120 - val_acc: 0.6250 - val_recall_12: 0.4250 - val_precision_12: 0.6296\n",
            "Epoch 354/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9947 - acc: 0.5203 - recall_12: 0.2733 - precision_12: 0.5949\n",
            "Epoch 354: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9947 - acc: 0.5203 - recall_12: 0.2733 - precision_12: 0.5949 - val_loss: 1.4867 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6786\n",
            "Epoch 355/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9952 - acc: 0.5145 - recall_12: 0.2922 - precision_12: 0.5982\n",
            "Epoch 355: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9952 - acc: 0.5145 - recall_12: 0.2922 - precision_12: 0.5982 - val_loss: 1.5663 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 356/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9968 - acc: 0.5104 - recall_12: 0.2865 - precision_12: 0.6134\n",
            "Epoch 356: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9955 - acc: 0.5174 - recall_12: 0.2849 - precision_12: 0.6049 - val_loss: 1.5458 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6129\n",
            "Epoch 357/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9883 - acc: 0.5281 - recall_12: 0.3000 - precision_12: 0.6134\n",
            "Epoch 357: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9895 - acc: 0.5233 - recall_12: 0.2980 - precision_12: 0.6029 - val_loss: 1.5835 - val_acc: 0.6000 - val_recall_12: 0.4250 - val_precision_12: 0.6296\n",
            "Epoch 358/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 1.0014 - acc: 0.4918 - recall_12: 0.2730 - precision_12: 0.5993\n",
            "Epoch 358: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9963 - acc: 0.5015 - recall_12: 0.2834 - precision_12: 0.6056 - val_loss: 1.6202 - val_acc: 0.6000 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 359/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9908 - acc: 0.5031 - recall_12: 0.2953 - precision_12: 0.5925\n",
            "Epoch 359: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9918 - acc: 0.5029 - recall_12: 0.2980 - precision_12: 0.5977 - val_loss: 1.5770 - val_acc: 0.6250 - val_recall_12: 0.4250 - val_precision_12: 0.7083\n",
            "Epoch 360/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9988 - acc: 0.5016 - recall_12: 0.2862 - precision_12: 0.6304\n",
            "Epoch 360: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9943 - acc: 0.5044 - recall_12: 0.2907 - precision_12: 0.6349 - val_loss: 1.5925 - val_acc: 0.6000 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 361/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9960 - acc: 0.5174 - recall_12: 0.2552 - precision_12: 0.5927\n",
            "Epoch 361: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9924 - acc: 0.5116 - recall_12: 0.2587 - precision_12: 0.5993 - val_loss: 1.5591 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.5926\n",
            "Epoch 362/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9926 - acc: 0.5247 - recall_12: 0.2762 - precision_12: 0.6070\n",
            "Epoch 362: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9926 - acc: 0.5247 - recall_12: 0.2762 - precision_12: 0.6070 - val_loss: 1.5133 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 363/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9945 - acc: 0.5203 - recall_12: 0.2703 - precision_12: 0.6078\n",
            "Epoch 363: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9945 - acc: 0.5203 - recall_12: 0.2703 - precision_12: 0.6078 - val_loss: 1.5864 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6129\n",
            "Epoch 364/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9995 - acc: 0.5066 - recall_12: 0.2829 - precision_12: 0.5791\n",
            "Epoch 364: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9885 - acc: 0.5145 - recall_12: 0.2892 - precision_12: 0.5940 - val_loss: 1.5684 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 365/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9848 - acc: 0.5148 - recall_12: 0.2977 - precision_12: 0.6033\n",
            "Epoch 365: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9830 - acc: 0.5116 - recall_12: 0.2965 - precision_12: 0.6108 - val_loss: 1.5798 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 366/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9827 - acc: 0.5266 - recall_12: 0.3047 - precision_12: 0.6019\n",
            "Epoch 366: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9909 - acc: 0.5247 - recall_12: 0.3038 - precision_12: 0.5971 - val_loss: 1.5453 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 367/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9874 - acc: 0.5219 - recall_12: 0.2750 - precision_12: 0.6154\n",
            "Epoch 367: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9814 - acc: 0.5262 - recall_12: 0.2834 - precision_12: 0.6230 - val_loss: 1.6015 - val_acc: 0.6250 - val_recall_12: 0.4250 - val_precision_12: 0.6071\n",
            "Epoch 368/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9986 - acc: 0.5073 - recall_12: 0.2863 - precision_12: 0.5811\n",
            "Epoch 368: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9986 - acc: 0.5073 - recall_12: 0.2863 - precision_12: 0.5811 - val_loss: 1.6328 - val_acc: 0.5500 - val_recall_12: 0.4000 - val_precision_12: 0.7273\n",
            "Epoch 369/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9851 - acc: 0.5160 - recall_12: 0.2834 - precision_12: 0.6132\n",
            "Epoch 369: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9851 - acc: 0.5160 - recall_12: 0.2834 - precision_12: 0.6132 - val_loss: 1.5974 - val_acc: 0.5750 - val_recall_12: 0.4000 - val_precision_12: 0.6957\n",
            "Epoch 370/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9874 - acc: 0.5266 - recall_12: 0.2828 - precision_12: 0.6115\n",
            "Epoch 370: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9891 - acc: 0.5218 - recall_12: 0.2791 - precision_12: 0.6000 - val_loss: 1.6271 - val_acc: 0.5750 - val_recall_12: 0.4000 - val_precision_12: 0.7273\n",
            "Epoch 371/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9830 - acc: 0.5278 - recall_12: 0.2899 - precision_12: 0.5901\n",
            "Epoch 371: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9799 - acc: 0.5291 - recall_12: 0.2980 - precision_12: 0.5977 - val_loss: 1.5424 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 372/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9884 - acc: 0.5145 - recall_12: 0.3081 - precision_12: 0.6127\n",
            "Epoch 372: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9884 - acc: 0.5145 - recall_12: 0.3081 - precision_12: 0.6127 - val_loss: 1.5102 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6429\n",
            "Epoch 373/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9785 - acc: 0.5296 - recall_12: 0.3141 - precision_12: 0.6346\n",
            "Epoch 373: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9881 - acc: 0.5218 - recall_12: 0.3052 - precision_12: 0.6325 - val_loss: 1.5885 - val_acc: 0.6250 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 374/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9834 - acc: 0.5193 - recall_12: 0.2768 - precision_12: 0.6179\n",
            "Epoch 374: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9843 - acc: 0.5203 - recall_12: 0.2733 - precision_12: 0.6164 - val_loss: 1.6811 - val_acc: 0.5500 - val_recall_12: 0.4000 - val_precision_12: 0.8000\n",
            "Epoch 375/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9871 - acc: 0.5247 - recall_12: 0.2812 - precision_12: 0.6151\n",
            "Epoch 375: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9834 - acc: 0.5291 - recall_12: 0.2892 - precision_12: 0.6317 - val_loss: 1.5421 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 376/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9931 - acc: 0.5298 - recall_12: 0.2946 - precision_12: 0.5910\n",
            "Epoch 376: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9915 - acc: 0.5291 - recall_12: 0.2951 - precision_12: 0.5901 - val_loss: 1.5143 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 377/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9995 - acc: 0.5145 - recall_12: 0.2965 - precision_12: 0.6239\n",
            "Epoch 377: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9995 - acc: 0.5145 - recall_12: 0.2965 - precision_12: 0.6239 - val_loss: 1.5083 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 378/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9805 - acc: 0.5391 - recall_12: 0.2953 - precision_12: 0.6156\n",
            "Epoch 378: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9817 - acc: 0.5363 - recall_12: 0.2951 - precision_12: 0.6189 - val_loss: 1.5408 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6129\n",
            "Epoch 379/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9841 - acc: 0.5156 - recall_12: 0.3109 - precision_12: 0.6104\n",
            "Epoch 379: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9841 - acc: 0.5131 - recall_12: 0.3067 - precision_12: 0.6134 - val_loss: 1.6226 - val_acc: 0.6000 - val_recall_12: 0.3750 - val_precision_12: 0.8824\n",
            "Epoch 380/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9715 - acc: 0.5280 - recall_12: 0.2812 - precision_12: 0.6173\n",
            "Epoch 380: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9782 - acc: 0.5247 - recall_12: 0.2820 - precision_12: 0.6101 - val_loss: 1.5453 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 381/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9781 - acc: 0.5283 - recall_12: 0.3051 - precision_12: 0.6119\n",
            "Epoch 381: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9824 - acc: 0.5247 - recall_12: 0.3009 - precision_12: 0.6106 - val_loss: 1.6095 - val_acc: 0.6000 - val_recall_12: 0.3750 - val_precision_12: 0.8824\n",
            "Epoch 382/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9972 - acc: 0.5312 - recall_12: 0.2681 - precision_12: 0.6037\n",
            "Epoch 382: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9916 - acc: 0.5291 - recall_12: 0.2747 - precision_12: 0.6197 - val_loss: 1.5439 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 383/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9897 - acc: 0.5283 - recall_12: 0.2991 - precision_12: 0.5964\n",
            "Epoch 383: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9864 - acc: 0.5334 - recall_12: 0.2980 - precision_12: 0.6012 - val_loss: 1.6152 - val_acc: 0.6000 - val_recall_12: 0.4000 - val_precision_12: 0.8000\n",
            "Epoch 384/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9894 - acc: 0.5131 - recall_12: 0.2791 - precision_12: 0.6174\n",
            "Epoch 384: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9894 - acc: 0.5131 - recall_12: 0.2791 - precision_12: 0.6174 - val_loss: 1.5275 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 385/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9643 - acc: 0.5378 - recall_12: 0.3207 - precision_12: 0.6132\n",
            "Epoch 385: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9746 - acc: 0.5291 - recall_12: 0.3081 - precision_12: 0.6023 - val_loss: 1.5791 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 386/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9833 - acc: 0.5164 - recall_12: 0.2976 - precision_12: 0.6349\n",
            "Epoch 386: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9864 - acc: 0.5145 - recall_12: 0.2951 - precision_12: 0.6285 - val_loss: 1.5889 - val_acc: 0.6250 - val_recall_12: 0.4250 - val_precision_12: 0.6296\n",
            "Epoch 387/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 1.0026 - acc: 0.5016 - recall_12: 0.2484 - precision_12: 0.5933\n",
            "Epoch 387: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9956 - acc: 0.5087 - recall_12: 0.2544 - precision_12: 0.6014 - val_loss: 1.5552 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 388/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9878 - acc: 0.5145 - recall_12: 0.2936 - precision_12: 0.5805\n",
            "Epoch 388: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9878 - acc: 0.5145 - recall_12: 0.2936 - precision_12: 0.5805 - val_loss: 1.5756 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 389/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9849 - acc: 0.5203 - recall_12: 0.3038 - precision_12: 0.6058\n",
            "Epoch 389: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9849 - acc: 0.5203 - recall_12: 0.3038 - precision_12: 0.6058 - val_loss: 1.5755 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 390/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9847 - acc: 0.5263 - recall_12: 0.2780 - precision_12: 0.6190\n",
            "Epoch 390: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9823 - acc: 0.5276 - recall_12: 0.2820 - precision_12: 0.6139 - val_loss: 1.5599 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6129\n",
            "Epoch 391/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9704 - acc: 0.5329 - recall_12: 0.2961 - precision_12: 0.6228\n",
            "Epoch 391: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9740 - acc: 0.5320 - recall_12: 0.2922 - precision_12: 0.6147 - val_loss: 1.5897 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 392/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9731 - acc: 0.5164 - recall_12: 0.3110 - precision_12: 0.6257\n",
            "Epoch 392: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9780 - acc: 0.5116 - recall_12: 0.3067 - precision_12: 0.6170 - val_loss: 1.5817 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 393/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9710 - acc: 0.5296 - recall_12: 0.3010 - precision_12: 0.6203\n",
            "Epoch 393: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9749 - acc: 0.5247 - recall_12: 0.3023 - precision_12: 0.6118 - val_loss: 1.5632 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 394/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9764 - acc: 0.5214 - recall_12: 0.3158 - precision_12: 0.6358\n",
            "Epoch 394: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9774 - acc: 0.5189 - recall_12: 0.3081 - precision_12: 0.6272 - val_loss: 1.6152 - val_acc: 0.5750 - val_recall_12: 0.4000 - val_precision_12: 0.7619\n",
            "Epoch 395/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9879 - acc: 0.5434 - recall_12: 0.2830 - precision_12: 0.6015\n",
            "Epoch 395: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9827 - acc: 0.5451 - recall_12: 0.2994 - precision_12: 0.6205 - val_loss: 1.5349 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 396/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9803 - acc: 0.5203 - recall_12: 0.2936 - precision_12: 0.6215\n",
            "Epoch 396: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9803 - acc: 0.5203 - recall_12: 0.2936 - precision_12: 0.6215 - val_loss: 1.5284 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 397/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9898 - acc: 0.5119 - recall_12: 0.2961 - precision_12: 0.6378\n",
            "Epoch 397: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9915 - acc: 0.5116 - recall_12: 0.2994 - precision_12: 0.6338 - val_loss: 1.5925 - val_acc: 0.5500 - val_recall_12: 0.4000 - val_precision_12: 0.7273\n",
            "Epoch 398/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9814 - acc: 0.5087 - recall_12: 0.3038 - precision_12: 0.6341\n",
            "Epoch 398: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9863 - acc: 0.5058 - recall_12: 0.2922 - precision_12: 0.6128 - val_loss: 1.6113 - val_acc: 0.5500 - val_recall_12: 0.4000 - val_precision_12: 0.6667\n",
            "Epoch 399/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9764 - acc: 0.5291 - recall_12: 0.3067 - precision_12: 0.6261\n",
            "Epoch 399: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9764 - acc: 0.5291 - recall_12: 0.3067 - precision_12: 0.6261 - val_loss: 1.5932 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 400/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9804 - acc: 0.5214 - recall_12: 0.2829 - precision_12: 0.6099\n",
            "Epoch 400: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9762 - acc: 0.5291 - recall_12: 0.2907 - precision_12: 0.6270 - val_loss: 1.5467 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6129\n",
            "Epoch 401/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9873 - acc: 0.5082 - recall_12: 0.3092 - precision_12: 0.6330\n",
            "Epoch 401: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9806 - acc: 0.5116 - recall_12: 0.3081 - precision_12: 0.6386 - val_loss: 1.5687 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 402/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9787 - acc: 0.5417 - recall_12: 0.3155 - precision_12: 0.6092\n",
            "Epoch 402: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9794 - acc: 0.5422 - recall_12: 0.3154 - precision_12: 0.6061 - val_loss: 1.5580 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 403/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9886 - acc: 0.5115 - recall_12: 0.3043 - precision_12: 0.6086\n",
            "Epoch 403: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.9818 - acc: 0.5145 - recall_12: 0.3052 - precision_12: 0.6176 - val_loss: 1.5316 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 404/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9755 - acc: 0.5191 - recall_12: 0.3090 - precision_12: 0.6224\n",
            "Epoch 404: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9766 - acc: 0.5247 - recall_12: 0.2951 - precision_12: 0.6227 - val_loss: 1.5965 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 405/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9876 - acc: 0.5312 - recall_12: 0.2778 - precision_12: 0.5970\n",
            "Epoch 405: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9873 - acc: 0.5392 - recall_12: 0.2820 - precision_12: 0.6006 - val_loss: 1.5472 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 406/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9736 - acc: 0.5291 - recall_12: 0.2863 - precision_12: 0.6375\n",
            "Epoch 406: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9736 - acc: 0.5291 - recall_12: 0.2863 - precision_12: 0.6375 - val_loss: 1.5985 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 407/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9852 - acc: 0.5115 - recall_12: 0.3109 - precision_12: 0.6342\n",
            "Epoch 407: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9739 - acc: 0.5160 - recall_12: 0.3183 - precision_12: 0.6385 - val_loss: 1.6123 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 408/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9644 - acc: 0.5203 - recall_12: 0.3096 - precision_12: 0.6302\n",
            "Epoch 408: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9644 - acc: 0.5203 - recall_12: 0.3096 - precision_12: 0.6302 - val_loss: 1.6030 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 409/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9669 - acc: 0.5344 - recall_12: 0.3156 - precision_12: 0.6516\n",
            "Epoch 409: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9695 - acc: 0.5334 - recall_12: 0.3154 - precision_12: 0.6420 - val_loss: 1.6396 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 410/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9861 - acc: 0.5074 - recall_12: 0.2991 - precision_12: 0.5877\n",
            "Epoch 410: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9861 - acc: 0.5073 - recall_12: 0.3009 - precision_12: 0.5931 - val_loss: 1.5661 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 411/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9769 - acc: 0.5218 - recall_12: 0.2805 - precision_12: 0.6349\n",
            "Epoch 411: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9769 - acc: 0.5218 - recall_12: 0.2805 - precision_12: 0.6349 - val_loss: 1.6326 - val_acc: 0.5250 - val_recall_12: 0.4250 - val_precision_12: 0.6538\n",
            "Epoch 412/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9674 - acc: 0.5295 - recall_12: 0.3229 - precision_12: 0.6263\n",
            "Epoch 412: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9757 - acc: 0.5291 - recall_12: 0.3154 - precision_12: 0.6165 - val_loss: 1.6149 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 413/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9800 - acc: 0.5203 - recall_12: 0.3067 - precision_12: 0.5927\n",
            "Epoch 413: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9800 - acc: 0.5203 - recall_12: 0.3067 - precision_12: 0.5927 - val_loss: 1.6005 - val_acc: 0.5750 - val_recall_12: 0.4250 - val_precision_12: 0.7391\n",
            "Epoch 414/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9636 - acc: 0.5312 - recall_12: 0.2977 - precision_12: 0.6534\n",
            "Epoch 414: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9795 - acc: 0.5218 - recall_12: 0.2936 - precision_12: 0.6273 - val_loss: 1.5939 - val_acc: 0.5500 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 415/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9734 - acc: 0.5266 - recall_12: 0.2937 - precision_12: 0.6438\n",
            "Epoch 415: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9800 - acc: 0.5131 - recall_12: 0.2936 - precision_12: 0.6372 - val_loss: 1.5726 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 416/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9735 - acc: 0.5363 - recall_12: 0.2994 - precision_12: 0.6438\n",
            "Epoch 416: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9735 - acc: 0.5363 - recall_12: 0.2994 - precision_12: 0.6438 - val_loss: 1.5959 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 417/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9854 - acc: 0.5226 - recall_12: 0.2726 - precision_12: 0.6109\n",
            "Epoch 417: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9824 - acc: 0.5218 - recall_12: 0.2834 - precision_12: 0.6210 - val_loss: 1.5537 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 418/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9731 - acc: 0.5392 - recall_12: 0.2965 - precision_12: 0.6108\n",
            "Epoch 418: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9731 - acc: 0.5392 - recall_12: 0.2965 - precision_12: 0.6108 - val_loss: 1.5635 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 419/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9850 - acc: 0.5156 - recall_12: 0.2760 - precision_12: 0.6069\n",
            "Epoch 419: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9854 - acc: 0.5189 - recall_12: 0.2878 - precision_12: 0.6130 - val_loss: 1.5785 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 420/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9685 - acc: 0.5375 - recall_12: 0.3063 - precision_12: 0.6323\n",
            "Epoch 420: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9781 - acc: 0.5305 - recall_12: 0.2980 - precision_12: 0.6156 - val_loss: 1.5735 - val_acc: 0.5750 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 421/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9666 - acc: 0.5375 - recall_12: 0.3187 - precision_12: 0.6296\n",
            "Epoch 421: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9831 - acc: 0.5262 - recall_12: 0.3081 - precision_12: 0.6181 - val_loss: 1.5598 - val_acc: 0.5750 - val_recall_12: 0.4250 - val_precision_12: 0.6800\n",
            "Epoch 422/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9742 - acc: 0.5160 - recall_12: 0.2791 - precision_12: 0.6761\n",
            "Epoch 422: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9742 - acc: 0.5160 - recall_12: 0.2791 - precision_12: 0.6761 - val_loss: 1.5507 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 423/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9686 - acc: 0.5444 - recall_12: 0.3026 - precision_12: 0.6195\n",
            "Epoch 423: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9664 - acc: 0.5451 - recall_12: 0.3067 - precision_12: 0.6206 - val_loss: 1.5769 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 424/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9724 - acc: 0.5392 - recall_12: 0.3198 - precision_12: 0.6250\n",
            "Epoch 424: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9724 - acc: 0.5392 - recall_12: 0.3198 - precision_12: 0.6250 - val_loss: 1.5708 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 425/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9852 - acc: 0.5247 - recall_12: 0.2911 - precision_12: 0.6082\n",
            "Epoch 425: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9785 - acc: 0.5320 - recall_12: 0.2965 - precision_12: 0.6182 - val_loss: 1.5503 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6129\n",
            "Epoch 426/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9764 - acc: 0.5395 - recall_12: 0.3059 - precision_12: 0.6327\n",
            "Epoch 426: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9679 - acc: 0.5436 - recall_12: 0.3081 - precision_12: 0.6405 - val_loss: 1.6252 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 427/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9741 - acc: 0.5268 - recall_12: 0.3036 - precision_12: 0.6108\n",
            "Epoch 427: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9727 - acc: 0.5276 - recall_12: 0.3038 - precision_12: 0.6111 - val_loss: 1.6276 - val_acc: 0.5500 - val_recall_12: 0.4250 - val_precision_12: 0.6296\n",
            "Epoch 428/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9622 - acc: 0.5609 - recall_12: 0.3207 - precision_12: 0.6113\n",
            "Epoch 428: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9755 - acc: 0.5407 - recall_12: 0.3067 - precision_12: 0.5960 - val_loss: 1.5318 - val_acc: 0.6250 - val_recall_12: 0.4250 - val_precision_12: 0.6800\n",
            "Epoch 429/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9793 - acc: 0.5233 - recall_12: 0.2878 - precision_12: 0.6556\n",
            "Epoch 429: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9793 - acc: 0.5233 - recall_12: 0.2878 - precision_12: 0.6556 - val_loss: 1.5771 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 430/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9728 - acc: 0.5320 - recall_12: 0.3140 - precision_12: 0.6261\n",
            "Epoch 430: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9728 - acc: 0.5320 - recall_12: 0.3140 - precision_12: 0.6261 - val_loss: 1.5157 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 431/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9788 - acc: 0.5145 - recall_12: 0.2863 - precision_12: 0.6215\n",
            "Epoch 431: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9788 - acc: 0.5145 - recall_12: 0.2863 - precision_12: 0.6215 - val_loss: 1.5495 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 432/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9790 - acc: 0.5305 - recall_12: 0.2965 - precision_12: 0.6201\n",
            "Epoch 432: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9790 - acc: 0.5305 - recall_12: 0.2965 - precision_12: 0.6201 - val_loss: 1.5475 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 433/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9703 - acc: 0.5193 - recall_12: 0.2946 - precision_12: 0.6450\n",
            "Epoch 433: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9712 - acc: 0.5189 - recall_12: 0.2951 - precision_12: 0.6404 - val_loss: 1.5981 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 434/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9790 - acc: 0.5280 - recall_12: 0.2944 - precision_12: 0.6130\n",
            "Epoch 434: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9759 - acc: 0.5218 - recall_12: 0.2907 - precision_12: 0.6098 - val_loss: 1.5995 - val_acc: 0.5500 - val_recall_12: 0.4250 - val_precision_12: 0.7083\n",
            "Epoch 435/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9682 - acc: 0.5334 - recall_12: 0.3023 - precision_12: 0.6284\n",
            "Epoch 435: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9682 - acc: 0.5334 - recall_12: 0.3023 - precision_12: 0.6284 - val_loss: 1.6026 - val_acc: 0.5500 - val_recall_12: 0.4250 - val_precision_12: 0.7083\n",
            "Epoch 436/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9701 - acc: 0.5349 - recall_12: 0.2922 - precision_12: 0.6204\n",
            "Epoch 436: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9701 - acc: 0.5349 - recall_12: 0.2922 - precision_12: 0.6204 - val_loss: 1.5399 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 437/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9635 - acc: 0.5218 - recall_12: 0.3140 - precision_12: 0.6506\n",
            "Epoch 437: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9635 - acc: 0.5218 - recall_12: 0.3140 - precision_12: 0.6506 - val_loss: 1.5767 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 438/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9616 - acc: 0.5253 - recall_12: 0.3036 - precision_12: 0.6435\n",
            "Epoch 438: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9671 - acc: 0.5218 - recall_12: 0.3009 - precision_12: 0.6389 - val_loss: 1.5736 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 439/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9468 - acc: 0.5451 - recall_12: 0.3160 - precision_12: 0.6594\n",
            "Epoch 439: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9634 - acc: 0.5262 - recall_12: 0.3081 - precision_12: 0.6386 - val_loss: 1.5507 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 440/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9586 - acc: 0.5407 - recall_12: 0.3023 - precision_12: 0.6624\n",
            "Epoch 440: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9586 - acc: 0.5407 - recall_12: 0.3023 - precision_12: 0.6624 - val_loss: 1.6123 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 441/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9694 - acc: 0.5378 - recall_12: 0.3454 - precision_12: 0.6105\n",
            "Epoch 441: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9716 - acc: 0.5378 - recall_12: 0.3358 - precision_12: 0.6111 - val_loss: 1.6246 - val_acc: 0.5500 - val_recall_12: 0.3750 - val_precision_12: 0.7895\n",
            "Epoch 442/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9682 - acc: 0.5089 - recall_12: 0.2991 - precision_12: 0.6442\n",
            "Epoch 442: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9685 - acc: 0.5102 - recall_12: 0.2965 - precision_12: 0.6435 - val_loss: 1.6132 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 443/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9722 - acc: 0.5349 - recall_12: 0.3110 - precision_12: 0.6239\n",
            "Epoch 443: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9722 - acc: 0.5349 - recall_12: 0.3110 - precision_12: 0.6239 - val_loss: 1.5950 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 444/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9670 - acc: 0.5349 - recall_12: 0.3183 - precision_12: 0.6257\n",
            "Epoch 444: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9670 - acc: 0.5349 - recall_12: 0.3183 - precision_12: 0.6257 - val_loss: 1.5679 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 445/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9840 - acc: 0.5156 - recall_12: 0.2937 - precision_12: 0.6124\n",
            "Epoch 445: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9780 - acc: 0.5160 - recall_12: 0.2951 - precision_12: 0.6189 - val_loss: 1.5673 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 446/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9591 - acc: 0.5334 - recall_12: 0.3125 - precision_12: 0.6232\n",
            "Epoch 446: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9591 - acc: 0.5334 - recall_12: 0.3125 - precision_12: 0.6232 - val_loss: 1.6147 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 447/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9543 - acc: 0.5359 - recall_12: 0.3234 - precision_12: 0.6530\n",
            "Epoch 447: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9603 - acc: 0.5305 - recall_12: 0.3198 - precision_12: 0.6433 - val_loss: 1.5742 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 448/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9828 - acc: 0.5234 - recall_12: 0.2859 - precision_12: 0.6100\n",
            "Epoch 448: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9730 - acc: 0.5305 - recall_12: 0.2922 - precision_12: 0.6262 - val_loss: 1.5640 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5588\n",
            "Epoch 449/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9670 - acc: 0.5116 - recall_12: 0.3212 - precision_12: 0.6243\n",
            "Epoch 449: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9670 - acc: 0.5116 - recall_12: 0.3212 - precision_12: 0.6243 - val_loss: 1.5854 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 450/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9586 - acc: 0.5510 - recall_12: 0.3224 - precision_12: 0.6343\n",
            "Epoch 450: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9659 - acc: 0.5407 - recall_12: 0.3183 - precision_12: 0.6275 - val_loss: 1.6160 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 451/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9679 - acc: 0.5203 - recall_12: 0.2922 - precision_12: 0.6381\n",
            "Epoch 451: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9679 - acc: 0.5203 - recall_12: 0.2922 - precision_12: 0.6381 - val_loss: 1.6249 - val_acc: 0.5500 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 452/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9572 - acc: 0.5312 - recall_12: 0.3140 - precision_12: 0.6188\n",
            "Epoch 452: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 17ms/step - loss: 0.9577 - acc: 0.5334 - recall_12: 0.3140 - precision_12: 0.6189 - val_loss: 1.6457 - val_acc: 0.5500 - val_recall_12: 0.4250 - val_precision_12: 0.7391\n",
            "Epoch 453/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9631 - acc: 0.5446 - recall_12: 0.3051 - precision_12: 0.6550\n",
            "Epoch 453: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9638 - acc: 0.5422 - recall_12: 0.3052 - precision_12: 0.6542 - val_loss: 1.6092 - val_acc: 0.5500 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 454/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9670 - acc: 0.5320 - recall_12: 0.3052 - precision_12: 0.6344\n",
            "Epoch 454: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9670 - acc: 0.5320 - recall_12: 0.3052 - precision_12: 0.6344 - val_loss: 1.6140 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6923\n",
            "Epoch 455/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9519 - acc: 0.5461 - recall_12: 0.2928 - precision_12: 0.6496\n",
            "Epoch 455: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9632 - acc: 0.5363 - recall_12: 0.2878 - precision_12: 0.6286 - val_loss: 1.5921 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 456/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9751 - acc: 0.5164 - recall_12: 0.3109 - precision_12: 0.6077\n",
            "Epoch 456: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9694 - acc: 0.5233 - recall_12: 0.3125 - precision_12: 0.6196 - val_loss: 1.5470 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 457/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9647 - acc: 0.5218 - recall_12: 0.3125 - precision_12: 0.6437\n",
            "Epoch 457: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9647 - acc: 0.5218 - recall_12: 0.3125 - precision_12: 0.6437 - val_loss: 1.6315 - val_acc: 0.5500 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 458/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9642 - acc: 0.5281 - recall_12: 0.3031 - precision_12: 0.6424\n",
            "Epoch 458: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9594 - acc: 0.5247 - recall_12: 0.3081 - precision_12: 0.6503 - val_loss: 1.5476 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5588\n",
            "Epoch 459/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9695 - acc: 0.5461 - recall_12: 0.3092 - precision_12: 0.6395\n",
            "Epoch 459: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9652 - acc: 0.5523 - recall_12: 0.3052 - precision_12: 0.6383 - val_loss: 1.6020 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 460/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9585 - acc: 0.5330 - recall_12: 0.3177 - precision_12: 0.6310\n",
            "Epoch 460: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9566 - acc: 0.5291 - recall_12: 0.3227 - precision_12: 0.6325 - val_loss: 1.5740 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5758\n",
            "Epoch 461/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9627 - acc: 0.5078 - recall_12: 0.3219 - precision_12: 0.6149\n",
            "Epoch 461: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9603 - acc: 0.5073 - recall_12: 0.3183 - precision_12: 0.6204 - val_loss: 1.6027 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 462/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9644 - acc: 0.5503 - recall_12: 0.3056 - precision_12: 0.6447\n",
            "Epoch 462: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9556 - acc: 0.5596 - recall_12: 0.3096 - precision_12: 0.6594 - val_loss: 1.5496 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5588\n",
            "Epoch 463/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9634 - acc: 0.5276 - recall_12: 0.3052 - precision_12: 0.6231\n",
            "Epoch 463: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9634 - acc: 0.5276 - recall_12: 0.3052 - precision_12: 0.6231 - val_loss: 1.6020 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 464/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9692 - acc: 0.5296 - recall_12: 0.3141 - precision_12: 0.6181\n",
            "Epoch 464: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9669 - acc: 0.5320 - recall_12: 0.3140 - precision_12: 0.6243 - val_loss: 1.6563 - val_acc: 0.5500 - val_recall_12: 0.3750 - val_precision_12: 0.7895\n",
            "Epoch 465/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9733 - acc: 0.5417 - recall_12: 0.2932 - precision_12: 0.6137\n",
            "Epoch 465: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9731 - acc: 0.5422 - recall_12: 0.2936 - precision_12: 0.6177 - val_loss: 1.5772 - val_acc: 0.5500 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 466/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9652 - acc: 0.5208 - recall_12: 0.2961 - precision_12: 0.6338\n",
            "Epoch 466: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9645 - acc: 0.5218 - recall_12: 0.3023 - precision_12: 0.6380 - val_loss: 1.5879 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 467/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9522 - acc: 0.5238 - recall_12: 0.3185 - precision_12: 0.6185\n",
            "Epoch 467: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9524 - acc: 0.5218 - recall_12: 0.3183 - precision_12: 0.6169 - val_loss: 1.6227 - val_acc: 0.5750 - val_recall_12: 0.4500 - val_precision_12: 0.6429\n",
            "Epoch 468/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9509 - acc: 0.5559 - recall_12: 0.3076 - precision_12: 0.6404\n",
            "Epoch 468: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9506 - acc: 0.5509 - recall_12: 0.3038 - precision_12: 0.6239 - val_loss: 1.6242 - val_acc: 0.5250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 469/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9655 - acc: 0.5357 - recall_12: 0.2976 - precision_12: 0.6061\n",
            "Epoch 469: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9637 - acc: 0.5334 - recall_12: 0.2994 - precision_12: 0.6095 - val_loss: 1.5666 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 470/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9762 - acc: 0.5164 - recall_12: 0.3092 - precision_12: 0.6205\n",
            "Epoch 470: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9661 - acc: 0.5262 - recall_12: 0.3154 - precision_12: 0.6290 - val_loss: 1.5732 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 471/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9643 - acc: 0.5334 - recall_12: 0.3125 - precision_12: 0.6232\n",
            "Epoch 471: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9643 - acc: 0.5334 - recall_12: 0.3125 - precision_12: 0.6232 - val_loss: 1.5782 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 472/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9687 - acc: 0.5139 - recall_12: 0.3021 - precision_12: 0.6374\n",
            "Epoch 472: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9630 - acc: 0.5247 - recall_12: 0.3067 - precision_12: 0.6375 - val_loss: 1.5907 - val_acc: 0.5500 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 473/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9604 - acc: 0.5148 - recall_12: 0.2993 - precision_12: 0.6254\n",
            "Epoch 473: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9616 - acc: 0.5189 - recall_12: 0.3023 - precision_12: 0.6303 - val_loss: 1.5966 - val_acc: 0.5500 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 474/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9591 - acc: 0.5363 - recall_12: 0.3241 - precision_12: 0.6501\n",
            "Epoch 474: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9591 - acc: 0.5363 - recall_12: 0.3241 - precision_12: 0.6501 - val_loss: 1.5747 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 475/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9542 - acc: 0.5493 - recall_12: 0.3503 - precision_12: 0.6283\n",
            "Epoch 475: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9622 - acc: 0.5378 - recall_12: 0.3358 - precision_12: 0.6226 - val_loss: 1.5706 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.6333\n",
            "Epoch 476/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9579 - acc: 0.5378 - recall_12: 0.3081 - precision_12: 0.6163\n",
            "Epoch 476: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9579 - acc: 0.5378 - recall_12: 0.3081 - precision_12: 0.6163 - val_loss: 1.6021 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 477/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9613 - acc: 0.5387 - recall_12: 0.3333 - precision_12: 0.6310\n",
            "Epoch 477: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9595 - acc: 0.5422 - recall_12: 0.3358 - precision_12: 0.6346 - val_loss: 1.5695 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 478/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9441 - acc: 0.5480 - recall_12: 0.3154 - precision_12: 0.6308\n",
            "Epoch 478: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9441 - acc: 0.5480 - recall_12: 0.3154 - precision_12: 0.6308 - val_loss: 1.6069 - val_acc: 0.6000 - val_recall_12: 0.4500 - val_precision_12: 0.6429\n",
            "Epoch 479/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9604 - acc: 0.5193 - recall_12: 0.3080 - precision_12: 0.6179\n",
            "Epoch 479: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9568 - acc: 0.5218 - recall_12: 0.3110 - precision_12: 0.6185 - val_loss: 1.5828 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5758\n",
            "Epoch 480/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9604 - acc: 0.5451 - recall_12: 0.3299 - precision_12: 0.6449\n",
            "Epoch 480: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9604 - acc: 0.5451 - recall_12: 0.3299 - precision_12: 0.6449 - val_loss: 1.5738 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 481/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9636 - acc: 0.5223 - recall_12: 0.3259 - precision_12: 0.6204\n",
            "Epoch 481: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9621 - acc: 0.5262 - recall_12: 0.3212 - precision_12: 0.6225 - val_loss: 1.5924 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 482/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9480 - acc: 0.5391 - recall_12: 0.3297 - precision_12: 0.6553\n",
            "Epoch 482: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9511 - acc: 0.5392 - recall_12: 0.3285 - precision_12: 0.6570 - val_loss: 1.6253 - val_acc: 0.5500 - val_recall_12: 0.4750 - val_precision_12: 0.6552\n",
            "Epoch 483/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9588 - acc: 0.5378 - recall_12: 0.3227 - precision_12: 0.6271\n",
            "Epoch 483: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9588 - acc: 0.5378 - recall_12: 0.3227 - precision_12: 0.6271 - val_loss: 1.6289 - val_acc: 0.5500 - val_recall_12: 0.4500 - val_precision_12: 0.7200\n",
            "Epoch 484/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9536 - acc: 0.5372 - recall_12: 0.3185 - precision_12: 0.6369\n",
            "Epoch 484: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9573 - acc: 0.5291 - recall_12: 0.3140 - precision_12: 0.6279 - val_loss: 1.6240 - val_acc: 0.5500 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 485/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9712 - acc: 0.5359 - recall_12: 0.3016 - precision_12: 0.6186\n",
            "Epoch 485: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9629 - acc: 0.5422 - recall_12: 0.3038 - precision_12: 0.6220 - val_loss: 1.5844 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5588\n",
            "Epoch 486/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9543 - acc: 0.5417 - recall_12: 0.3333 - precision_12: 0.6418\n",
            "Epoch 486: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9555 - acc: 0.5407 - recall_12: 0.3328 - precision_12: 0.6379 - val_loss: 1.5877 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 487/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9473 - acc: 0.5395 - recall_12: 0.3092 - precision_12: 0.6330\n",
            "Epoch 487: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 14ms/step - loss: 0.9548 - acc: 0.5334 - recall_12: 0.3038 - precision_12: 0.6295 - val_loss: 1.6762 - val_acc: 0.5250 - val_recall_12: 0.4000 - val_precision_12: 0.8000\n",
            "Epoch 488/500\n",
            "19/22 [========================>.....] - ETA: 0s - loss: 0.9708 - acc: 0.5263 - recall_12: 0.3076 - precision_12: 0.6131\n",
            "Epoch 488: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9708 - acc: 0.5262 - recall_12: 0.3009 - precision_12: 0.6216 - val_loss: 1.5675 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 489/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9652 - acc: 0.5218 - recall_12: 0.3241 - precision_12: 0.6160\n",
            "Epoch 489: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9652 - acc: 0.5218 - recall_12: 0.3241 - precision_12: 0.6160 - val_loss: 1.5580 - val_acc: 0.5500 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 490/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9469 - acc: 0.5538 - recall_12: 0.3154 - precision_12: 0.6420\n",
            "Epoch 490: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9469 - acc: 0.5538 - recall_12: 0.3154 - precision_12: 0.6420 - val_loss: 1.5641 - val_acc: 0.5500 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 491/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9544 - acc: 0.5342 - recall_12: 0.3125 - precision_12: 0.6287\n",
            "Epoch 491: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9525 - acc: 0.5378 - recall_12: 0.3140 - precision_12: 0.6297 - val_loss: 1.5709 - val_acc: 0.5750 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 492/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9636 - acc: 0.5203 - recall_12: 0.3227 - precision_12: 0.6271\n",
            "Epoch 492: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9636 - acc: 0.5203 - recall_12: 0.3227 - precision_12: 0.6271 - val_loss: 1.6125 - val_acc: 0.5500 - val_recall_12: 0.4250 - val_precision_12: 0.7083\n",
            "Epoch 493/500\n",
            "20/22 [==========================>...] - ETA: 0s - loss: 0.9516 - acc: 0.5359 - recall_12: 0.3016 - precision_12: 0.6477\n",
            "Epoch 493: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9479 - acc: 0.5334 - recall_12: 0.3067 - precision_12: 0.6512 - val_loss: 1.5943 - val_acc: 0.5500 - val_recall_12: 0.4750 - val_precision_12: 0.6129\n",
            "Epoch 494/500\n",
            "18/22 [=======================>......] - ETA: 0s - loss: 0.9462 - acc: 0.5521 - recall_12: 0.3281 - precision_12: 0.6342\n",
            "Epoch 494: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9498 - acc: 0.5407 - recall_12: 0.3212 - precision_12: 0.6208 - val_loss: 1.5724 - val_acc: 0.6000 - val_recall_12: 0.4750 - val_precision_12: 0.5758\n",
            "Epoch 495/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9495 - acc: 0.5494 - recall_12: 0.3081 - precision_12: 0.6386\n",
            "Epoch 495: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9495 - acc: 0.5494 - recall_12: 0.3081 - precision_12: 0.6386 - val_loss: 1.5290 - val_acc: 0.6250 - val_recall_12: 0.4750 - val_precision_12: 0.5588\n",
            "Epoch 496/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9550 - acc: 0.5378 - recall_12: 0.3241 - precision_12: 0.6371\n",
            "Epoch 496: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9550 - acc: 0.5378 - recall_12: 0.3241 - precision_12: 0.6371 - val_loss: 1.6190 - val_acc: 0.5500 - val_recall_12: 0.4500 - val_precision_12: 0.6667\n",
            "Epoch 497/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9553 - acc: 0.5298 - recall_12: 0.3110 - precision_12: 0.6391\n",
            "Epoch 497: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9538 - acc: 0.5334 - recall_12: 0.3140 - precision_12: 0.6409 - val_loss: 1.5867 - val_acc: 0.5250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 498/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9670 - acc: 0.5327 - recall_12: 0.3021 - precision_12: 0.6344\n",
            "Epoch 498: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 15ms/step - loss: 0.9637 - acc: 0.5363 - recall_12: 0.3067 - precision_12: 0.6375 - val_loss: 1.6250 - val_acc: 0.5250 - val_recall_12: 0.4750 - val_precision_12: 0.5938\n",
            "Epoch 499/500\n",
            "21/22 [===========================>..] - ETA: 0s - loss: 0.9480 - acc: 0.5491 - recall_12: 0.3244 - precision_12: 0.6412\n",
            "Epoch 499: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9493 - acc: 0.5465 - recall_12: 0.3227 - precision_12: 0.6416 - val_loss: 1.6673 - val_acc: 0.5250 - val_recall_12: 0.4250 - val_precision_12: 0.6800\n",
            "Epoch 500/500\n",
            "22/22 [==============================] - ETA: 0s - loss: 0.9534 - acc: 0.5334 - recall_12: 0.3256 - precision_12: 0.6493\n",
            "Epoch 500: val_loss did not improve from 1.22515\n",
            "22/22 [==============================] - 0s 16ms/step - loss: 0.9534 - acc: 0.5334 - recall_12: 0.3256 - precision_12: 0.6493 - val_loss: 1.5691 - val_acc: 0.6250 - val_recall_12: 0.4500 - val_precision_12: 0.6207\n"
          ]
        }
      ],
      "source": [
        "best_weights_file = 'weights.best.hdf5'\n",
        "checkpoint = ModelCheckpoint(best_weights_file,monitor='val_loss',verbose=1,\n",
        "                             save_best_only=True,mode='min')\n",
        "\n",
        "callbacks = [checkpoint]\n",
        "\n",
        "\n",
        "train_history = model.fit([x_delta_aug2[40:],x_theta_aug2[40:],x_alpha_aug2[40:],\n",
        "                           x_beta_aug2[40:],x_gamma_aug2[40:]],y_train_aug2[40:],\n",
        "                          batch_size=32,epochs=500,callbacks=callbacks,\n",
        "                          validation_data=validation_data,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = train_history.epoch[-1]+1\n",
        "history = train_history.history"
      ],
      "metadata": {
        "id": "UeDq8adZ2KBg"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
        "\n",
        "axes[0].plot(range(1, num_epochs+1), history['loss'])\n",
        "axes[0].plot(range(1, num_epochs+1), history['val_loss'])\n",
        "axes[0].legend(['Train loss curve', 'Validation loss curve'])\n",
        "axes[0].set_xlabel('epoch')\n",
        "axes[0].set_ylabel('loss')\n",
        "\n",
        "\n",
        "axes[1].plot(range(1, num_epochs+1), history['acc'])\n",
        "axes[1].plot(range(1, num_epochs+1), history['val_acc'])\n",
        "axes[1].legend(['Train accuracy curve', 'Validation accuracy curve'])\n",
        "axes[1].set_xlabel('epoch')\n",
        "axes[1].set_ylabel('accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "leqtoAtf28yj",
        "outputId": "389be29f-23cd-46a2-b24c-038c47642906"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping\n",
        "early_stop_epoch = np.argmax(history['val_acc'])\n",
        "print('The early stop epoch is: ', early_stop_epoch)\n",
        "print('Train acc (Early stopping): %.2f' % history['acc'][early_stop_epoch])\n",
        "print('Validation acc (Early stopping): %.2f' % history['val_acc'][early_stop_epoch])"
      ],
      "metadata": {
        "id": "d3W6JmXP3P-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13fa1099-fc9e-46cd-f0d3-799c26c78fb5"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The early stop epoch is:  101\n",
            "Train acc (Early stopping): 0.42\n",
            "Validation acc (Early stopping): 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SexDioE2XZZH"
      },
      "execution_count": 194,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hBIzO3XHML7S"
      ],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}