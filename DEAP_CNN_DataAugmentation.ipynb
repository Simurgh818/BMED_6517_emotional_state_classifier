{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simurgh818/BMED_6517_emotional_state_classifier/blob/main/DEAP_CNN_DataAugmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1: Clone repo and get dataset"
      ],
      "metadata": {
        "id": "hBIzO3XHML7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKKDN0tIXKTt",
        "outputId": "b88fbf0c-a4e8-41bb-eeae-29c383a55636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-04 16:29:47--  https://drive.google.com/drive/folders/1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.24.102, 74.125.24.100, 74.125.24.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.24.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing’\n",
            "\n",
            "1_9n-kRKkpnCC2wVovO     [  <=>               ] 221.04K   761KB/s    in 0.3s    \n",
            "\n",
            "2022-12-04 16:29:47 (761 KB/s) - ‘1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing’ saved [226340]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Importing the preprocessed dataset files\n",
        "!wget https://drive.google.com/drive/folders/1_9n-kRKkpnCC2wVovOJsIXOyY3BeW1TB?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTiKd_g15Kl0",
        "outputId": "d693f6ab-3b8c-4a6f-e674-4743e78c9d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "     0K .......... .......... .......... .......... .......... 11.2M\n",
            "    50K .......... .......... .......... .......... .......... 8.72M\n",
            "   100K .......... .......... .......... .......... .......... 22.5M\n",
            "   150K .......... .......... ..                               11.7M=0.01sCloning into 'BMED_6517_emotional_state_classifier'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 139 (delta 6), reused 12 (delta 1), pack-reused 114\u001b[K\n",
            "Receiving objects: 100% (139/139), 78.37 MiB | 16.39 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ],
      "source": [
        "#importing our git repo\n",
        "import os\n",
        "if not os.path.exists('/content/BMED_6517_emotional_state_classifier'):\n",
        "  !wget https://github.com/Simurgh818/BMED_6517_emotional_state_classifier/blob/main/requirements.txt -q --show-progress --progress=dot\n",
        "  !git clone https://github.com/Simurgh818/BMED_6517_emotional_state_classifier.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPTiWGmT7qwu",
        "outputId": "49cb5d3b-bb87-46ba-9a96-dbf72baa2838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(880, 5, 32, 32)\n",
            "(880, 5, 512)\n",
            "(880, 32, 6)\n",
            "(880,)\n",
            "(880,)\n",
            "(880,)\n"
          ]
        }
      ],
      "source": [
        "# import data from numpy arrays\n",
        "import numpy as np\n",
        "\n",
        "loaded_features = np.load('/content/BMED_6517_emotional_state_classifier/results/npy/EEG_features.npy', allow_pickle=True)\n",
        "\n",
        "connectivityMatrix = loaded_features.item().get('connectivity_matrix')\n",
        "connectivityLinear = loaded_features.item().get('connectivity_linear')\n",
        "wavelet = loaded_features.item().get('waveletEntropy')\n",
        "Valence = loaded_features.item().get('Valence')\n",
        "Arousal = loaded_features.item().get('Arousal')\n",
        "Classes = loaded_features.item().get('Classes')\n",
        "\n",
        "print(connectivityMatrix.shape)\n",
        "print(connectivityLinear.shape)\n",
        "print(wavelet.shape)\n",
        "print(Valence.shape)\n",
        "print(Arousal.shape)\n",
        "print(Classes.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0N8fbUuUbqYp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "# Importing Deep Learning Libraries\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense,Input,Dropout,GlobalAveragePooling2D,Flatten,Conv2D,BatchNormalization,Activation,MaxPooling2D\n",
        "from keras.models import Model,Sequential\n",
        "from keras.optimizers import Adam,SGD,RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8YfwL18POR_d"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "l2teHjzaOeHr"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "#dataset_labels = np.load('content/gdrive/MyDrive/Colab Notebooks/Copy of labels_1_22.npy', mmap_mode='r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ofakMc12O3EG"
      },
      "outputs": [],
      "source": [
        "#dataset_name1 = 'Copy of bipolar_feats.npy'\n",
        "\n",
        "#dataset_bipolarfts = np.load(dataset_name1, encoding='bytes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YYNle4ynO_uM"
      },
      "outputs": [],
      "source": [
        "#dataset_name2 = 'Copy of labels_1_22.npy'\n",
        "\n",
        "#dataset_labels = np.load(dataset_name2, encoding='bytes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q3o_RfZLPRoI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.datasets import load_iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phTMsRTyi6T8",
        "outputId": "8789b845-17bd-4602-9943-ed22bce724ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(880,)\n",
            "[0. 1. 2. 3.]\n"
          ]
        }
      ],
      "source": [
        "print(np.shape(Classes))\n",
        "print(np.unique(Classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2: class balancing"
      ],
      "metadata": {
        "id": "O78d6tyWMFE6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bStEfQ5h4t1",
        "outputId": "85aa9ac7-3175-41dd-9fae-0b0ab8e4db42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[172]\n"
          ]
        }
      ],
      "source": [
        "# Class balance check:\n",
        "labels = ['Classes']\n",
        "zeros = [Classes[Classes == 0].shape[0]]\n",
        "ones = [Classes[Classes == 1].shape[0]]\n",
        "twos = [Classes[Classes == 2].shape[0]]\n",
        "threes = [Classes[Classes == 3].shape[0]]\n",
        "\n",
        "x = np.arange(1)  # the label locations\n",
        "width = 0.25  # the width of the bars\n",
        "\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "eq2q6c7NniMF",
        "outputId": "13a72625-96b8-4aeb-b337-acdfb870d7ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, ax = plt.subplots()\n",
        "rects1 = ax.bar(x - 1.5*width, zeros, width, label='zeros')\n",
        "rects2 = ax.bar(x - width/2, ones, width, label='ones')\n",
        "rects3 = ax.bar(x + width/2, twos, width, label='twos')\n",
        "rects4 = ax.bar(x + 1.5*width, threes, width, label='threes')\n",
        "\n",
        "ax.set_title('The numbers of each class')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACiGDaBJh1YO",
        "outputId": "9a2fd690-7cb0-4433-84a2-a571beb56a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(688,) \n",
            " 128*4 is:  512\n",
            "(688, 5, 32, 32)\n"
          ]
        }
      ],
      "source": [
        "# Rebalance classes based on class 1 = 172: Take the first 172 from class 0,2 and 3.\n",
        "zero_mask = (Classes == 0)\n",
        "one_mask = (Classes == 1)\n",
        "two_mask = (Classes == 2)\n",
        "three_mask = (Classes == 3)\n",
        "size = 172\n",
        "\n",
        "Classes_bl = np.concatenate([Classes[zero_mask][:size],Classes[one_mask][:size],\n",
        "                            Classes[two_mask][:size],Classes[three_mask][:size]])\n",
        "print(np.shape(Classes_bl),'\\n 128*4 is: ', 128*4)\n",
        "\n",
        "connectivityMatrix_bl = np.concatenate([connectivityMatrix[zero_mask][:size],connectivityMatrix[one_mask][:size],\n",
        "                            connectivityMatrix[two_mask][:size],connectivityMatrix[three_mask][:size]])\n",
        "print(np.shape(connectivityMatrix_bl))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Classes_bl[Classes_bl == 0].shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UntIH3oLMy1",
        "outputId": "0470ded9-9d7d-4db0-a54c-2845d248d59c"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVukc0CcuTdR",
        "outputId": "7d47fe67-c378-4a22-b7ca-c7a94ec68189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valence shape is:  (688,)\n",
            "Arousal shape is:  (688,)\n"
          ]
        }
      ],
      "source": [
        "# balancing valence and arrousal\n",
        "Valence_bl = np.concatenate([Valence[zero_mask][:size],Valence[one_mask][:size],\n",
        "                            Valence[two_mask][:size],Valence[three_mask][:size]])\n",
        "print('Valence shape is: ', np.shape(Valence_bl))\n",
        "\n",
        "Arousal_bl = np.concatenate([Arousal[zero_mask][:size],Arousal[one_mask][:size],\n",
        "                            Arousal[two_mask][:size],Arousal[three_mask][:size]])\n",
        "print('Arousal shape is: ', np.shape(Arousal_bl))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zeros_bl = [Classes_bl[Classes_bl == 0].shape[0]]\n",
        "ones_bl = [Classes_bl[Classes_bl == 1].shape[0]]\n",
        "twos_bl = [Classes_bl[Classes_bl == 2].shape[0]]\n",
        "threes_bl = [Classes_bl[Classes_bl == 3].shape[0]]\n",
        "x = np.arange(1)  # the label locations\n",
        "fig2, ax2 = plt.subplots()\n",
        "rects1 = ax2.bar(x - 1.5*width, zeros_bl, width, label='zeros_bl')\n",
        "rects2 = ax2.bar(x - width/2, ones_bl, width, label='ones_bl')\n",
        "rects3 = ax2.bar(x + width/2, twos_bl, width, label='twos_bl')\n",
        "rects4 = ax2.bar(x + 1.5*width, threes_bl, width, label='threes_bl')\n",
        "\n",
        "ax2.set_title('The numbers of each class')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(labels)\n",
        "ax2.legend()\n",
        "\n",
        "fig2.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "dfzl5-1-J_Uf",
        "outputId": "789bd049-8a46-428e-f10c-5cef5901ba99"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXgV5dnH8e/Nvq9GZBEjFEFFCBBjcauIFqhaiq8igvsCWvelvmIRreK+L7iAUrClSARBWy1I9aVUBJEgUNBaAUHCvhoW0RDu948zwUPIRs45yST5fa7rXDnnmZln7jm0+TnPTOYxd0dERCRsqpR1ASIiIvlRQImISCgpoEREJJQUUCIiEkoKKBERCSUFlIiIhJICShLOzO43sz+XdR2HyszGmtmIsq4jLzPrZ2arzWynmXUppX3G5bsI63cq4VStrAuQ8s/MdkZ9rAP8AOQEn4eUfkUV3pPAje7+TlkXIpJIOoOSmLl7vdwX8C1wXlTb+LKuLwzMrGocuzsKWBrH/kRCSQElpaWGmb1hZjvMbKmZpeYuMLMWZjbZzDaZ2TdmdnNBnQRDRCPN7L2gr0/NrG2wLNnM3MyqRa0/08yuCd5fYWazzewZM9tuZivM7OSgfbWZbTSzy/Ps8jAzmxHs659mdlRU3x2CZVvN7Csz65+nzpfN7H0z2wX0MLNfmdkXQV9rzOzOAo6xipkNM7NVQU1vmFlDM6sZnK1WBRaZ2fICti+srnPM7HMzywqO+f48255qZp8E389qM7sianHj/L73AmoorJ/cdRqb2d+Cf/dtwftWUcuvCP6NdgT/uxgUtP8s+Lf4zsw2m9nEguqQcs7d9dIrbi9gJXBWnrb7gT3Ar4j8cn0EmBssqwJkAMOBGkAbYAXQq4D+xwJbgDQiQ9TjgTeDZcmAA9Wi1p8JXBO8vwLYC1wZ1DGCyBnfSKAm8EtgB1Aval87gNOD5c8BHwfL6gKrg76qAV2AzcBxUdt+B5wSHGMtYB1wWrC8MdC1gGO8ClgWfBf1gLeBP0Utd+BnBWxbVF1nACcENXUCNgC/CZYdFRzvxUB1oCmQUtT3nk8NRfUzInjfFPgfIsPC9YG3gKlRx5EFtA8+NweOD95PAH4f9b2eWtb/u9crMS+dQUlp+djd33f3HOBPQOeg/UQgyd0fcPcf3X0FMBoYUEhfU9x9nrvvJfKLMuUQ6vjG3f8Y1DEROBJ4wN1/cPcPgB+Bn0Wt/567z3L3H4j8UuxuZkcC5wIrg772uvvnwGTgwqht33H32e6+z933ANnAcWbWwN23ufuCAmocBDzt7ivcfScwFBgQfWZYiELrcveZ7v7voKbFRH7Z/yLYdiDwD3ef4O7Z7r7F3RdG9V3c772ofghq2eLuk919t7vvAB6KqgVgH9DRzGq7+zp3zx3WzCYSgi3cfY+7f1yM70XKIQWUlJb1Ue93A7WCX7hHAS2CoaDtZrYduAdodgh91TuEOjZEvf8ewN3ztkX3tzr3TRAWW4EWQd0n5al7EHBEftsG/ofIWeSqYIiqewE1tgBWRX1eReSspbDvJFehdZnZSWb2f8Gw2nfAdcBhwbZHAvkOGwaK+70X1Q9BLXXM7NVgKDMLmAU0MrOq7r4LuCiob10wtNgh2PQuwIB5FhkuvqqofUn5pLv4pKytJnJW0y4Ofe0KftYhMjwEBwZGSRyZ+8bM6gFNgLVE6v6nu59dyLYHTBXg7p8Bfc2sOnAjkB7df5S1RIImV2siQ5Mb8lk3r6Lq+gvwItDH3feY2bP8FFCriQzhxaq4/dwBtAdOcvf1ZpYCfE4kfHD36cB0M6tNZDh2NJEh0vXAtRC51gX8w8xmufuyONQuIaIzKClr84AdZva/ZlbbzKqaWUczO/FQO3L3TcAa4JKgn6uAAi/kF9Ovggv+NYAHiVw7Ww38DTjGzC41s+rB60QzOza/TsyshpkNMrOG7p5NJED3FbDPCcBtZnZ0EIoPAxODobWiFFVXfWBrEE5pRIbjco0HzjKz/mZWzcyaBqFxqIrbT30iZ6zbzawJcF/uAjNrZmZ9zawukT9b2EnwfZnZhVE3U2wj8h8CBX2XUo4poKRMBdeCziVyPeMbIhf0XwMalrDLa4HfEbmgfzzwSYwl/oXIL86tQDfgEoDgmskviVwrW0tk+OsxIjdTFORSYGUwnHUdkaG3/Iwhcp1uFpHvZA9wU3GKLUZdvwUeMLMdRG5MSY/a9lsiQ5B3BMe7kJ+uFRbbIfTzLFCbyL/5XGBa1LIqwO3BMWwlcm3q+mDZicCnwR2N7wK3BNcupYIxd01YKCIi4aMzKBERCSUFlIiIhJICSkREQkkBJSIioRSKv4M67LDDPDk5uazLEBGRMpCRkbHZ3ZPytocioJKTk5k/f35ZlyEiImXAzFbl164hPhERCSUFlIiIhFKRAWVmYywyJ82SqLaJZrYweK00s4VBe7KZfR+17JVEFi8iIhVXca5BjSXycMk3chvc/aLc92b2FJF5b3Itd/eSPL9LRCS0srOzyczMZM+ePWVdSrlVq1YtWrVqRfXq1Yu1fpEB5e6zzCw5v2VmZkB/4MxDqFFEpNzJzMykfv36JCcnE/nVJ4fC3dmyZQuZmZkcffTRxdom1mtQpwEb3P3rqLajLTKl9D/N7LQY+xcRCYU9e/bQtGlThVMJmRlNmzY9pDPQWG8zv5jI1AC51gGt3X2LmXUDpprZ8e6elXdDMxsMDAZo3bp1jGWIiCSewik2h/r9lfgMKpgN9Xwi02YDEEybvSV4n0FkVs1j8tve3Ue5e6q7pyYlHfT3WSIiUsnFcgZ1FvAfd8/MbTCzJCKToeWYWRugHaB5WkSkwkm++7249rfy0XPi2l9FUJzbzCcAc4D2ZpZpZlcHiwZw4PAewOnA4uC280nAde6+NZ4Fi4hI6TrjjDPyfdrP2LFjufHGGxO23+LcxXdxAe1X5NM2GZgce1klE+//opGfrKw1sOiVpEROOFrXYBMp/ZG9cekne+SLfJ+TE5e+8vP9kiVFr1QCOTk5VK1aNaY+9u3axZ7ly/m+Vq04VVU8oXgWn4iIFG50ejqvpacDkLVzJ0e1aMGd11zDiJEj+SE7mzatWvHqiBHUq1OHDr16cUGvXnw4dy63X3kl7s4To0fjQO/TTmPE7beTk5PD9cOHs+CLLzDgsn79uOmyywrc/4S//pXf3ncfOTk5vPzAA5x4wgkJP2YFlIhIOXBt//5c278/2dnZ9LnmGi7r14/HXn2V90aPpm6dOjz1+us8P24c91x/PQBNGjViTno6azdu5IxBg5g9cSKNGzTgvCFDePfDD2l1xBGs3biR+VOmALA966CbrQ+we88ePp00iY/nz+f64cP3b5dICigRkXLkzsce4xdpaTRq0ID/rFjBmcFZT3Z2NmmdO+9f74LevQHIWLKE0088kaQmTQC46JxzmJ2Rwd1DhvBNZia3P/wwvU8/nbNOPrnQ/fbv0weAU1NTydq5s8hAiwcFlIhIOfGnqVP5du1anrnnHv4+axZndu/OuMcfz3fdOrVrF9pX44YN+XTyZP4xezavpaczefp0Xn3wwQLXz/s3TKXxN2EKKBGREvjykqNKdX8Lli7luXHjmDF2LFWqVCGtUydue+ghln/7LW1bt2bX7t2s3biRdnkmf0094QTufPRRNm/bRuMGDXjr/fe5buBANm/bRo3q1fnN2WfTLjmZq4YOLXT/k6ZN4xdpaXyyYAEN69WjYf36CTzaCAWUiEg58MqECWz97jt6Xx35S5+uxx/PqBEjuPyuu/jxxx8BGH7TTQcFVPOkJB689Vb6XHXV/pskzjvzTBZ/9RVD7r2Xffv2AfDALbcUuv9aNWvy8wsvZO/evbz8wANxP778mLuXyo4Kk5qa6vGYUVe3mSeObjNPHN1mnljxvM28XbNmcemroqjdseMhb/Pll19y7LHHHtBmZhnunpp3XU1YKCIioaQhPhERAeDWESOYs3DhAW03DBrEZf36lUk9CigREQHg2WHDyrqEA2iIT0REQkkBJSIioaSAEhGRUNI1KBGREqg96ZS49vf9BbPj2l9FoDMoERE5yJ+mTuW2hx7Kd1lSWlqp1KCAEhGRUNIQn4hIOfH8uHG8MXUqAFecfz7nnXkmv7n+erp37cqnCxfS4vDDSX/+eWrXqsWK1au59aGH2Lx1K3Vq12bkfffRvk0b3p4+nYdfeYWqVarQoF49ZowbV+D+Mtevp9eVV7J240YGnHsuvw+m8igtOoMSESkHFixdyp+mTuWf48czc/x4/jh5Mtuyslj27bcMGTCAjKlTaVi/PlNnzADgxj/8gaeHDuWT9HQevuMObg2G6x555RXeeeUVPp08mbdeeKHQfc5fsoS/PPMM8yZPZsoHH5CxdGnCjzOazqBERMqBOZ9/znk9e1K3Th0Aft2zJ58sWEByy5Z07tABgC7HHceqtWvZuXs3cxcuZNAdd+zfPveBsj/v0oUhw4Zxfq9e9D3rrEL32bN7d5o2arR/f3MWLKDb8ccn4vDypYASESnHataosf991apV+f6HH9i3bx8N69fn00mTDlr/heHDmbd4MdNmzeKUiy5i9sSJ+0Mor7KYAyqaAkpEpARK+7bwk7t2ZciwYdx59dW4O3/96CNee/hhxuQTQg3q1SO5ZUvenj6d83v1wt3593//S6f27VmxejVpnTqR1qkTH3z8MZnr1xcYUB/OmcPW776jds2a/O2jj3i5kAkNE0EBJSJSDnQ57jgu6duX0wdGpr654vzzadygQYHr//HRR7l5xAgeGzWK7L17uaB3bzq1b889Tz3F8lWrcOCMk06iU/v2BfaR2rEjA2+7jTUbNjDg3HNLdXgPNB+UFJPmg0oczQeVWJoPKnE0H5SIiFRKRQ7xmdkY4Fxgo7t3DNruB64FNgWr3ePu7wfLhgJXAznAze4+PQF1i4hIHMyYPZthzzxzQFtyy5ZMfO65MqroJ8W5BjUWeBF4I0/7M+7+ZHSDmR0HDACOB1oA/zCzY9w9Jw61iohInJ19yimcfUp8nysYL0UO8bn7LGBrMfvrC7zp7j+4+zfAMqB0HtokIiIVSizXoG40s8VmNsbMGgdtLYHVUetkBm0HMbPBZjbfzOZv2rQpv1VERKQSK2lAvQy0BVKAdcBTh9qBu49y91R3T01KSiphGSIiUlGV6O+g3H1D7nszGw38Lfi4BjgyatVWQZuISIWSlnFxXPub121CXPurCEp0BmVmzaM+9gOWBO/fBQaYWU0zOxpoB8yLrUQREdmelcWrb75Zavvr0KsXm7dtO6h9xEsv8ezYsaVSQ5EBZWYTgDlAezPLNLOrgcfN7N9mthjoAdwG4O5LgXTgC2AacIPu4BMRid13O3YweuLEsi6jVBU5xOfu+Z3Hvl7I+g8B+U/DKCIiJXLvs8+yYvVqTrrgAjp36MCve/bk3B49uOiWW2jUoAGvPvgg46ZMYcXq1fzh5psPmjvqxksvZdfu3Vxy552s3bCBnH37uHvIEC7o3bvAfT7zxz/ywb/+Ra1atRj72GO0bV26Tz3Rs/hERMqBB2+9lS+WLePTSZN46+9/55MFCzi3Rw/WbtzI+s2bAZidkcGFffocMHeUA78YOJBTU1NZmZlJ88MPZ8pLLwGRs7LCNKhXj8+mTGH8u+/yu8ce4+2RIxN9mAfQo45ERMqZk7t2ZfaCBXy5fDnHtmnD4U2asG7TJuYtWsTPU1IOmDuqXp06++eOOr5dOz6aM4dhTz/N7IwMGtavX+h++vfps//nvEWLSuPQDqCAEhEpZ1o2a8Z3WVnM+PhjTklN5ZRu3Xh7+nTq1qlD/bp1C9yuXXIyn6Snc3y7dvzhhRd4+OWXC91P9PxPpT0XFGiIT0SkREr7tvB6deuyY9eu/Z/TOnXixT//mb+//jpbtm9n0O230+/ss4GC545au3EjTRo25OLzzqNhgwaMnTy50H1OmjaNO6+5hknTppHWuXNCjy8/CigRkXKgaaNGdE9JIbVfP3556qmc3K0b/5gzh7atW9O6eXO2ZWVxcrduQP5zR6UceywzZs/m9089hVWpQvVq1Xju3nsL3ef2rCzSzj+fGjVqMO7xxxN+jHlpPigpFs0HlTiaDyqxNB9U4mg+KBERqZQ0xCciUolddMstrFxz4BPpRtx2Wyim4FBAiYhUYmGYmLAgGuITEZFQUkCJiEgoKaBERCSUdA1KRKQEVl5wYVz7S570Vlz7qwh0BiUiUg5Ezwc167PPOP+GG8q0nhEvvcSTTz55UPvKlSvpWIK/j8qPAkpEpBwoyXxQOTnlezo+DfGJiJQD0fNBVa9Wjbq1azPw9tv54uuv6XLccYx59FHMjA69enFBr158OHcut195JY0bNmTEyJH8kJ1Nm1ateHXECOrVqcOCpUu5+4kn2Ll7N4c1bsyrI0bQPCmJl8aP57X0dKpVrUqHtm1544knCqxp0aJFdO/enc2bN3PXXXdx7bXXxvWYFVAiIuVA9HxQsz77jP4338z8KVNocfjhnHnppcz5/HNO7toVgCaNGjEnPZ3N27Zx8a238t7o0dStU4enXn+d58eN43fXXMMdjzxC+vPPk9SkCZOmTeP+55/n1Qcf5MnXX+fLadOoWaMG27OyCq1p8eLFzJ07l127dtGlSxfOOeecuB6zAkpEpBxK7diRVkccAUCnDh1YtWbN/oDKnSV33uLF/GfFCs687DIAsrOzSevcmf+uXMkXy5Zx7uDBAOzLyeGIpCQAOh5zDFfefTfn9ejBeT17FlpD3759qV27NrVr16ZHjx7MmzePlJSUuB2jAkpEpByqUaPG/vdVq1Rhb9T1pjq1awPg7pzZvftBTyJf8t//cmzbtswcP/6gfqeMHMnHGRm8P3Mmj48ezWdvv021avlHRd45ouI9Z5QCSkSkBEr7tvC880EVR1qnTtz20EMs//Zb2rZuza7du1m7cSPHHH00m7dt49OFCzkpJYXs7Gy+XrWKDm3akLl+Pb9IS+PkLl14a9o0du7eTaMGDfLt/5133mHo0KHs2rWLmTNn8uijj/Ljjz/G43ABBZSISLkQPR9UrZo1Obxp0yK3SWrShFEjRnD5XXftD47hN91Eu+Rkxj/9NHc+8ghZO3eyNyeHGy65hHZHHcVVQ4eStWMHDvx24MACwwmgU6dO9OjRg82bN3PvvffSokULVq5cGacj1nxQUkyaDypxNB9UYmk+qMTRfFAiIlIpFTnEZ2ZjgHOBje7eMWh7AjgP+BFYDlzp7tvNLBn4Evgq2Hyuu1+XgLpFRKQUvDFlCiPz3EzRPSWFZ4cNS/i+i3MNaizwIvBGVNsMYKi77zWzx4ChwP8Gy5a7e/zuMxQRCYN9+3D3uN+pFnaX9evHZf36xaWvQ72kVOQQn7vPArbmafvA3XMHducCrQ5pryIi5YytXs327OxD/iUrEe7Oli1bqFWrVrG3icddfFcB0Q+IOtrMPgeygGHu/q/8NjKzwcBggNatdZFYRMKt6iuvsuW6IWw+8kioosv3ANWrVj2k9WvVqkWrVsU/n4kpoMzs98BeIHeAch3Q2t23mFk3YKqZHe/uBz0vw91HAaMgchdfLHWIiCSaZWVR7fGCn0tXGR37ny8T2n+J/zPAzK4gcvPEIA/Oed39B3ffErzPIHIDxTFxqFNERCqZEgWUmfUG7gJ+7e67o9qTzKxq8L4N0A5YEY9CRUSkcinObeYTgDOAw8wsE7iPyF17NYEZwR0tubeTnw48YGbZwD7gOnffmm/HIiIihSgyoNz94nyaXy9g3cnA5FiLEhER0a0oIiISSgooEREJJQWUiIiEkgJKRERCSQElIiKhpIASEZFQUkCJiEgoKaBERCSUFFAiIhJKCigREQklBZSIiISSAkpEREJJASUiIqGkgBIRkVBSQImISCgpoEREJJQUUCIiEkoKKBERCSUFlIiIhJICSkREQkkBJSIioaSAEhGRUFJAiYhIKBUroMxsjJltNLMlUW1NzGyGmX0d/GwctJuZPW9my8xssZl1TVTxIiJScRX3DGos0DtP293Ah+7eDvgw+AzQB2gXvAYDL8depoiIVDbFCih3nwVszdPcFxgXvB8H/Caq/Q2PmAs0MrPm8ShWREQqj1iuQTVz93XB+/VAs+B9S2B11HqZQZuIiEixxeUmCXd3wA9lGzMbbGbzzWz+pk2b4lGGiIhUILEE1Ibcobvg58agfQ1wZNR6rYK2A7j7KHdPdffUpKSkGMoQEZGKKJaAehe4PHh/OfBOVPtlwd18Pwe+ixoKFBERKZZqxVnJzCYAZwCHmVkmcB/wKJBuZlcDq4D+wervA78ClgG7gSvjXLOIiFQCxQood7+4gEU981nXgRtiKUpERERPkhARkVBSQImISCgpoEREJJQUUCIiEkoKKBERCSUFlIiIhJICSkREQkkBJSIioaSAEhGRUFJAiYhIKCmgREQklBRQIiISSgooEREJJQWUiIiEkgJKRERCSQElIiKhpIASEZFQUkCJiEgoKaBERCSUFFAiIhJKCigREQklBZSIiISSAkpEREJJASUiIqFUraQbmll7YGJUUxtgONAIuBbYFLTf4+7vl7hCERGplEocUO7+FZACYGZVgTXAFOBK4Bl3fzIuFYqISKUUryG+nsByd18Vp/5ERKSSi1dADQAmRH2+0cwWm9kYM2uc3wZmNtjM5pvZ/E2bNuW3ioiIVGIxB5SZ1QB+DbwVNL0MtCUy/LcOeCq/7dx9lLununtqUlJSrGWIiEgFE48zqD7AAnffAODuG9w9x933AaOBtDjsQ0REKpl4BNTFRA3vmVnzqGX9gCVx2IeIiFQyJb6LD8DM6gJnA0Oimh83sxTAgZV5lomIiBRLTAHl7ruApnnaLo2pIhEREfQkCRERCSkFlIiIhJICSkREQkkBJSIioaSAEhGRUFJAiYhIKCmgREQklBRQIiISSgooEREJJQWUiIiEkgJKRERCSQElIiKhpIASEZFQUkCJiEgoKaBERCSUFFAiIhJKCigREQklBZSIiISSAkpEREJJASUiIqGkgBIRkVBSQImISCgpoEREJJSqxdqBma0EdgA5wF53TzWzJsBEIBlYCfR3922x7ktERCqPeJ1B9XD3FHdPDT7fDXzo7u2AD4PPIiIixZaoIb6+wLjg/TjgNwnaj4iIVFDxCCgHPjCzDDMbHLQ1c/d1wfv1QLO8G5nZYDObb2bzN23aFIcyRESkIon5GhRwqruvMbPDgRlm9p/ohe7uZuZ5N3L3UcAogNTU1IOWi4hI5RbzGZS7rwl+bgSmAGnABjNrDhD83BjrfkREpHKJKaDMrK6Z1c99D/wSWAK8C1werHY58E4s+xERkcon1iG+ZsAUM8vt6y/uPs3MPgPSzexqYBXQP8b9iIhIJRNTQLn7CqBzPu1bgJ6x9C0iIpWbniQhIiKhpIASEZFQUkCJiEgoKaBERCSUFFAiIhJKCigREQklBZSIiISSAkpEREJJASUiIqGkgBIRkVBSQImISCgpoEREJJQUUCIiEkoKKBERCSUFlIiIhJICSkREQkkBJSIioaSAEhGRUFJAiYhIKCmgREQklBRQIiISSgooEREJJQWUiIiEUokDysyONLP/M7MvzGypmd0StN9vZmvMbGHw+lX8yhURkcqiWgzb7gXucPcFZlYfyDCzGcGyZ9z9ydjLExGRyqrEAeXu64B1wfsdZvYl0DJehYmISOUWl2tQZpYMdAE+DZpuNLPFZjbGzBoXsM1gM5tvZvM3bdoUjzJERKQCiTmgzKweMBm41d2zgJeBtkAKkTOsp/Lbzt1HuXuqu6cmJSXFWoaIiFQwMQWUmVUnEk7j3f1tAHff4O457r4PGA2kxV6miIhUNrHcxWfA68CX7v50VHvzqNX6AUtKXp6IiFRWsdzFdwpwKfBvM1sYtN0DXGxmKYADK4EhMVUoIiKVUix38X0MWD6L3i95OSIiIhF6koSIiISSAkpEREJJASUiIqGkgBIRkVBSQImISCgpoEREJJQUUCIiEkoKKBERCSUFlIiIhJICSkREQkkBJSIioaSAEhGRUFJAiYhIKCmgREQklBRQIiISSgooEREJJQWUiIiEkgJKRERCSQElIiKhpIASEZFQUkCJiEgoKaBERCSUFFAiIhJKCigREQmlhAWUmfU2s6/MbJmZ3Z2o/YiISMWUkIAys6rASKAPcBxwsZkdl4h9iYhIxZSoM6g0YJm7r3D3H4E3gb4J2peIiFRA1RLUb0tgddTnTOCk6BXMbDAwOPi408y+SlAtEgdW1gVUaEsOAzaXdRUVlYZuEsji9pvhqPwaExVQRXL3UcCostq/SFiY2Xx3Ty3rOkTCJlFDfGuAI6M+twraREREiiVRAfUZ0M7MjjazGsAA4N0E7UtERCqghAzxufteM7sRmA5UBca4+9JE7EukAtBQt0g+zN3LugYREZGD6EkSIiISSgooEREJJQWUSAzM7Agze9PMlptZhpm9b2bHmNmSsq5NpLwrs7+DEinvzMyAKcA4dx8QtHUGmpVpYSIVhM6gREquB5Dt7q/kNrj7IqKeomJmyWb2LzNbELxODtqbm9ksM1toZkvM7DQzq2pmY4PP/zaz24J125rZtOAM7V9m1iFovzBYd5GZzSrdQxdJPJ1BiZRcRyCjiHU2Ame7+x4zawdMAFKBgcB0d38oeLhyHSAFaOnuHQHMrFHQxyjgOnf/2sxOAl4CzgSGA73cfU3UuiIVhgJKJLGqAy+aWQqQAxwTtH8GjDGz6sBUd19oZiuANmb2AvAe8IGZ1QNOBt6yn557VjP4ORsYa2bpwNulczgipUdDfCIltxToVsQ6twEbgM5EzpxqALj7LOB0Io8AG2tml7n7tmC9mcB1wGtE/j+63d1Tol7HBn1cBwwj8lixDDNrGufjEylTCiiRkvsIqBk8mR8AM+vEgc+hbAisc/d9wKVEnqyCmR0FbHD30USCqKuZHQZUcffJRIKnq7tnAd+Y2YXBdhbciIGZtXX3T919OLApz35Fyj0FlEgJeeQxLP2As4LbzJcCjwDro1Z7CbjczBYBHYBdQfsZwCIz+xy4CHiOyDQ1M81sIfBnYGiw7kK9Me0AAABPSURBVCDg6qCPpfw0t9oTwc0US4BPgEWJOVKRsqFHHYmISCjpDEpEREJJASUiIqGkgBIRkVBSQImISCgpoEREJJQUUCIiEkoKKBERCaX/B42ymXsXZ/THAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3: CNN Model"
      ],
      "metadata": {
        "id": "R4t43f_NNH_G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyU4fNdmCL-l",
        "outputId": "34f9c3cf-49f6-4b7b-b70e-5843ccffbd22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 2. 3.]\n",
            "(688,)\n",
            "[0. 1.]\n",
            "(688, 4)\n",
            "[116. 164. 176. 232.]\n"
          ]
        }
      ],
      "source": [
        "# Algorithm 1 for Convolutional Neural Model :\n",
        "##Require: Training EEG Dataset nntrX, Training Valence/Arousal Values nntrY, Testing subject’s EEG\n",
        "#Dataset nnteX, Testing Valence/Arousal Values nnteY\n",
        "# cnn = model(trainX, trainY )\n",
        "\n",
        "#x = dataset_bipolarfts\n",
        "x = connectivityMatrix_bl\n",
        "#y = dataset_labels[1:881]\n",
        "y = np.vstack([Valence_bl,Arousal_bl]).T\n",
        "#x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2)\n",
        "y[np.where(y<4.5)] = 0\n",
        "y[np.where(y>=4.5)] = 1\n",
        "y_4_class = y[:,0]*2+y[:,1]\n",
        "print(np.unique(y_4_class))\n",
        "print(y_4_class.shape)\n",
        "y_one_hot = np.zeros((y.shape[0],4))\n",
        "y_one_hot[np.where(y_4_class==0),0] = 1\n",
        "y_one_hot[np.where(y_4_class==1),1] = 1\n",
        "y_one_hot[np.where(y_4_class==2),2] = 1\n",
        "y_one_hot[np.where(y_4_class==3),3] = 1\n",
        "print(np.unique(y_one_hot))\n",
        "print(y_one_hot.shape)\n",
        "print(np.sum(y_one_hot,axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dgqnDGha6IE",
        "outputId": "4eed15d6-c8b3-4cc1-b43c-dbc5c9a96906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(160, 5, 32, 32)\n",
            "(528, 5, 32, 32)\n",
            "(528, 4)\n"
          ]
        }
      ],
      "source": [
        "val_size = 4\n",
        "y_test = y_one_hot[:val_size*40,:]\n",
        "x_test = x[:val_size*40,:,:,:]\n",
        "y_train = y_one_hot[val_size*40:,:]\n",
        "x_train = x[val_size*40:,:,:,:]\n",
        "print(x_test.shape)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "1zFYJl5heF2R"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import optimizers\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Conv2D, MaxPooling2D, Activation, concatenate\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.metrics import CategoricalAccuracy,CategoricalCrossentropy,Precision,Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "gc1zs7Lfex41"
      },
      "outputs": [],
      "source": [
        "from keras.layers.activation.relu import ReLU\n",
        "def make_CNN_layers(input_mat):\n",
        "  # layer 1\n",
        "  model = Conv2D(16,(3,3),padding='same',input_shape=input_shape)(input_mat)\n",
        "  model = ReLU()(model)\n",
        "  model = MaxPooling2D((2,2),padding='same')(model)\n",
        "  model = Dropout(0.2)(model)\n",
        "\n",
        "  # layer 2\n",
        "  model = Conv2D(32,(3,3),padding='same')(model)\n",
        "  model = ReLU()(model)\n",
        "  model = MaxPooling2D((2,2),padding='same')(model)\n",
        "  model = Dropout(0.2)(model)\n",
        "\n",
        "  #layer 3\n",
        "  model = Conv2D(64,(3,3),padding='same')(model)\n",
        "  model = ReLU()(model)\n",
        "  model = MaxPooling2D((2,2),padding='same')(model)\n",
        "  model = Dropout(0.5)(model)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "djeO0LSKdSAQ"
      },
      "outputs": [],
      "source": [
        "input_shape = x_test[0,0].shape\n",
        "input_shape = (32,32,1)\n",
        "delta_input = Input(shape = input_shape)\n",
        "delta_model = make_CNN_layers(delta_input)\n",
        "\n",
        "theta_input = Input(shape = input_shape)\n",
        "theta_model = make_CNN_layers(theta_input)\n",
        "\n",
        "alpha_input = Input(shape = input_shape)\n",
        "alpha_model = make_CNN_layers(alpha_input)\n",
        "\n",
        "beta_input = Input(shape = input_shape)\n",
        "beta_model = make_CNN_layers(beta_input)\n",
        "\n",
        "gamma_input = Input(shape = input_shape)\n",
        "gamma_model = make_CNN_layers(gamma_input)\n",
        "\n",
        "conv = concatenate([delta_model,theta_model,alpha_model,beta_model,gamma_model])\n",
        "\n",
        "conv = Flatten()(conv)\n",
        "\n",
        "dense = Dense(512)(conv)\n",
        "dense = ReLU()(dense)\n",
        "dense = Dropout(0.3)(dense)\n",
        "\n",
        "output = Dense(4,activation='softmax')(dense)\n",
        "\n",
        "model = Model(inputs=[delta_input,theta_input,alpha_input,beta_input,gamma_input],\n",
        "              outputs=[output])\n",
        "\n",
        "opt = optimizers.SGD(learning_rate=1e-2, momentum=0)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['acc',Recall(),Precision()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sspGx1ekjYhR",
        "outputId": "d818a4ba-b18a-4150-d1e5-735466340230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_26 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_27 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_28 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_29 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " input_30 (InputLayer)          [(None, 32, 32, 1)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d_75 (Conv2D)             (None, 32, 32, 16)   160         ['input_26[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_78 (Conv2D)             (None, 32, 32, 16)   160         ['input_27[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_81 (Conv2D)             (None, 32, 32, 16)   160         ['input_28[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_84 (Conv2D)             (None, 32, 32, 16)   160         ['input_29[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_87 (Conv2D)             (None, 32, 32, 16)   160         ['input_30[0][0]']               \n",
            "                                                                                                  \n",
            " re_lu_80 (ReLU)                (None, 32, 32, 16)   0           ['conv2d_75[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_83 (ReLU)                (None, 32, 32, 16)   0           ['conv2d_78[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_86 (ReLU)                (None, 32, 32, 16)   0           ['conv2d_81[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_89 (ReLU)                (None, 32, 32, 16)   0           ['conv2d_84[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_92 (ReLU)                (None, 32, 32, 16)   0           ['conv2d_87[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_75 (MaxPooling2D  (None, 16, 16, 16)  0           ['re_lu_80[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_78 (MaxPooling2D  (None, 16, 16, 16)  0           ['re_lu_83[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_81 (MaxPooling2D  (None, 16, 16, 16)  0           ['re_lu_86[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_84 (MaxPooling2D  (None, 16, 16, 16)  0           ['re_lu_89[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_87 (MaxPooling2D  (None, 16, 16, 16)  0           ['re_lu_92[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_80 (Dropout)           (None, 16, 16, 16)   0           ['max_pooling2d_75[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_83 (Dropout)           (None, 16, 16, 16)   0           ['max_pooling2d_78[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_86 (Dropout)           (None, 16, 16, 16)   0           ['max_pooling2d_81[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_89 (Dropout)           (None, 16, 16, 16)   0           ['max_pooling2d_84[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_92 (Dropout)           (None, 16, 16, 16)   0           ['max_pooling2d_87[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_76 (Conv2D)             (None, 16, 16, 32)   4640        ['dropout_80[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_79 (Conv2D)             (None, 16, 16, 32)   4640        ['dropout_83[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_82 (Conv2D)             (None, 16, 16, 32)   4640        ['dropout_86[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_85 (Conv2D)             (None, 16, 16, 32)   4640        ['dropout_89[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_88 (Conv2D)             (None, 16, 16, 32)   4640        ['dropout_92[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_81 (ReLU)                (None, 16, 16, 32)   0           ['conv2d_76[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_84 (ReLU)                (None, 16, 16, 32)   0           ['conv2d_79[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_87 (ReLU)                (None, 16, 16, 32)   0           ['conv2d_82[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_90 (ReLU)                (None, 16, 16, 32)   0           ['conv2d_85[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_93 (ReLU)                (None, 16, 16, 32)   0           ['conv2d_88[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_76 (MaxPooling2D  (None, 8, 8, 32)    0           ['re_lu_81[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_79 (MaxPooling2D  (None, 8, 8, 32)    0           ['re_lu_84[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_82 (MaxPooling2D  (None, 8, 8, 32)    0           ['re_lu_87[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_85 (MaxPooling2D  (None, 8, 8, 32)    0           ['re_lu_90[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_88 (MaxPooling2D  (None, 8, 8, 32)    0           ['re_lu_93[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_81 (Dropout)           (None, 8, 8, 32)     0           ['max_pooling2d_76[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_84 (Dropout)           (None, 8, 8, 32)     0           ['max_pooling2d_79[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_87 (Dropout)           (None, 8, 8, 32)     0           ['max_pooling2d_82[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_90 (Dropout)           (None, 8, 8, 32)     0           ['max_pooling2d_85[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_93 (Dropout)           (None, 8, 8, 32)     0           ['max_pooling2d_88[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_77 (Conv2D)             (None, 8, 8, 64)     18496       ['dropout_81[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_80 (Conv2D)             (None, 8, 8, 64)     18496       ['dropout_84[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_83 (Conv2D)             (None, 8, 8, 64)     18496       ['dropout_87[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_86 (Conv2D)             (None, 8, 8, 64)     18496       ['dropout_90[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_89 (Conv2D)             (None, 8, 8, 64)     18496       ['dropout_93[0][0]']             \n",
            "                                                                                                  \n",
            " re_lu_82 (ReLU)                (None, 8, 8, 64)     0           ['conv2d_77[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_85 (ReLU)                (None, 8, 8, 64)     0           ['conv2d_80[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_88 (ReLU)                (None, 8, 8, 64)     0           ['conv2d_83[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_91 (ReLU)                (None, 8, 8, 64)     0           ['conv2d_86[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_94 (ReLU)                (None, 8, 8, 64)     0           ['conv2d_89[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d_77 (MaxPooling2D  (None, 4, 4, 64)    0           ['re_lu_82[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_80 (MaxPooling2D  (None, 4, 4, 64)    0           ['re_lu_85[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_83 (MaxPooling2D  (None, 4, 4, 64)    0           ['re_lu_88[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_86 (MaxPooling2D  (None, 4, 4, 64)    0           ['re_lu_91[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " max_pooling2d_89 (MaxPooling2D  (None, 4, 4, 64)    0           ['re_lu_94[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dropout_82 (Dropout)           (None, 4, 4, 64)     0           ['max_pooling2d_77[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_85 (Dropout)           (None, 4, 4, 64)     0           ['max_pooling2d_80[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_88 (Dropout)           (None, 4, 4, 64)     0           ['max_pooling2d_83[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_91 (Dropout)           (None, 4, 4, 64)     0           ['max_pooling2d_86[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_94 (Dropout)           (None, 4, 4, 64)     0           ['max_pooling2d_89[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 4, 4, 320)    0           ['dropout_82[0][0]',             \n",
            "                                                                  'dropout_85[0][0]',             \n",
            "                                                                  'dropout_88[0][0]',             \n",
            "                                                                  'dropout_91[0][0]',             \n",
            "                                                                  'dropout_94[0][0]']             \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 5120)         0           ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 512)          2621952     ['flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " re_lu_95 (ReLU)                (None, 512)          0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " dropout_95 (Dropout)           (None, 512)          0           ['re_lu_95[0][0]']               \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 4)            2052        ['dropout_95[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,740,484\n",
            "Trainable params: 2,740,484\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_delta = x_train[:,0,:,:]\n",
        "x_theta = x_train[:,1,:,:]\n",
        "x_alpha = x_train[:,2,:,:]\n",
        "x_beta = x_train[:,3,:,:]\n",
        "x_gamma = x_train[:,4,:,:]\n",
        "validation_data=([x_delta[:40],x_theta[:40],x_alpha[:40],x_beta[:40],x_gamma[:40]],y_train[:40])"
      ],
      "metadata": {
        "id": "n0lyC-YhphlO"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3.1: Data Augmentation"
      ],
      "metadata": {
        "id": "hKQRxNcZMXBa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "gfXgpkvKs812"
      },
      "outputs": [],
      "source": [
        "# Data Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation: cropping\n",
        "x_delta = x_train[:,0,:,:]\n",
        "print(np.shape(x_delta))\n",
        "rnd_idx = int(np.round(np.random.rand(1)*100)[0])\n",
        "print(rnd_idx)\n",
        "x_delta_clip = x_train[rnd_idx:rnd_idx+100,0,:,:]\n",
        "print(np.shape(x_delta_clip))\n",
        "x_delta_aug = np.append(x_delta,x_delta_clip, axis=0)\n",
        "print(np.shape(x_delta_aug))\n",
        "\n",
        "x_theta = x_train[:,1,:,:]\n",
        "x_theta_clip = x_train[rnd_idx:rnd_idx+100,1,:,:]\n",
        "x_theta_aug = np.append(x_theta,x_theta_clip, axis=0)\n",
        "\n",
        "x_alpha = x_train[:,2,:,:]\n",
        "x_alpha_clip = x_train[rnd_idx:rnd_idx+100,2,:,:]\n",
        "x_alpha_aug = np.append(x_alpha,x_alpha_clip, axis=0)\n",
        "\n",
        "x_beta = x_train[:,3,:,:]\n",
        "x_beta_clip = x_train[rnd_idx:rnd_idx+100,3,:,:]\n",
        "x_beta_aug = np.append(x_beta,x_beta_clip, axis=0)\n",
        "\n",
        "x_gamma = x_train[:,4,:,:]\n",
        "x_gamma_clip = x_train[rnd_idx:rnd_idx+100,4,:,:]\n",
        "x_gamma_aug = np.append(x_gamma,x_gamma_clip, axis=0)\n",
        "\n",
        "y_clip = y_train[rnd_idx:rnd_idx+100]\n",
        "y_train_aug = np.append(y_train,y_clip, axis=0)\n",
        "print(np.shape(y_train_aug),'\\n', y_train_aug[-1,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaaW295Zjp0E",
        "outputId": "06d89704-7054-41f6-d3f3-6b39657712ad"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(528, 32, 32)\n",
            "14\n",
            "(100, 32, 32)\n",
            "(628, 32, 32)\n",
            "(628, 4) \n",
            " [0. 0. 1. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,4))\n",
        "x_plt = np.arange(0,32*32)\n",
        "plt.subplot(1,5,1)\n",
        "plt.plot(x_plt, x_delta_aug[0,:,:].ravel());\n",
        "plt.subplot(1,5,2)\n",
        "plt.plot(x_plt, x_theta_aug[0,:,:].ravel());\n",
        "plt.subplot(1,5,3)\n",
        "plt.plot(x_plt, x_alpha_aug[0,:,:].ravel());\n",
        "plt.subplot(1,5,4)\n",
        "plt.plot(x_plt, x_beta_aug[0,:,:].ravel());\n",
        "plt.subplot(1,5,5)\n",
        "plt.plot(x_plt, x_gamma_aug[0,:,:].ravel());"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "K46-VTOENUYS",
        "outputId": "ff7055bd-edaf-4f8a-d641-75fbafce5e06"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adding flipping augmentation\n",
        "\n",
        "x_delta = x_train[:,0,:,:]\n",
        "print(np.shape(x_delta))\n",
        "rnd_idx2 = int(np.round(np.random.rand(1)*100)[0])\n",
        "print(rnd_idx2)\n",
        "x_delta_clip2 = x_train[rnd_idx2:rnd_idx2+100,0,:,:]\n",
        "print(np.shape(x_delta_clip2))\n",
        "x_theta_clip2 = x_train[rnd_idx2:rnd_idx2+100,1,:,:]\n",
        "x_alpha_clip2 = x_train[rnd_idx2:rnd_idx2+100,2,:,:]\n",
        "x_beta_clip2 = x_train[rnd_idx2:rnd_idx2+100,3,:,:]\n",
        "x_gamma_clip2 = x_train[rnd_idx2:rnd_idx2+100,4,:,:]\n",
        "\n",
        "x_size = np.shape(x_delta_clip2)[0]\n",
        "x_delta_flip = np.empty((x_size, 32,32))\n",
        "x_theta_flip = np.empty((x_size, 32,32))\n",
        "x_alpha_flip = np.empty((x_size, 32,32))\n",
        "x_beta_flip = np.empty((x_size, 32,32))\n",
        "x_gamma_flip = np.empty((x_size, 32,32))\n",
        "\n",
        "for f in range(0, x_size):\n",
        "  x_delta_fl = np.flip(x_delta_clip2[f,:,:].ravel())\n",
        "  x_delta_flip[f,:,:] = np.reshape(x_delta_fl, (32,32))\n",
        "\n",
        "  x_theta_fl = np.flip(x_theta_clip2[f,:,:].ravel())\n",
        "  x_theta_flip[f,:,:] = np.reshape(x_theta_fl, (32,32))\n",
        "\n",
        "  x_alpha_fl = np.flip(x_alpha_clip2[f,:,:].ravel())\n",
        "  x_alpha_flip[f,:,:] = np.reshape(x_alpha_fl, (32,32))\n",
        "\n",
        "  x_beta_fl = np.flip(x_beta_clip2[f,:,:].ravel())\n",
        "  x_beta_flip[f,:,:] = np.reshape(x_beta_fl, (32,32))\n",
        "\n",
        "  x_gamma_fl = np.flip(x_gamma_clip2[f,:,:].ravel())\n",
        "  x_gamma_flip[f,:,:] = np.reshape(x_gamma_fl, (32,32))\n",
        "\n",
        "print(np.shape(x_delta_flip))\n",
        "\n",
        "plt.figure(figsize=(20,4))\n",
        "x_plt = np.arange(0,32*32)\n",
        "plt.subplot(1,5,1)\n",
        "plt.plot(x_plt, x_delta_aug[0,:,:].ravel());\n",
        "plt.subplot(1,5,2)\n",
        "plt.plot(x_plt, x_delta_flip[0,:,:].ravel());\n",
        "\n",
        "x_delta_aug2 = np.append(x_delta_aug,x_delta_flip, axis=0)\n",
        "print(np.shape(x_delta_aug2))\n",
        "x_theta_aug2 = np.append(x_theta_aug,x_theta_flip, axis=0)\n",
        "x_alpha_aug2 = np.append(x_alpha_aug,x_alpha_flip, axis=0)\n",
        "x_beta_aug2 = np.append(x_beta_aug,x_beta_flip, axis=0)\n",
        "x_gamma_aug2 = np.append(x_gamma_aug,x_gamma_flip, axis=0)\n",
        "\n",
        "y_clip2 = y_train[rnd_idx2:rnd_idx2+100]\n",
        "\n",
        "y_train_aug2 = np.append(y_train_aug,y_clip2, axis=0)\n",
        "print(np.shape(y_train_aug2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "wVeha0tKPtDH",
        "outputId": "5ca39ce8-7a6a-43a8-aefb-2f043d0e2c56"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(528, 32, 32)\n",
            "83\n",
            "(100, 32, 32)\n",
            "(100, 32, 32)\n",
            "(728, 32, 32)\n",
            "(728, 4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FutIrekXjult",
        "outputId": "28291281-f81d-4e1d-9e94-63aec3270aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.1569 - acc: 0.3894 - recall_5: 0.0705 - precision_5: 0.4272\n",
            "Epoch 1: val_loss improved from inf to 1.32595, saving model to weights.best.hdf5\n",
            "43/43 [==============================] - 2s 24ms/step - loss: 1.1541 - acc: 0.3895 - recall_5: 0.0727 - precision_5: 0.4274 - val_loss: 1.3259 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 2/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.1133 - acc: 0.3828 - recall_5: 0.1031 - precision_5: 0.3860\n",
            "Epoch 2: val_loss did not improve from 1.32595\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.1137 - acc: 0.3823 - recall_5: 0.1032 - precision_5: 0.3923 - val_loss: 1.3482 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 3/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.1055 - acc: 0.4012 - recall_5: 0.0887 - precision_5: 0.4122\n",
            "Epoch 3: val_loss improved from 1.32595 to 1.27970, saving model to weights.best.hdf5\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 1.1055 - acc: 0.4012 - recall_5: 0.0887 - precision_5: 0.4122 - val_loss: 1.2797 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 4/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0936 - acc: 0.4122 - recall_5: 0.0952 - precision_5: 0.4103\n",
            "Epoch 4: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0938 - acc: 0.4128 - recall_5: 0.0945 - precision_5: 0.4012 - val_loss: 1.3056 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 5/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0810 - acc: 0.4201 - recall_5: 0.1047 - precision_5: 0.4898\n",
            "Epoch 5: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0810 - acc: 0.4201 - recall_5: 0.1047 - precision_5: 0.4898 - val_loss: 1.3735 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 6/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.1021 - acc: 0.3734 - recall_5: 0.0689 - precision_5: 0.4343\n",
            "Epoch 6: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0987 - acc: 0.3852 - recall_5: 0.0698 - precision_5: 0.4248 - val_loss: 1.3343 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 7/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0785 - acc: 0.4062 - recall_5: 0.0952 - precision_5: 0.4706\n",
            "Epoch 7: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0775 - acc: 0.4113 - recall_5: 0.0945 - precision_5: 0.4676 - val_loss: 1.3558 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 8/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0851 - acc: 0.4122 - recall_5: 0.0848 - precision_5: 0.4634\n",
            "Epoch 8: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0864 - acc: 0.4055 - recall_5: 0.0843 - precision_5: 0.4677 - val_loss: 1.3849 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 9/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0737 - acc: 0.4018 - recall_5: 0.0610 - precision_5: 0.4881\n",
            "Epoch 9: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0750 - acc: 0.4026 - recall_5: 0.0640 - precision_5: 0.4783 - val_loss: 1.3421 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 10/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0809 - acc: 0.4018 - recall_5: 0.0580 - precision_5: 0.4286\n",
            "Epoch 10: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0787 - acc: 0.4041 - recall_5: 0.0610 - precision_5: 0.4375 - val_loss: 1.3385 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 11/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0966 - acc: 0.3854 - recall_5: 0.0506 - precision_5: 0.3617\n",
            "Epoch 11: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0936 - acc: 0.3910 - recall_5: 0.0494 - precision_5: 0.3617 - val_loss: 1.3711 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 12/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0845 - acc: 0.4099 - recall_5: 0.0654 - precision_5: 0.4945\n",
            "Epoch 12: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0845 - acc: 0.4099 - recall_5: 0.0654 - precision_5: 0.4945 - val_loss: 1.3613 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 13/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0747 - acc: 0.4314 - recall_5: 0.0610 - precision_5: 0.4762\n",
            "Epoch 13: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0745 - acc: 0.4346 - recall_5: 0.0596 - precision_5: 0.4767 - val_loss: 1.3836 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 14/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0692 - acc: 0.4286 - recall_5: 0.0670 - precision_5: 0.4891\n",
            "Epoch 14: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0702 - acc: 0.4244 - recall_5: 0.0669 - precision_5: 0.4946 - val_loss: 1.4256 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 15/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0797 - acc: 0.4238 - recall_5: 0.0152 - precision_5: 0.3333\n",
            "Epoch 15: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0770 - acc: 0.4288 - recall_5: 0.0233 - precision_5: 0.3902 - val_loss: 1.3569 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 16/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0754 - acc: 0.4187 - recall_5: 0.0656 - precision_5: 0.5316\n",
            "Epoch 16: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0716 - acc: 0.4215 - recall_5: 0.0640 - precision_5: 0.5176 - val_loss: 1.3993 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 17/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0716 - acc: 0.4137 - recall_5: 0.0387 - precision_5: 0.3333\n",
            "Epoch 17: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0727 - acc: 0.4142 - recall_5: 0.0422 - precision_5: 0.3537 - val_loss: 1.3957 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 18/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0690 - acc: 0.4360 - recall_5: 0.1280 - precision_5: 0.5409\n",
            "Epoch 18: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0677 - acc: 0.4390 - recall_5: 0.1250 - precision_5: 0.5409 - val_loss: 1.4631 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 19/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0815 - acc: 0.4077 - recall_5: 0.0298 - precision_5: 0.4651\n",
            "Epoch 19: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0798 - acc: 0.4084 - recall_5: 0.0305 - precision_5: 0.4773 - val_loss: 1.4169 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 20/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0615 - acc: 0.4286 - recall_5: 0.0506 - precision_5: 0.5667\n",
            "Epoch 20: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0626 - acc: 0.4288 - recall_5: 0.0494 - precision_5: 0.5667 - val_loss: 1.4531 - val_acc: 0.4750 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 21/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0721 - acc: 0.4177 - recall_5: 0.0534 - precision_5: 0.4930\n",
            "Epoch 21: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0719 - acc: 0.4201 - recall_5: 0.0523 - precision_5: 0.5000 - val_loss: 1.4347 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 22/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0829 - acc: 0.3958 - recall_5: 0.0357 - precision_5: 0.3750\n",
            "Epoch 22: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0837 - acc: 0.3968 - recall_5: 0.0349 - precision_5: 0.3750 - val_loss: 1.4230 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 23/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0804 - acc: 0.3899 - recall_5: 0.0164 - precision_5: 0.3143\n",
            "Epoch 23: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0788 - acc: 0.3939 - recall_5: 0.0160 - precision_5: 0.3056 - val_loss: 1.3952 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 24/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0764 - acc: 0.4033 - recall_5: 0.0432 - precision_5: 0.4603\n",
            "Epoch 24: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0749 - acc: 0.4055 - recall_5: 0.0422 - precision_5: 0.4603 - val_loss: 1.4139 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 25/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0676 - acc: 0.4360 - recall_5: 0.0327 - precision_5: 0.5789\n",
            "Epoch 25: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0654 - acc: 0.4404 - recall_5: 0.0320 - precision_5: 0.5789 - val_loss: 1.4383 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 26/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0779 - acc: 0.3948 - recall_5: 0.0412 - precision_5: 0.4737\n",
            "Epoch 26: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0809 - acc: 0.3924 - recall_5: 0.0392 - precision_5: 0.4737 - val_loss: 1.4308 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 27/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0743 - acc: 0.4273 - recall_5: 0.0378 - precision_5: 0.4333\n",
            "Epoch 27: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0743 - acc: 0.4273 - recall_5: 0.0378 - precision_5: 0.4333 - val_loss: 1.4250 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 28/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0746 - acc: 0.4226 - recall_5: 0.0238 - precision_5: 0.4211\n",
            "Epoch 28: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0744 - acc: 0.4215 - recall_5: 0.0247 - precision_5: 0.4250 - val_loss: 1.4114 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 29/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0707 - acc: 0.4137 - recall_5: 0.0238 - precision_5: 0.3478\n",
            "Epoch 29: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0712 - acc: 0.4186 - recall_5: 0.0233 - precision_5: 0.3478 - val_loss: 1.4078 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 30/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0708 - acc: 0.4345 - recall_5: 0.0259 - precision_5: 0.5000\n",
            "Epoch 30: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0697 - acc: 0.4346 - recall_5: 0.0262 - precision_5: 0.4615 - val_loss: 1.4083 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 31/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0720 - acc: 0.4085 - recall_5: 0.0320 - precision_5: 0.4118\n",
            "Epoch 31: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0716 - acc: 0.4070 - recall_5: 0.0320 - precision_5: 0.4151 - val_loss: 1.4225 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 32/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0751 - acc: 0.4241 - recall_5: 0.0060 - precision_5: 0.1739\n",
            "Epoch 32: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0733 - acc: 0.4259 - recall_5: 0.0087 - precision_5: 0.2400 - val_loss: 1.4049 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 33/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0704 - acc: 0.4137 - recall_5: 0.0595 - precision_5: 0.4938\n",
            "Epoch 33: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0711 - acc: 0.4084 - recall_5: 0.0581 - precision_5: 0.4938 - val_loss: 1.4681 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 34/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0791 - acc: 0.3929 - recall_5: 0.0149 - precision_5: 0.5556\n",
            "Epoch 34: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0786 - acc: 0.3983 - recall_5: 0.0145 - precision_5: 0.5556 - val_loss: 1.4129 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 35/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0710 - acc: 0.4182 - recall_5: 0.0193 - precision_5: 0.5652\n",
            "Epoch 35: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0714 - acc: 0.4215 - recall_5: 0.0218 - precision_5: 0.6000 - val_loss: 1.4100 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 36/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0737 - acc: 0.4238 - recall_5: 0.0534 - precision_5: 0.4217\n",
            "Epoch 36: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0743 - acc: 0.4172 - recall_5: 0.0509 - precision_5: 0.4217 - val_loss: 1.4534 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 37/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0663 - acc: 0.4201 - recall_5: 0.0174 - precision_5: 0.5217\n",
            "Epoch 37: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0663 - acc: 0.4201 - recall_5: 0.0174 - precision_5: 0.5217 - val_loss: 1.4277 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 38/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0678 - acc: 0.4211 - recall_5: 0.0253 - precision_5: 0.6538\n",
            "Epoch 38: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0695 - acc: 0.4215 - recall_5: 0.0247 - precision_5: 0.6296 - val_loss: 1.4233 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 39/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0732 - acc: 0.4279 - recall_5: 0.0433 - precision_5: 0.3803\n",
            "Epoch 39: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0703 - acc: 0.4259 - recall_5: 0.0392 - precision_5: 0.3750 - val_loss: 1.4499 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 40/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0701 - acc: 0.4449 - recall_5: 0.0208 - precision_5: 0.4242\n",
            "Epoch 40: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0708 - acc: 0.4462 - recall_5: 0.0203 - precision_5: 0.4242 - val_loss: 1.4420 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 41/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0669 - acc: 0.4167 - recall_5: 0.0253 - precision_5: 0.4146\n",
            "Epoch 41: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0694 - acc: 0.4186 - recall_5: 0.0247 - precision_5: 0.4146 - val_loss: 1.4342 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 42/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0673 - acc: 0.4256 - recall_5: 0.0179 - precision_5: 0.5714\n",
            "Epoch 42: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0665 - acc: 0.4244 - recall_5: 0.0189 - precision_5: 0.5417 - val_loss: 1.4137 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 43/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0693 - acc: 0.4405 - recall_5: 0.0268 - precision_5: 0.4186\n",
            "Epoch 43: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0691 - acc: 0.4433 - recall_5: 0.0262 - precision_5: 0.4186 - val_loss: 1.4416 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 44/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0602 - acc: 0.4329 - recall_5: 0.0137 - precision_5: 0.3913\n",
            "Epoch 44: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0627 - acc: 0.4273 - recall_5: 0.0131 - precision_5: 0.3750 - val_loss: 1.4629 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 45/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0671 - acc: 0.4273 - recall_5: 0.0247 - precision_5: 0.4857\n",
            "Epoch 45: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0671 - acc: 0.4273 - recall_5: 0.0247 - precision_5: 0.4857 - val_loss: 1.4461 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 46/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0660 - acc: 0.4464 - recall_5: 0.0357 - precision_5: 0.5333\n",
            "Epoch 46: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0657 - acc: 0.4477 - recall_5: 0.0349 - precision_5: 0.5333 - val_loss: 1.4534 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 47/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0679 - acc: 0.4201 - recall_5: 0.0334 - precision_5: 0.5476\n",
            "Epoch 47: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0679 - acc: 0.4201 - recall_5: 0.0334 - precision_5: 0.5476 - val_loss: 1.4394 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 48/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0661 - acc: 0.4314 - recall_5: 0.0351 - precision_5: 0.4182\n",
            "Epoch 48: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0673 - acc: 0.4288 - recall_5: 0.0349 - precision_5: 0.4286 - val_loss: 1.4764 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 49/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0706 - acc: 0.4177 - recall_5: 0.0229 - precision_5: 0.5172\n",
            "Epoch 49: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0703 - acc: 0.4186 - recall_5: 0.0218 - precision_5: 0.5000 - val_loss: 1.4624 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 50/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0643 - acc: 0.4172 - recall_5: 0.0247 - precision_5: 0.4474\n",
            "Epoch 50: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0643 - acc: 0.4172 - recall_5: 0.0247 - precision_5: 0.4474 - val_loss: 1.4667 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 51/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0653 - acc: 0.4286 - recall_5: 0.0327 - precision_5: 0.5366\n",
            "Epoch 51: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0657 - acc: 0.4288 - recall_5: 0.0320 - precision_5: 0.5366 - val_loss: 1.4735 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 52/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0697 - acc: 0.4301 - recall_5: 0.0208 - precision_5: 0.5185\n",
            "Epoch 52: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0668 - acc: 0.4346 - recall_5: 0.0218 - precision_5: 0.5357 - val_loss: 1.4513 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 53/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0712 - acc: 0.4207 - recall_5: 0.0320 - precision_5: 0.5250\n",
            "Epoch 53: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0698 - acc: 0.4230 - recall_5: 0.0320 - precision_5: 0.5366 - val_loss: 1.4569 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 54/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0568 - acc: 0.4449 - recall_5: 0.0565 - precision_5: 0.5938\n",
            "Epoch 54: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0579 - acc: 0.4433 - recall_5: 0.0552 - precision_5: 0.5938 - val_loss: 1.4865 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 55/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0610 - acc: 0.4390 - recall_5: 0.0335 - precision_5: 0.6286\n",
            "Epoch 55: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0583 - acc: 0.4462 - recall_5: 0.0349 - precision_5: 0.6486 - val_loss: 1.4352 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 56/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0698 - acc: 0.4201 - recall_5: 0.0349 - precision_5: 0.4444\n",
            "Epoch 56: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0698 - acc: 0.4201 - recall_5: 0.0349 - precision_5: 0.4444 - val_loss: 1.4620 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 57/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0665 - acc: 0.4404 - recall_5: 0.0233 - precision_5: 0.4000\n",
            "Epoch 57: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0665 - acc: 0.4404 - recall_5: 0.0233 - precision_5: 0.4000 - val_loss: 1.4531 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 58/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0606 - acc: 0.4284 - recall_5: 0.0381 - precision_5: 0.5682\n",
            "Epoch 58: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0608 - acc: 0.4317 - recall_5: 0.0363 - precision_5: 0.5435 - val_loss: 1.4744 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 59/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0624 - acc: 0.4329 - recall_5: 0.0488 - precision_5: 0.5517\n",
            "Epoch 59: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0598 - acc: 0.4390 - recall_5: 0.0480 - precision_5: 0.5593 - val_loss: 1.4628 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 60/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0619 - acc: 0.4223 - recall_5: 0.0381 - precision_5: 0.5208\n",
            "Epoch 60: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0624 - acc: 0.4215 - recall_5: 0.0378 - precision_5: 0.5306 - val_loss: 1.4585 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 61/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0643 - acc: 0.4299 - recall_5: 0.0290 - precision_5: 0.5429\n",
            "Epoch 61: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0675 - acc: 0.4244 - recall_5: 0.0305 - precision_5: 0.5526 - val_loss: 1.4578 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 62/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 1.0618 - acc: 0.4260 - recall_5: 0.0378 - precision_5: 0.5476\n",
            "Epoch 62: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0623 - acc: 0.4230 - recall_5: 0.0349 - precision_5: 0.5333 - val_loss: 1.4564 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 63/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0555 - acc: 0.4500 - recall_5: 0.0531 - precision_5: 0.5000\n",
            "Epoch 63: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0599 - acc: 0.4491 - recall_5: 0.0509 - precision_5: 0.5072 - val_loss: 1.4560 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 64/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0646 - acc: 0.4344 - recall_5: 0.0328 - precision_5: 0.4286\n",
            "Epoch 64: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0656 - acc: 0.4259 - recall_5: 0.0349 - precision_5: 0.4444 - val_loss: 1.4663 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 65/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0618 - acc: 0.4462 - recall_5: 0.0305 - precision_5: 0.5526\n",
            "Epoch 65: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0618 - acc: 0.4462 - recall_5: 0.0305 - precision_5: 0.5526 - val_loss: 1.4487 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 66/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0584 - acc: 0.4390 - recall_5: 0.0351 - precision_5: 0.5111\n",
            "Epoch 66: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0587 - acc: 0.4433 - recall_5: 0.0334 - precision_5: 0.5111 - val_loss: 1.4769 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 67/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0598 - acc: 0.4405 - recall_5: 0.0396 - precision_5: 0.4333\n",
            "Epoch 67: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0625 - acc: 0.4390 - recall_5: 0.0392 - precision_5: 0.4355 - val_loss: 1.4849 - val_acc: 0.6500 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 68/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0672 - acc: 0.4405 - recall_5: 0.0168 - precision_5: 0.4231\n",
            "Epoch 68: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0655 - acc: 0.4448 - recall_5: 0.0174 - precision_5: 0.4138 - val_loss: 1.4431 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 69/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0601 - acc: 0.4598 - recall_5: 0.0298 - precision_5: 0.6250\n",
            "Epoch 69: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0595 - acc: 0.4608 - recall_5: 0.0334 - precision_5: 0.6053 - val_loss: 1.4162 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 70/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0534 - acc: 0.4360 - recall_5: 0.0685 - precision_5: 0.5897\n",
            "Epoch 70: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0533 - acc: 0.4331 - recall_5: 0.0683 - precision_5: 0.5949 - val_loss: 1.4721 - val_acc: 0.7000 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 71/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0626 - acc: 0.4665 - recall_5: 0.0396 - precision_5: 0.5652\n",
            "Epoch 71: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0606 - acc: 0.4680 - recall_5: 0.0407 - precision_5: 0.5714 - val_loss: 1.4417 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 72/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0553 - acc: 0.4512 - recall_5: 0.0305 - precision_5: 0.4651\n",
            "Epoch 72: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0556 - acc: 0.4477 - recall_5: 0.0334 - precision_5: 0.4600 - val_loss: 1.4409 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 73/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0580 - acc: 0.4437 - recall_5: 0.0562 - precision_5: 0.5625\n",
            "Epoch 73: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0621 - acc: 0.4419 - recall_5: 0.0538 - precision_5: 0.5286 - val_loss: 1.4601 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 74/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0710 - acc: 0.4122 - recall_5: 0.0446 - precision_5: 0.4412\n",
            "Epoch 74: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0701 - acc: 0.4113 - recall_5: 0.0451 - precision_5: 0.4429 - val_loss: 1.4599 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 75/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0634 - acc: 0.4345 - recall_5: 0.0372 - precision_5: 0.5682\n",
            "Epoch 75: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0618 - acc: 0.4360 - recall_5: 0.0378 - precision_5: 0.5778 - val_loss: 1.4218 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 76/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0660 - acc: 0.4297 - recall_5: 0.0328 - precision_5: 0.4773\n",
            "Epoch 76: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0667 - acc: 0.4331 - recall_5: 0.0305 - precision_5: 0.4667 - val_loss: 1.4268 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 77/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0581 - acc: 0.4688 - recall_5: 0.0406 - precision_5: 0.5652\n",
            "Epoch 77: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0592 - acc: 0.4666 - recall_5: 0.0422 - precision_5: 0.5800 - val_loss: 1.4381 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 78/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0565 - acc: 0.4420 - recall_5: 0.0759 - precision_5: 0.5930\n",
            "Epoch 78: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0549 - acc: 0.4433 - recall_5: 0.0741 - precision_5: 0.5930 - val_loss: 1.4833 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 79/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0592 - acc: 0.4420 - recall_5: 0.0312 - precision_5: 0.4667\n",
            "Epoch 79: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0592 - acc: 0.4419 - recall_5: 0.0320 - precision_5: 0.4151 - val_loss: 1.4175 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 80/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0605 - acc: 0.4375 - recall_5: 0.0655 - precision_5: 0.4889\n",
            "Epoch 80: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0596 - acc: 0.4390 - recall_5: 0.0669 - precision_5: 0.4842 - val_loss: 1.4573 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 81/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0563 - acc: 0.4345 - recall_5: 0.0446 - precision_5: 0.5660\n",
            "Epoch 81: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0552 - acc: 0.4346 - recall_5: 0.0436 - precision_5: 0.5556 - val_loss: 1.4413 - val_acc: 0.7000 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 82/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0593 - acc: 0.4672 - recall_5: 0.0703 - precision_5: 0.5056\n",
            "Epoch 82: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0591 - acc: 0.4637 - recall_5: 0.0669 - precision_5: 0.4894 - val_loss: 1.4798 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 83/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0618 - acc: 0.4439 - recall_5: 0.0593 - precision_5: 0.5873\n",
            "Epoch 83: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0612 - acc: 0.4448 - recall_5: 0.0552 - precision_5: 0.5846 - val_loss: 1.4605 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 84/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0480 - acc: 0.4844 - recall_5: 0.0422 - precision_5: 0.5510\n",
            "Epoch 84: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0486 - acc: 0.4855 - recall_5: 0.0552 - precision_5: 0.5672 - val_loss: 1.4290 - val_acc: 0.7250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 85/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0537 - acc: 0.4578 - recall_5: 0.1003 - precision_5: 0.6330\n",
            "Epoch 85: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0537 - acc: 0.4578 - recall_5: 0.1003 - precision_5: 0.6330 - val_loss: 1.4809 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 86/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0492 - acc: 0.4500 - recall_5: 0.0500 - precision_5: 0.5614\n",
            "Epoch 86: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0465 - acc: 0.4477 - recall_5: 0.0552 - precision_5: 0.5846 - val_loss: 1.4789 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 87/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0488 - acc: 0.4433 - recall_5: 0.0727 - precision_5: 0.5495\n",
            "Epoch 87: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0488 - acc: 0.4433 - recall_5: 0.0727 - precision_5: 0.5495 - val_loss: 1.5102 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 88/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0286 - acc: 0.4808 - recall_5: 0.1202 - precision_5: 0.6410\n",
            "Epoch 88: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0364 - acc: 0.4724 - recall_5: 0.1163 - precision_5: 0.6400 - val_loss: 1.5595 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 89/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0618 - acc: 0.4588 - recall_5: 0.0625 - precision_5: 0.6029\n",
            "Epoch 89: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1.0608 - acc: 0.4608 - recall_5: 0.0596 - precision_5: 0.5857 - val_loss: 1.4394 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 90/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0522 - acc: 0.4598 - recall_5: 0.0997 - precision_5: 0.5929\n",
            "Epoch 90: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0507 - acc: 0.4608 - recall_5: 0.0974 - precision_5: 0.5877 - val_loss: 1.4749 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 91/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0454 - acc: 0.4649 - recall_5: 0.1006 - precision_5: 0.5841\n",
            "Epoch 91: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0465 - acc: 0.4651 - recall_5: 0.1003 - precision_5: 0.5565 - val_loss: 1.4596 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 92/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0518 - acc: 0.4497 - recall_5: 0.0899 - precision_5: 0.5413\n",
            "Epoch 92: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0517 - acc: 0.4491 - recall_5: 0.0872 - precision_5: 0.5217 - val_loss: 1.4637 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 93/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0465 - acc: 0.4421 - recall_5: 0.0991 - precision_5: 0.6311\n",
            "Epoch 93: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0466 - acc: 0.4433 - recall_5: 0.0974 - precision_5: 0.6147 - val_loss: 1.4910 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 94/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0409 - acc: 0.4656 - recall_5: 0.1156 - precision_5: 0.5920\n",
            "Epoch 94: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0438 - acc: 0.4651 - recall_5: 0.1105 - precision_5: 0.5891 - val_loss: 1.5179 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 95/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0497 - acc: 0.4519 - recall_5: 0.1138 - precision_5: 0.5504\n",
            "Epoch 95: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0482 - acc: 0.4578 - recall_5: 0.1076 - precision_5: 0.5481 - val_loss: 1.5551 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 96/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0444 - acc: 0.4732 - recall_5: 0.0952 - precision_5: 0.5981\n",
            "Epoch 96: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0438 - acc: 0.4724 - recall_5: 0.0988 - precision_5: 0.6126 - val_loss: 1.4711 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 97/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0430 - acc: 0.4734 - recall_5: 0.0953 - precision_5: 0.5922\n",
            "Epoch 97: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0412 - acc: 0.4695 - recall_5: 0.1003 - precision_5: 0.5798 - val_loss: 1.5353 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 98/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0330 - acc: 0.4787 - recall_5: 0.1235 - precision_5: 0.5625\n",
            "Epoch 98: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0354 - acc: 0.4797 - recall_5: 0.1206 - precision_5: 0.5608 - val_loss: 1.5537 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 99/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0454 - acc: 0.4588 - recall_5: 0.1220 - precision_5: 0.5517\n",
            "Epoch 99: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0442 - acc: 0.4593 - recall_5: 0.1235 - precision_5: 0.5629 - val_loss: 1.5240 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 100/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0429 - acc: 0.4812 - recall_5: 0.0938 - precision_5: 0.5310\n",
            "Epoch 100: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 1.0421 - acc: 0.4811 - recall_5: 0.0988 - precision_5: 0.5440 - val_loss: 1.4595 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 101/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0540 - acc: 0.4625 - recall_5: 0.1172 - precision_5: 0.5396\n",
            "Epoch 101: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0492 - acc: 0.4666 - recall_5: 0.1250 - precision_5: 0.5584 - val_loss: 1.4671 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 102/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0338 - acc: 0.4812 - recall_5: 0.1281 - precision_5: 0.5541\n",
            "Epoch 102: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0369 - acc: 0.4753 - recall_5: 0.1323 - precision_5: 0.5617 - val_loss: 1.4514 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 103/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0428 - acc: 0.4649 - recall_5: 0.1341 - precision_5: 0.5946\n",
            "Epoch 103: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0430 - acc: 0.4622 - recall_5: 0.1366 - precision_5: 0.5987 - val_loss: 1.5016 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 104/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0423 - acc: 0.4688 - recall_5: 0.1422 - precision_5: 0.5583\n",
            "Epoch 104: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0377 - acc: 0.4782 - recall_5: 0.1439 - precision_5: 0.5723 - val_loss: 1.5562 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 105/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0451 - acc: 0.4599 - recall_5: 0.1538 - precision_5: 0.6000\n",
            "Epoch 105: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0398 - acc: 0.4724 - recall_5: 0.1468 - precision_5: 0.6084 - val_loss: 1.5490 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 106/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0320 - acc: 0.4856 - recall_5: 0.1458 - precision_5: 0.5796\n",
            "Epoch 106: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0381 - acc: 0.4826 - recall_5: 0.1366 - precision_5: 0.5802 - val_loss: 1.4558 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 107/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0289 - acc: 0.4848 - recall_5: 0.1753 - precision_5: 0.5928\n",
            "Epoch 107: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 1.0305 - acc: 0.4811 - recall_5: 0.1701 - precision_5: 0.5939 - val_loss: 1.5050 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 108/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0359 - acc: 0.4792 - recall_5: 0.1474 - precision_5: 0.6133\n",
            "Epoch 108: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0372 - acc: 0.4753 - recall_5: 0.1497 - precision_5: 0.6242 - val_loss: 1.4923 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 109/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0314 - acc: 0.4911 - recall_5: 0.1562 - precision_5: 0.5932\n",
            "Epoch 109: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0288 - acc: 0.4956 - recall_5: 0.1541 - precision_5: 0.5922 - val_loss: 1.4615 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 110/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0337 - acc: 0.4840 - recall_5: 0.1651 - precision_5: 0.5988\n",
            "Epoch 110: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0354 - acc: 0.4826 - recall_5: 0.1613 - precision_5: 0.5873 - val_loss: 1.4577 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 111/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0378 - acc: 0.4663 - recall_5: 0.1699 - precision_5: 0.5889\n",
            "Epoch 111: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0362 - acc: 0.4709 - recall_5: 0.1730 - precision_5: 0.5777 - val_loss: 1.5193 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 112/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0301 - acc: 0.4832 - recall_5: 0.1829 - precision_5: 0.6091\n",
            "Epoch 112: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0297 - acc: 0.4811 - recall_5: 0.1846 - precision_5: 0.6019 - val_loss: 1.5694 - val_acc: 0.6250 - val_recall_5: 0.0250 - val_precision_5: 0.2500\n",
            "Epoch 113/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0302 - acc: 0.4741 - recall_5: 0.1829 - precision_5: 0.6186\n",
            "Epoch 113: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0323 - acc: 0.4738 - recall_5: 0.1773 - precision_5: 0.6040 - val_loss: 1.5026 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 114/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 1.0202 - acc: 0.4901 - recall_5: 0.1859 - precision_5: 0.6141\n",
            "Epoch 114: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0208 - acc: 0.4855 - recall_5: 0.1875 - precision_5: 0.6028 - val_loss: 1.4607 - val_acc: 0.6250 - val_recall_5: 0.3750 - val_precision_5: 0.9375\n",
            "Epoch 115/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0210 - acc: 0.4984 - recall_5: 0.2196 - precision_5: 0.5905\n",
            "Epoch 115: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0196 - acc: 0.4985 - recall_5: 0.2166 - precision_5: 0.5936 - val_loss: 1.4712 - val_acc: 0.6250 - val_recall_5: 0.3750 - val_precision_5: 0.9375\n",
            "Epoch 116/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0182 - acc: 0.4904 - recall_5: 0.2212 - precision_5: 0.6389\n",
            "Epoch 116: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0164 - acc: 0.4956 - recall_5: 0.2253 - precision_5: 0.6327 - val_loss: 1.4470 - val_acc: 0.6250 - val_recall_5: 0.3750 - val_precision_5: 0.7895\n",
            "Epoch 117/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 1.0307 - acc: 0.4901 - recall_5: 0.1891 - precision_5: 0.5227\n",
            "Epoch 117: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0280 - acc: 0.4913 - recall_5: 0.1904 - precision_5: 0.5325 - val_loss: 1.5225 - val_acc: 0.6250 - val_recall_5: 0.0000e+00 - val_precision_5: 0.0000e+00\n",
            "Epoch 118/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0238 - acc: 0.4848 - recall_5: 0.2043 - precision_5: 0.5877\n",
            "Epoch 118: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 1.0239 - acc: 0.4855 - recall_5: 0.2020 - precision_5: 0.5840 - val_loss: 1.5083 - val_acc: 0.6250 - val_recall_5: 0.1500 - val_precision_5: 1.0000\n",
            "Epoch 119/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0155 - acc: 0.5016 - recall_5: 0.2266 - precision_5: 0.6277\n",
            "Epoch 119: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0137 - acc: 0.5015 - recall_5: 0.2282 - precision_5: 0.6356 - val_loss: 1.4437 - val_acc: 0.6250 - val_recall_5: 0.4250 - val_precision_5: 0.7083\n",
            "Epoch 120/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 1.0161 - acc: 0.4967 - recall_5: 0.2188 - precision_5: 0.6129\n",
            "Epoch 120: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0223 - acc: 0.4913 - recall_5: 0.2166 - precision_5: 0.5889 - val_loss: 1.5878 - val_acc: 0.6250 - val_recall_5: 0.0250 - val_precision_5: 0.2500\n",
            "Epoch 121/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0240 - acc: 0.4954 - recall_5: 0.2012 - precision_5: 0.6055\n",
            "Epoch 121: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0217 - acc: 0.4971 - recall_5: 0.2064 - precision_5: 0.6068 - val_loss: 1.4469 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6923\n",
            "Epoch 122/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0206 - acc: 0.4781 - recall_5: 0.2000 - precision_5: 0.5494\n",
            "Epoch 122: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0224 - acc: 0.4782 - recall_5: 0.2049 - precision_5: 0.5486 - val_loss: 1.4957 - val_acc: 0.6250 - val_recall_5: 0.3250 - val_precision_5: 0.8667\n",
            "Epoch 123/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0165 - acc: 0.5031 - recall_5: 0.2344 - precision_5: 0.5814\n",
            "Epoch 123: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0178 - acc: 0.5000 - recall_5: 0.2340 - precision_5: 0.5876 - val_loss: 1.5606 - val_acc: 0.6250 - val_recall_5: 0.0500 - val_precision_5: 0.6667\n",
            "Epoch 124/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0214 - acc: 0.4970 - recall_5: 0.1994 - precision_5: 0.5751\n",
            "Epoch 124: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0228 - acc: 0.4942 - recall_5: 0.1977 - precision_5: 0.5714 - val_loss: 1.5056 - val_acc: 0.6250 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 125/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0153 - acc: 0.4696 - recall_5: 0.2147 - precision_5: 0.6063\n",
            "Epoch 125: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0145 - acc: 0.4767 - recall_5: 0.2224 - precision_5: 0.6047 - val_loss: 1.4934 - val_acc: 0.6250 - val_recall_5: 0.3750 - val_precision_5: 0.7895\n",
            "Epoch 126/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0038 - acc: 0.5192 - recall_5: 0.2500 - precision_5: 0.6070\n",
            "Epoch 126: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0103 - acc: 0.5087 - recall_5: 0.2413 - precision_5: 0.5929 - val_loss: 1.5652 - val_acc: 0.6250 - val_recall_5: 0.4000 - val_precision_5: 0.8889\n",
            "Epoch 127/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0147 - acc: 0.5000 - recall_5: 0.2302 - precision_5: 0.5853\n",
            "Epoch 127: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 1.0143 - acc: 0.5015 - recall_5: 0.2267 - precision_5: 0.5865 - val_loss: 1.5073 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6667\n",
            "Epoch 128/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0139 - acc: 0.5030 - recall_5: 0.2332 - precision_5: 0.5862\n",
            "Epoch 128: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 1.0156 - acc: 0.5000 - recall_5: 0.2340 - precision_5: 0.5855 - val_loss: 1.5040 - val_acc: 0.6250 - val_recall_5: 0.4250 - val_precision_5: 0.7727\n",
            "Epoch 129/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 1.0103 - acc: 0.4967 - recall_5: 0.2632 - precision_5: 0.5634\n",
            "Epoch 129: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0055 - acc: 0.5044 - recall_5: 0.2602 - precision_5: 0.5774 - val_loss: 1.4988 - val_acc: 0.6250 - val_recall_5: 0.4000 - val_precision_5: 0.7619\n",
            "Epoch 130/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0005 - acc: 0.5208 - recall_5: 0.2452 - precision_5: 0.6071\n",
            "Epoch 130: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0014 - acc: 0.5160 - recall_5: 0.2413 - precision_5: 0.6036 - val_loss: 1.5403 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 131/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0151 - acc: 0.5000 - recall_5: 0.2456 - precision_5: 0.5559\n",
            "Epoch 131: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0151 - acc: 0.5000 - recall_5: 0.2456 - precision_5: 0.5559 - val_loss: 1.6146 - val_acc: 0.6250 - val_recall_5: 0.2000 - val_precision_5: 0.5714\n",
            "Epoch 132/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0187 - acc: 0.5000 - recall_5: 0.2355 - precision_5: 0.5455\n",
            "Epoch 132: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0187 - acc: 0.5000 - recall_5: 0.2355 - precision_5: 0.5455 - val_loss: 1.4479 - val_acc: 0.6250 - val_recall_5: 0.2250 - val_precision_5: 0.6000\n",
            "Epoch 133/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0051 - acc: 0.4985 - recall_5: 0.2426 - precision_5: 0.5971\n",
            "Epoch 133: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0089 - acc: 0.4942 - recall_5: 0.2427 - precision_5: 0.5922 - val_loss: 1.5102 - val_acc: 0.6250 - val_recall_5: 0.4000 - val_precision_5: 0.8000\n",
            "Epoch 134/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0068 - acc: 0.5074 - recall_5: 0.2396 - precision_5: 0.5941\n",
            "Epoch 134: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 1.0065 - acc: 0.5087 - recall_5: 0.2398 - precision_5: 0.5978 - val_loss: 1.5121 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 135/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0094 - acc: 0.5128 - recall_5: 0.2580 - precision_5: 0.5730\n",
            "Epoch 135: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0090 - acc: 0.5102 - recall_5: 0.2631 - precision_5: 0.5801 - val_loss: 1.4874 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6923\n",
            "Epoch 136/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0042 - acc: 0.5058 - recall_5: 0.2631 - precision_5: 0.6074\n",
            "Epoch 136: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 1.0042 - acc: 0.5058 - recall_5: 0.2631 - precision_5: 0.6074 - val_loss: 1.4812 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6333\n",
            "Epoch 137/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0066 - acc: 0.4875 - recall_5: 0.2625 - precision_5: 0.5676\n",
            "Epoch 137: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0021 - acc: 0.4971 - recall_5: 0.2674 - precision_5: 0.5823 - val_loss: 1.5189 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6333\n",
            "Epoch 138/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0058 - acc: 0.5131 - recall_5: 0.2456 - precision_5: 0.5788\n",
            "Epoch 138: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0058 - acc: 0.5131 - recall_5: 0.2456 - precision_5: 0.5788 - val_loss: 1.4798 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6333\n",
            "Epoch 139/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0020 - acc: 0.5029 - recall_5: 0.2791 - precision_5: 0.6174\n",
            "Epoch 139: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0020 - acc: 0.5029 - recall_5: 0.2791 - precision_5: 0.6174 - val_loss: 1.5803 - val_acc: 0.6250 - val_recall_5: 0.4250 - val_precision_5: 0.6538\n",
            "Epoch 140/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0057 - acc: 0.5089 - recall_5: 0.2649 - precision_5: 0.5761\n",
            "Epoch 140: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 1.0067 - acc: 0.5044 - recall_5: 0.2616 - precision_5: 0.5714 - val_loss: 1.4719 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6667\n",
            "Epoch 141/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0152 - acc: 0.5015 - recall_5: 0.2398 - precision_5: 0.5978\n",
            "Epoch 141: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0152 - acc: 0.5015 - recall_5: 0.2398 - precision_5: 0.5978 - val_loss: 1.4660 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6786\n",
            "Epoch 142/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0075 - acc: 0.4855 - recall_5: 0.2660 - precision_5: 0.5980\n",
            "Epoch 142: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 1.0075 - acc: 0.4855 - recall_5: 0.2660 - precision_5: 0.5980 - val_loss: 1.5332 - val_acc: 0.6250 - val_recall_5: 0.4000 - val_precision_5: 0.8421\n",
            "Epoch 143/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 1.0024 - acc: 0.4922 - recall_5: 0.2172 - precision_5: 0.6096\n",
            "Epoch 143: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0025 - acc: 0.4956 - recall_5: 0.2267 - precision_5: 0.6118 - val_loss: 1.4729 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 144/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0019 - acc: 0.5030 - recall_5: 0.2902 - precision_5: 0.6113\n",
            "Epoch 144: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9982 - acc: 0.5073 - recall_5: 0.2936 - precision_5: 0.6140 - val_loss: 1.4802 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6333\n",
            "Epoch 145/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 1.0037 - acc: 0.5016 - recall_5: 0.2697 - precision_5: 0.6236\n",
            "Epoch 145: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9962 - acc: 0.5087 - recall_5: 0.2863 - precision_5: 0.6274 - val_loss: 1.4899 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 146/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0187 - acc: 0.4968 - recall_5: 0.2420 - precision_5: 0.5830\n",
            "Epoch 146: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0066 - acc: 0.4985 - recall_5: 0.2515 - precision_5: 0.5966 - val_loss: 1.4846 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6786\n",
            "Epoch 147/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0031 - acc: 0.5189 - recall_5: 0.2660 - precision_5: 0.5828\n",
            "Epoch 147: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0031 - acc: 0.5189 - recall_5: 0.2660 - precision_5: 0.5828 - val_loss: 1.4674 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6786\n",
            "Epoch 148/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 1.0132 - acc: 0.4893 - recall_5: 0.2774 - precision_5: 0.5652\n",
            "Epoch 148: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 1.0105 - acc: 0.4913 - recall_5: 0.2820 - precision_5: 0.5757 - val_loss: 1.5644 - val_acc: 0.6250 - val_recall_5: 0.4000 - val_precision_5: 0.7619\n",
            "Epoch 149/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0077 - acc: 0.4911 - recall_5: 0.2500 - precision_5: 0.6043\n",
            "Epoch 149: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0070 - acc: 0.4927 - recall_5: 0.2515 - precision_5: 0.6092 - val_loss: 1.5196 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 150/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 1.0008 - acc: 0.4970 - recall_5: 0.2619 - precision_5: 0.6027\n",
            "Epoch 150: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0015 - acc: 0.4956 - recall_5: 0.2631 - precision_5: 0.6033 - val_loss: 1.4849 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 151/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9892 - acc: 0.5141 - recall_5: 0.2781 - precision_5: 0.6224\n",
            "Epoch 151: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9932 - acc: 0.5116 - recall_5: 0.2747 - precision_5: 0.6217 - val_loss: 1.6035 - val_acc: 0.6250 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 152/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 0.9912 - acc: 0.5280 - recall_5: 0.2878 - precision_5: 0.6295\n",
            "Epoch 152: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9963 - acc: 0.5203 - recall_5: 0.2863 - precision_5: 0.6294 - val_loss: 1.5020 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6923\n",
            "Epoch 153/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 0.9951 - acc: 0.5033 - recall_5: 0.2615 - precision_5: 0.5846\n",
            "Epoch 153: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9970 - acc: 0.4985 - recall_5: 0.2645 - precision_5: 0.5833 - val_loss: 1.5364 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 154/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9909 - acc: 0.5244 - recall_5: 0.2988 - precision_5: 0.6183\n",
            "Epoch 154: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9918 - acc: 0.5203 - recall_5: 0.2936 - precision_5: 0.6196 - val_loss: 1.6133 - val_acc: 0.6250 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 155/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9970 - acc: 0.5094 - recall_5: 0.2500 - precision_5: 0.5926\n",
            "Epoch 155: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9995 - acc: 0.5044 - recall_5: 0.2500 - precision_5: 0.5890 - val_loss: 1.5771 - val_acc: 0.6250 - val_recall_5: 0.4250 - val_precision_5: 0.6538\n",
            "Epoch 156/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0072 - acc: 0.5015 - recall_5: 0.2558 - precision_5: 0.5809\n",
            "Epoch 156: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 1.0072 - acc: 0.5015 - recall_5: 0.2558 - precision_5: 0.5809 - val_loss: 1.5710 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6207\n",
            "Epoch 157/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0036 - acc: 0.4956 - recall_5: 0.2689 - precision_5: 0.6106\n",
            "Epoch 157: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 1.0036 - acc: 0.4956 - recall_5: 0.2689 - precision_5: 0.6106 - val_loss: 1.5388 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 158/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9885 - acc: 0.5032 - recall_5: 0.2837 - precision_5: 0.5880\n",
            "Epoch 158: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9952 - acc: 0.4985 - recall_5: 0.2747 - precision_5: 0.5833 - val_loss: 1.5930 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.7200\n",
            "Epoch 159/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9995 - acc: 0.5091 - recall_5: 0.2744 - precision_5: 0.6000\n",
            "Epoch 159: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0002 - acc: 0.5073 - recall_5: 0.2703 - precision_5: 0.6019 - val_loss: 1.5517 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 160/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 1.0010 - acc: 0.5058 - recall_5: 0.2805 - precision_5: 0.6050\n",
            "Epoch 160: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 1.0010 - acc: 0.5058 - recall_5: 0.2805 - precision_5: 0.6050 - val_loss: 1.5960 - val_acc: 0.6250 - val_recall_5: 0.4250 - val_precision_5: 0.6296\n",
            "Epoch 161/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9916 - acc: 0.4942 - recall_5: 0.2791 - precision_5: 0.6000\n",
            "Epoch 161: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9916 - acc: 0.4942 - recall_5: 0.2791 - precision_5: 0.6000 - val_loss: 1.5715 - val_acc: 0.6250 - val_recall_5: 0.4000 - val_precision_5: 0.7619\n",
            "Epoch 162/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9913 - acc: 0.5183 - recall_5: 0.2744 - precision_5: 0.6360\n",
            "Epoch 162: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9924 - acc: 0.5160 - recall_5: 0.2733 - precision_5: 0.6351 - val_loss: 1.5499 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 163/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9928 - acc: 0.5102 - recall_5: 0.2762 - precision_5: 0.5846\n",
            "Epoch 163: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9928 - acc: 0.5102 - recall_5: 0.2762 - precision_5: 0.5846 - val_loss: 1.5349 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6333\n",
            "Epoch 164/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 0.9844 - acc: 0.5296 - recall_5: 0.2944 - precision_5: 0.6259\n",
            "Epoch 164: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9909 - acc: 0.5305 - recall_5: 0.2892 - precision_5: 0.6142 - val_loss: 1.4881 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 165/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 1.0049 - acc: 0.5096 - recall_5: 0.2724 - precision_5: 0.5986\n",
            "Epoch 165: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1.0041 - acc: 0.5087 - recall_5: 0.2718 - precision_5: 0.5955 - val_loss: 1.5514 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 166/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 0.9856 - acc: 0.5132 - recall_5: 0.2763 - precision_5: 0.6022\n",
            "Epoch 166: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9859 - acc: 0.5189 - recall_5: 0.2747 - precision_5: 0.5981 - val_loss: 1.6437 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6429\n",
            "Epoch 167/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9987 - acc: 0.5192 - recall_5: 0.2436 - precision_5: 0.5846\n",
            "Epoch 167: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9938 - acc: 0.5247 - recall_5: 0.2529 - precision_5: 0.5979 - val_loss: 1.4911 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 168/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9951 - acc: 0.5238 - recall_5: 0.2679 - precision_5: 0.5882\n",
            "Epoch 168: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9920 - acc: 0.5291 - recall_5: 0.2703 - precision_5: 0.5924 - val_loss: 1.5234 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 169/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9886 - acc: 0.5089 - recall_5: 0.2753 - precision_5: 0.6250\n",
            "Epoch 169: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9877 - acc: 0.5087 - recall_5: 0.2776 - precision_5: 0.6221 - val_loss: 1.4731 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 170/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9975 - acc: 0.5091 - recall_5: 0.2576 - precision_5: 0.6123\n",
            "Epoch 170: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9905 - acc: 0.5160 - recall_5: 0.2631 - precision_5: 0.6220 - val_loss: 1.5041 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 171/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9846 - acc: 0.5060 - recall_5: 0.3051 - precision_5: 0.6156\n",
            "Epoch 171: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9862 - acc: 0.5058 - recall_5: 0.3009 - precision_5: 0.6106 - val_loss: 1.5865 - val_acc: 0.6250 - val_recall_5: 0.4250 - val_precision_5: 0.6538\n",
            "Epoch 172/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9893 - acc: 0.5045 - recall_5: 0.2946 - precision_5: 0.6367\n",
            "Epoch 172: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9894 - acc: 0.5044 - recall_5: 0.2907 - precision_5: 0.6309 - val_loss: 1.5034 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 173/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9908 - acc: 0.5244 - recall_5: 0.2896 - precision_5: 0.5882\n",
            "Epoch 173: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9928 - acc: 0.5203 - recall_5: 0.2907 - precision_5: 0.5917 - val_loss: 1.4877 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6786\n",
            "Epoch 174/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9847 - acc: 0.5000 - recall_5: 0.2750 - precision_5: 0.6132\n",
            "Epoch 174: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9794 - acc: 0.5058 - recall_5: 0.2791 - precision_5: 0.6115 - val_loss: 1.6336 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 175/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9867 - acc: 0.5174 - recall_5: 0.3052 - precision_5: 0.5882\n",
            "Epoch 175: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9867 - acc: 0.5174 - recall_5: 0.3052 - precision_5: 0.5882 - val_loss: 1.4895 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6667\n",
            "Epoch 176/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9871 - acc: 0.5102 - recall_5: 0.2863 - precision_5: 0.6156\n",
            "Epoch 176: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9871 - acc: 0.5102 - recall_5: 0.2863 - precision_5: 0.6156 - val_loss: 1.5326 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 177/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9764 - acc: 0.5312 - recall_5: 0.2857 - precision_5: 0.6174\n",
            "Epoch 177: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9720 - acc: 0.5363 - recall_5: 0.2863 - precision_5: 0.6234 - val_loss: 1.5028 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 178/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9826 - acc: 0.5031 - recall_5: 0.3125 - precision_5: 0.6192\n",
            "Epoch 178: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9817 - acc: 0.5029 - recall_5: 0.3125 - precision_5: 0.6250 - val_loss: 1.5903 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 179/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9872 - acc: 0.5351 - recall_5: 0.2698 - precision_5: 0.6413\n",
            "Epoch 179: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9807 - acc: 0.5407 - recall_5: 0.2791 - precision_5: 0.6486 - val_loss: 1.5166 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 180/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 0.9708 - acc: 0.5230 - recall_5: 0.3092 - precision_5: 0.6164\n",
            "Epoch 180: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9824 - acc: 0.5131 - recall_5: 0.2980 - precision_5: 0.6156 - val_loss: 1.5718 - val_acc: 0.6250 - val_recall_5: 0.3500 - val_precision_5: 0.8235\n",
            "Epoch 181/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9875 - acc: 0.5172 - recall_5: 0.2594 - precision_5: 0.6264\n",
            "Epoch 181: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9850 - acc: 0.5203 - recall_5: 0.2631 - precision_5: 0.6241 - val_loss: 1.5295 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 182/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9862 - acc: 0.5145 - recall_5: 0.2849 - precision_5: 0.6203\n",
            "Epoch 182: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9862 - acc: 0.5145 - recall_5: 0.2849 - precision_5: 0.6203 - val_loss: 1.4987 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 183/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9800 - acc: 0.5183 - recall_5: 0.2713 - precision_5: 0.6380\n",
            "Epoch 183: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9826 - acc: 0.5131 - recall_5: 0.2718 - precision_5: 0.6339 - val_loss: 1.4755 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 184/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9854 - acc: 0.5029 - recall_5: 0.2892 - precision_5: 0.6012\n",
            "Epoch 184: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9854 - acc: 0.5029 - recall_5: 0.2892 - precision_5: 0.6012 - val_loss: 1.5115 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 185/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9818 - acc: 0.4926 - recall_5: 0.2783 - precision_5: 0.6151\n",
            "Epoch 185: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9830 - acc: 0.4927 - recall_5: 0.2776 - precision_5: 0.6122 - val_loss: 1.6833 - val_acc: 0.6000 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 186/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9839 - acc: 0.4955 - recall_5: 0.3125 - precision_5: 0.6287\n",
            "Epoch 186: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9882 - acc: 0.4942 - recall_5: 0.3096 - precision_5: 0.6210 - val_loss: 1.5650 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 187/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9962 - acc: 0.5032 - recall_5: 0.2853 - precision_5: 0.6117\n",
            "Epoch 187: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9913 - acc: 0.5044 - recall_5: 0.2820 - precision_5: 0.6139 - val_loss: 1.5566 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 188/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9907 - acc: 0.5125 - recall_5: 0.2781 - precision_5: 0.5973\n",
            "Epoch 188: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9879 - acc: 0.5131 - recall_5: 0.2820 - precision_5: 0.5988 - val_loss: 1.5110 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 189/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 0.9814 - acc: 0.5247 - recall_5: 0.2648 - precision_5: 0.5771\n",
            "Epoch 189: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9831 - acc: 0.5233 - recall_5: 0.2645 - precision_5: 0.5796 - val_loss: 1.4956 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 190/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9897 - acc: 0.5104 - recall_5: 0.2827 - precision_5: 0.6070\n",
            "Epoch 190: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9866 - acc: 0.5102 - recall_5: 0.2849 - precision_5: 0.6106 - val_loss: 1.5193 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 191/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9859 - acc: 0.5134 - recall_5: 0.2798 - precision_5: 0.5968\n",
            "Epoch 191: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9875 - acc: 0.5087 - recall_5: 0.2747 - precision_5: 0.5962 - val_loss: 1.5741 - val_acc: 0.6250 - val_recall_5: 0.4250 - val_precision_5: 0.6538\n",
            "Epoch 192/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9705 - acc: 0.5320 - recall_5: 0.3003 - precision_5: 0.6545\n",
            "Epoch 192: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9752 - acc: 0.5247 - recall_5: 0.2951 - precision_5: 0.6548 - val_loss: 1.5000 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 193/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9720 - acc: 0.5164 - recall_5: 0.3006 - precision_5: 0.6313\n",
            "Epoch 193: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9718 - acc: 0.5145 - recall_5: 0.3009 - precision_5: 0.6292 - val_loss: 1.6162 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6207\n",
            "Epoch 194/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9747 - acc: 0.5335 - recall_5: 0.3064 - precision_5: 0.6166\n",
            "Epoch 194: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9800 - acc: 0.5291 - recall_5: 0.3009 - precision_5: 0.6142 - val_loss: 1.5741 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6429\n",
            "Epoch 195/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9749 - acc: 0.5375 - recall_5: 0.2719 - precision_5: 0.6084\n",
            "Epoch 195: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9731 - acc: 0.5363 - recall_5: 0.2776 - precision_5: 0.6161 - val_loss: 1.5006 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 196/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9736 - acc: 0.5363 - recall_5: 0.2936 - precision_5: 0.6066\n",
            "Epoch 196: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9736 - acc: 0.5363 - recall_5: 0.2936 - precision_5: 0.6066 - val_loss: 1.5906 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 197/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9878 - acc: 0.5096 - recall_5: 0.2676 - precision_5: 0.6007\n",
            "Epoch 197: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9791 - acc: 0.5189 - recall_5: 0.2762 - precision_5: 0.6109 - val_loss: 1.5406 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 198/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9726 - acc: 0.5145 - recall_5: 0.3081 - precision_5: 0.6040\n",
            "Epoch 198: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9726 - acc: 0.5145 - recall_5: 0.3081 - precision_5: 0.6040 - val_loss: 1.5081 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5758\n",
            "Epoch 199/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9735 - acc: 0.5193 - recall_5: 0.2961 - precision_5: 0.6086\n",
            "Epoch 199: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9736 - acc: 0.5203 - recall_5: 0.2965 - precision_5: 0.6108 - val_loss: 1.5166 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6333\n",
            "Epoch 200/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9631 - acc: 0.5349 - recall_5: 0.3009 - precision_5: 0.6509\n",
            "Epoch 200: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9631 - acc: 0.5349 - recall_5: 0.3009 - precision_5: 0.6509 - val_loss: 1.5805 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 201/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9647 - acc: 0.5253 - recall_5: 0.3036 - precision_5: 0.6182\n",
            "Epoch 201: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9637 - acc: 0.5276 - recall_5: 0.3023 - precision_5: 0.6228 - val_loss: 1.5600 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 202/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 0.9671 - acc: 0.5280 - recall_5: 0.2928 - precision_5: 0.6357\n",
            "Epoch 202: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9757 - acc: 0.5145 - recall_5: 0.2878 - precision_5: 0.6226 - val_loss: 1.5924 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 203/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9611 - acc: 0.5422 - recall_5: 0.3096 - precision_5: 0.6320\n",
            "Epoch 203: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9611 - acc: 0.5422 - recall_5: 0.3096 - precision_5: 0.6320 - val_loss: 1.5593 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6000\n",
            "Epoch 204/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9809 - acc: 0.5203 - recall_5: 0.2766 - precision_5: 0.6344\n",
            "Epoch 204: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9764 - acc: 0.5262 - recall_5: 0.2791 - precision_5: 0.6421 - val_loss: 1.5562 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 205/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9640 - acc: 0.5334 - recall_5: 0.3009 - precision_5: 0.5897\n",
            "Epoch 205: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9640 - acc: 0.5334 - recall_5: 0.3009 - precision_5: 0.5897 - val_loss: 1.5345 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6429\n",
            "Epoch 206/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9748 - acc: 0.5372 - recall_5: 0.2887 - precision_5: 0.6382\n",
            "Epoch 206: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9767 - acc: 0.5334 - recall_5: 0.2878 - precision_5: 0.6326 - val_loss: 1.4533 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6207\n",
            "Epoch 207/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9638 - acc: 0.5422 - recall_5: 0.2922 - precision_5: 0.6262\n",
            "Epoch 207: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9638 - acc: 0.5422 - recall_5: 0.2922 - precision_5: 0.6262 - val_loss: 1.5608 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 208/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9713 - acc: 0.5291 - recall_5: 0.2922 - precision_5: 0.5947\n",
            "Epoch 208: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9713 - acc: 0.5291 - recall_5: 0.2922 - precision_5: 0.5947 - val_loss: 1.5369 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 209/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9621 - acc: 0.5381 - recall_5: 0.3262 - precision_5: 0.6276\n",
            "Epoch 209: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9604 - acc: 0.5334 - recall_5: 0.3256 - precision_5: 0.6328 - val_loss: 1.5453 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5758\n",
            "Epoch 210/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9679 - acc: 0.5349 - recall_5: 0.2980 - precision_5: 0.6347\n",
            "Epoch 210: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9679 - acc: 0.5349 - recall_5: 0.2980 - precision_5: 0.6347 - val_loss: 1.5854 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 211/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9770 - acc: 0.5342 - recall_5: 0.2917 - precision_5: 0.6144\n",
            "Epoch 211: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9771 - acc: 0.5363 - recall_5: 0.2878 - precision_5: 0.6149 - val_loss: 1.5575 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 212/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9624 - acc: 0.5417 - recall_5: 0.2917 - precision_5: 0.6298\n",
            "Epoch 212: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9638 - acc: 0.5451 - recall_5: 0.2951 - precision_5: 0.6285 - val_loss: 1.5505 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 213/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9648 - acc: 0.5432 - recall_5: 0.3229 - precision_5: 0.6236\n",
            "Epoch 213: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9675 - acc: 0.5392 - recall_5: 0.3169 - precision_5: 0.6176 - val_loss: 1.5595 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 214/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9641 - acc: 0.5176 - recall_5: 0.2885 - precision_5: 0.6316\n",
            "Epoch 214: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9596 - acc: 0.5218 - recall_5: 0.2922 - precision_5: 0.6381 - val_loss: 1.6509 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 215/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9616 - acc: 0.5387 - recall_5: 0.2991 - precision_5: 0.6341\n",
            "Epoch 215: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9589 - acc: 0.5436 - recall_5: 0.2994 - precision_5: 0.6358 - val_loss: 1.5593 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 216/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9668 - acc: 0.5134 - recall_5: 0.3095 - precision_5: 0.5892\n",
            "Epoch 216: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9677 - acc: 0.5102 - recall_5: 0.3096 - precision_5: 0.5868 - val_loss: 1.6722 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6923\n",
            "Epoch 217/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9545 - acc: 0.5396 - recall_5: 0.3095 - precision_5: 0.6114\n",
            "Epoch 217: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9585 - acc: 0.5378 - recall_5: 0.3125 - precision_5: 0.6143 - val_loss: 1.5679 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 218/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9681 - acc: 0.5281 - recall_5: 0.3031 - precision_5: 0.6510\n",
            "Epoch 218: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9651 - acc: 0.5276 - recall_5: 0.3096 - precision_5: 0.6534 - val_loss: 1.6556 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 219/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9552 - acc: 0.5290 - recall_5: 0.3415 - precision_5: 0.6154\n",
            "Epoch 219: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9590 - acc: 0.5247 - recall_5: 0.3314 - precision_5: 0.6096 - val_loss: 1.6361 - val_acc: 0.6250 - val_recall_5: 0.3500 - val_precision_5: 0.9333\n",
            "Epoch 220/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9559 - acc: 0.5381 - recall_5: 0.2942 - precision_5: 0.6520\n",
            "Epoch 220: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9586 - acc: 0.5407 - recall_5: 0.2965 - precision_5: 0.6497 - val_loss: 1.5893 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 221/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9652 - acc: 0.5437 - recall_5: 0.3094 - precision_5: 0.6326\n",
            "Epoch 221: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9672 - acc: 0.5451 - recall_5: 0.3052 - precision_5: 0.6306 - val_loss: 1.5706 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 222/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9487 - acc: 0.5536 - recall_5: 0.3274 - precision_5: 0.6395\n",
            "Epoch 222: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9471 - acc: 0.5538 - recall_5: 0.3285 - precision_5: 0.6439 - val_loss: 1.5517 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 223/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9464 - acc: 0.5578 - recall_5: 0.3406 - precision_5: 0.6606\n",
            "Epoch 223: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9532 - acc: 0.5494 - recall_5: 0.3314 - precision_5: 0.6404 - val_loss: 1.5987 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.5806\n",
            "Epoch 224/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9657 - acc: 0.5233 - recall_5: 0.3081 - precision_5: 0.6092\n",
            "Epoch 224: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9657 - acc: 0.5233 - recall_5: 0.3081 - precision_5: 0.6092 - val_loss: 1.6053 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6207\n",
            "Epoch 225/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9711 - acc: 0.5353 - recall_5: 0.2917 - precision_5: 0.6169\n",
            "Epoch 225: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9699 - acc: 0.5363 - recall_5: 0.2922 - precision_5: 0.6128 - val_loss: 1.5683 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5758\n",
            "Epoch 226/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9652 - acc: 0.5193 - recall_5: 0.3110 - precision_5: 0.6239\n",
            "Epoch 226: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9671 - acc: 0.5174 - recall_5: 0.3096 - precision_5: 0.6228 - val_loss: 1.5218 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 227/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9607 - acc: 0.5402 - recall_5: 0.2946 - precision_5: 0.6130\n",
            "Epoch 227: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9599 - acc: 0.5407 - recall_5: 0.2965 - precision_5: 0.6126 - val_loss: 1.4993 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 228/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9535 - acc: 0.5417 - recall_5: 0.3214 - precision_5: 0.6207\n",
            "Epoch 228: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9515 - acc: 0.5451 - recall_5: 0.3256 - precision_5: 0.6257 - val_loss: 1.5710 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 229/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9436 - acc: 0.5446 - recall_5: 0.3512 - precision_5: 0.6260\n",
            "Epoch 229: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9469 - acc: 0.5422 - recall_5: 0.3488 - precision_5: 0.6234 - val_loss: 1.5741 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 230/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9662 - acc: 0.5238 - recall_5: 0.2976 - precision_5: 0.6042\n",
            "Epoch 230: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9651 - acc: 0.5233 - recall_5: 0.2965 - precision_5: 0.6053 - val_loss: 1.5770 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 231/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9616 - acc: 0.5078 - recall_5: 0.2875 - precision_5: 0.6013\n",
            "Epoch 231: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9540 - acc: 0.5174 - recall_5: 0.2936 - precision_5: 0.6159 - val_loss: 1.5814 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 232/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9712 - acc: 0.5518 - recall_5: 0.2912 - precision_5: 0.6262\n",
            "Epoch 232: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9669 - acc: 0.5494 - recall_5: 0.2980 - precision_5: 0.6288 - val_loss: 1.5336 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 233/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9503 - acc: 0.5469 - recall_5: 0.3391 - precision_5: 0.6327\n",
            "Epoch 233: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9513 - acc: 0.5422 - recall_5: 0.3372 - precision_5: 0.6253 - val_loss: 1.5975 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5758\n",
            "Epoch 234/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9544 - acc: 0.5387 - recall_5: 0.3304 - precision_5: 0.6325\n",
            "Epoch 234: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9534 - acc: 0.5407 - recall_5: 0.3285 - precision_5: 0.6313 - val_loss: 1.5182 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 235/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9518 - acc: 0.5297 - recall_5: 0.3187 - precision_5: 0.6518\n",
            "Epoch 235: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9577 - acc: 0.5218 - recall_5: 0.3154 - precision_5: 0.6327 - val_loss: 1.5584 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 236/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9533 - acc: 0.5453 - recall_5: 0.3156 - precision_5: 0.6273\n",
            "Epoch 236: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9633 - acc: 0.5349 - recall_5: 0.3081 - precision_5: 0.6145 - val_loss: 1.5431 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 237/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9505 - acc: 0.5385 - recall_5: 0.3125 - precision_5: 0.6352\n",
            "Epoch 237: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9516 - acc: 0.5349 - recall_5: 0.3081 - precision_5: 0.6310 - val_loss: 1.5989 - val_acc: 0.6000 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 238/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9575 - acc: 0.5253 - recall_5: 0.3274 - precision_5: 0.6322\n",
            "Epoch 238: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9592 - acc: 0.5233 - recall_5: 0.3241 - precision_5: 0.6246 - val_loss: 1.5857 - val_acc: 0.5500 - val_recall_5: 0.4250 - val_precision_5: 0.7083\n",
            "Epoch 239/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9584 - acc: 0.5351 - recall_5: 0.3003 - precision_5: 0.6234\n",
            "Epoch 239: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9518 - acc: 0.5422 - recall_5: 0.3023 - precision_5: 0.6265 - val_loss: 1.6204 - val_acc: 0.5750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 240/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9538 - acc: 0.5305 - recall_5: 0.3567 - precision_5: 0.6158\n",
            "Epoch 240: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9512 - acc: 0.5320 - recall_5: 0.3532 - precision_5: 0.6183 - val_loss: 1.5356 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 241/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9459 - acc: 0.5513 - recall_5: 0.3269 - precision_5: 0.6335\n",
            "Epoch 241: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9481 - acc: 0.5509 - recall_5: 0.3256 - precision_5: 0.6310 - val_loss: 1.6440 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 242/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9432 - acc: 0.5469 - recall_5: 0.3281 - precision_5: 0.6000\n",
            "Epoch 242: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9484 - acc: 0.5422 - recall_5: 0.3198 - precision_5: 0.5930 - val_loss: 1.5936 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6923\n",
            "Epoch 243/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9535 - acc: 0.5385 - recall_5: 0.3157 - precision_5: 0.6156\n",
            "Epoch 243: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9471 - acc: 0.5480 - recall_5: 0.3183 - precision_5: 0.6204 - val_loss: 1.6084 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 244/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9650 - acc: 0.5359 - recall_5: 0.3187 - precision_5: 0.6220\n",
            "Epoch 244: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9621 - acc: 0.5349 - recall_5: 0.3183 - precision_5: 0.6239 - val_loss: 1.5341 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 245/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9517 - acc: 0.5595 - recall_5: 0.3430 - precision_5: 0.6522\n",
            "Epoch 245: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9483 - acc: 0.5581 - recall_5: 0.3445 - precision_5: 0.6529 - val_loss: 1.5492 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 246/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9441 - acc: 0.5721 - recall_5: 0.3590 - precision_5: 0.6588\n",
            "Epoch 246: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9515 - acc: 0.5567 - recall_5: 0.3576 - precision_5: 0.6508 - val_loss: 1.5936 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 247/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9441 - acc: 0.5480 - recall_5: 0.3285 - precision_5: 0.6313\n",
            "Epoch 247: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9441 - acc: 0.5480 - recall_5: 0.3285 - precision_5: 0.6313 - val_loss: 1.6032 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 248/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9362 - acc: 0.5484 - recall_5: 0.3641 - precision_5: 0.6247\n",
            "Epoch 248: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9442 - acc: 0.5451 - recall_5: 0.3547 - precision_5: 0.6162 - val_loss: 1.5224 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 249/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9409 - acc: 0.5688 - recall_5: 0.3438 - precision_5: 0.6509\n",
            "Epoch 249: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9514 - acc: 0.5625 - recall_5: 0.3372 - precision_5: 0.6374 - val_loss: 1.5760 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 250/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9472 - acc: 0.5375 - recall_5: 0.3328 - precision_5: 0.6283\n",
            "Epoch 250: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9441 - acc: 0.5407 - recall_5: 0.3299 - precision_5: 0.6271 - val_loss: 1.7347 - val_acc: 0.6250 - val_recall_5: 0.4250 - val_precision_5: 0.8947\n",
            "Epoch 251/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9535 - acc: 0.5256 - recall_5: 0.3269 - precision_5: 0.6145\n",
            "Epoch 251: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9536 - acc: 0.5305 - recall_5: 0.3285 - precision_5: 0.6125 - val_loss: 1.5843 - val_acc: 0.6000 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 252/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 0.9350 - acc: 0.5526 - recall_5: 0.3487 - precision_5: 0.6310\n",
            "Epoch 252: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9407 - acc: 0.5451 - recall_5: 0.3445 - precision_5: 0.6220 - val_loss: 1.6229 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 253/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9422 - acc: 0.5469 - recall_5: 0.3516 - precision_5: 0.6320\n",
            "Epoch 253: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9406 - acc: 0.5465 - recall_5: 0.3503 - precision_5: 0.6359 - val_loss: 1.5540 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 254/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9558 - acc: 0.5402 - recall_5: 0.3408 - precision_5: 0.6291\n",
            "Epoch 254: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 11ms/step - loss: 0.9532 - acc: 0.5436 - recall_5: 0.3401 - precision_5: 0.6307 - val_loss: 1.5851 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 255/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9458 - acc: 0.5427 - recall_5: 0.3140 - precision_5: 0.6224\n",
            "Epoch 255: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9417 - acc: 0.5451 - recall_5: 0.3212 - precision_5: 0.6296 - val_loss: 1.7006 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.5625\n",
            "Epoch 256/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9374 - acc: 0.5565 - recall_5: 0.3363 - precision_5: 0.6278\n",
            "Epoch 256: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9369 - acc: 0.5552 - recall_5: 0.3343 - precision_5: 0.6233 - val_loss: 1.6198 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6000\n",
            "Epoch 257/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9416 - acc: 0.5712 - recall_5: 0.3430 - precision_5: 0.6277\n",
            "Epoch 257: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9416 - acc: 0.5712 - recall_5: 0.3430 - precision_5: 0.6277 - val_loss: 1.6285 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6207\n",
            "Epoch 258/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9450 - acc: 0.5357 - recall_5: 0.3601 - precision_5: 0.6488\n",
            "Epoch 258: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9454 - acc: 0.5334 - recall_5: 0.3576 - precision_5: 0.6457 - val_loss: 1.6743 - val_acc: 0.5500 - val_recall_5: 0.4500 - val_precision_5: 0.6000\n",
            "Epoch 259/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9316 - acc: 0.5698 - recall_5: 0.3547 - precision_5: 0.6455\n",
            "Epoch 259: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9316 - acc: 0.5698 - recall_5: 0.3547 - precision_5: 0.6455 - val_loss: 1.5985 - val_acc: 0.6000 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 260/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9234 - acc: 0.5473 - recall_5: 0.3598 - precision_5: 0.6501\n",
            "Epoch 260: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9277 - acc: 0.5480 - recall_5: 0.3532 - precision_5: 0.6429 - val_loss: 1.6644 - val_acc: 0.6000 - val_recall_5: 0.4500 - val_precision_5: 0.5625\n",
            "Epoch 261/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9530 - acc: 0.5580 - recall_5: 0.3304 - precision_5: 0.6099\n",
            "Epoch 261: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9518 - acc: 0.5581 - recall_5: 0.3314 - precision_5: 0.6129 - val_loss: 1.6082 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 262/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9491 - acc: 0.5381 - recall_5: 0.3354 - precision_5: 0.6268\n",
            "Epoch 262: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9466 - acc: 0.5451 - recall_5: 0.3387 - precision_5: 0.6263 - val_loss: 1.5669 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 263/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9295 - acc: 0.5564 - recall_5: 0.3491 - precision_5: 0.6240\n",
            "Epoch 263: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9306 - acc: 0.5552 - recall_5: 0.3488 - precision_5: 0.6316 - val_loss: 1.6413 - val_acc: 0.5750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 264/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9344 - acc: 0.5610 - recall_5: 0.3561 - precision_5: 0.6266\n",
            "Epoch 264: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9344 - acc: 0.5610 - recall_5: 0.3561 - precision_5: 0.6266 - val_loss: 1.6126 - val_acc: 0.5750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 265/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9327 - acc: 0.5580 - recall_5: 0.3557 - precision_5: 0.6390\n",
            "Epoch 265: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9351 - acc: 0.5552 - recall_5: 0.3517 - precision_5: 0.6368 - val_loss: 1.5908 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 266/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9416 - acc: 0.5491 - recall_5: 0.3333 - precision_5: 0.6104\n",
            "Epoch 266: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9462 - acc: 0.5465 - recall_5: 0.3328 - precision_5: 0.6042 - val_loss: 1.6111 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 267/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9347 - acc: 0.5609 - recall_5: 0.3494 - precision_5: 0.6246\n",
            "Epoch 267: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9356 - acc: 0.5581 - recall_5: 0.3401 - precision_5: 0.6190 - val_loss: 1.6044 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 268/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9437 - acc: 0.5297 - recall_5: 0.3422 - precision_5: 0.6083\n",
            "Epoch 268: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9403 - acc: 0.5378 - recall_5: 0.3459 - precision_5: 0.6134 - val_loss: 1.6133 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 269/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9298 - acc: 0.5506 - recall_5: 0.3586 - precision_5: 0.6309\n",
            "Epoch 269: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9319 - acc: 0.5480 - recall_5: 0.3561 - precision_5: 0.6266 - val_loss: 1.7519 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 270/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9360 - acc: 0.5578 - recall_5: 0.3562 - precision_5: 0.6423\n",
            "Epoch 270: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9434 - acc: 0.5538 - recall_5: 0.3517 - precision_5: 0.6368 - val_loss: 1.6544 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 271/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9216 - acc: 0.5580 - recall_5: 0.3676 - precision_5: 0.6552\n",
            "Epoch 271: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9266 - acc: 0.5552 - recall_5: 0.3692 - precision_5: 0.6480 - val_loss: 1.5946 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 272/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9420 - acc: 0.5686 - recall_5: 0.3476 - precision_5: 0.6048\n",
            "Epoch 272: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9473 - acc: 0.5640 - recall_5: 0.3459 - precision_5: 0.6025 - val_loss: 1.6276 - val_acc: 0.5000 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 273/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9229 - acc: 0.5549 - recall_5: 0.3537 - precision_5: 0.6374\n",
            "Epoch 273: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.9249 - acc: 0.5581 - recall_5: 0.3547 - precision_5: 0.6321 - val_loss: 1.6350 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 274/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9385 - acc: 0.5610 - recall_5: 0.3482 - precision_5: 0.6273\n",
            "Epoch 274: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.9435 - acc: 0.5552 - recall_5: 0.3474 - precision_5: 0.6257 - val_loss: 1.7154 - val_acc: 0.6250 - val_recall_5: 0.4500 - val_precision_5: 0.6429\n",
            "Epoch 275/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9350 - acc: 0.5494 - recall_5: 0.3517 - precision_5: 0.6576\n",
            "Epoch 275: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 17ms/step - loss: 0.9350 - acc: 0.5494 - recall_5: 0.3517 - precision_5: 0.6576 - val_loss: 1.6418 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 276/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9431 - acc: 0.5422 - recall_5: 0.3391 - precision_5: 0.6382\n",
            "Epoch 276: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 0.9359 - acc: 0.5436 - recall_5: 0.3430 - precision_5: 0.6431 - val_loss: 1.7227 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 277/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9434 - acc: 0.5412 - recall_5: 0.3491 - precision_5: 0.6189\n",
            "Epoch 277: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9404 - acc: 0.5436 - recall_5: 0.3503 - precision_5: 0.6211 - val_loss: 1.6272 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 278/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9349 - acc: 0.5594 - recall_5: 0.3313 - precision_5: 0.6328\n",
            "Epoch 278: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9352 - acc: 0.5581 - recall_5: 0.3328 - precision_5: 0.6361 - val_loss: 1.6747 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 279/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9233 - acc: 0.5577 - recall_5: 0.3654 - precision_5: 0.6423\n",
            "Epoch 279: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9279 - acc: 0.5567 - recall_5: 0.3663 - precision_5: 0.6380 - val_loss: 1.6253 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 280/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9375 - acc: 0.5640 - recall_5: 0.3689 - precision_5: 0.6402\n",
            "Epoch 280: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9373 - acc: 0.5640 - recall_5: 0.3677 - precision_5: 0.6357 - val_loss: 1.6647 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 281/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9382 - acc: 0.5491 - recall_5: 0.3482 - precision_5: 0.6142\n",
            "Epoch 281: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9349 - acc: 0.5523 - recall_5: 0.3532 - precision_5: 0.6183 - val_loss: 1.6649 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6207\n",
            "Epoch 282/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9369 - acc: 0.5671 - recall_5: 0.3308 - precision_5: 0.6439\n",
            "Epoch 282: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9326 - acc: 0.5698 - recall_5: 0.3343 - precision_5: 0.6497 - val_loss: 1.6354 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 283/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9362 - acc: 0.5500 - recall_5: 0.3531 - precision_5: 0.6313\n",
            "Epoch 283: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9341 - acc: 0.5538 - recall_5: 0.3532 - precision_5: 0.6295 - val_loss: 1.6381 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 284/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9462 - acc: 0.5473 - recall_5: 0.3643 - precision_5: 0.6257\n",
            "Epoch 284: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9388 - acc: 0.5567 - recall_5: 0.3663 - precision_5: 0.6316 - val_loss: 1.6077 - val_acc: 0.5250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 285/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9279 - acc: 0.5640 - recall_5: 0.3537 - precision_5: 0.6203\n",
            "Epoch 285: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9268 - acc: 0.5683 - recall_5: 0.3590 - precision_5: 0.6285 - val_loss: 1.5944 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 286/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9268 - acc: 0.5610 - recall_5: 0.3445 - precision_5: 0.6209\n",
            "Epoch 286: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9277 - acc: 0.5596 - recall_5: 0.3445 - precision_5: 0.6140 - val_loss: 1.7190 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 287/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9311 - acc: 0.5328 - recall_5: 0.3578 - precision_5: 0.6026\n",
            "Epoch 287: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9311 - acc: 0.5305 - recall_5: 0.3503 - precision_5: 0.6010 - val_loss: 1.7025 - val_acc: 0.5500 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 288/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9381 - acc: 0.5688 - recall_5: 0.3328 - precision_5: 0.6210\n",
            "Epoch 288: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9413 - acc: 0.5683 - recall_5: 0.3343 - precision_5: 0.6233 - val_loss: 1.6576 - val_acc: 0.5000 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 289/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9233 - acc: 0.5579 - recall_5: 0.3735 - precision_5: 0.6414\n",
            "Epoch 289: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9202 - acc: 0.5596 - recall_5: 0.3765 - precision_5: 0.6475 - val_loss: 1.6795 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 290/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9473 - acc: 0.5747 - recall_5: 0.3704 - precision_5: 0.6263\n",
            "Epoch 290: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9375 - acc: 0.5770 - recall_5: 0.3794 - precision_5: 0.6366 - val_loss: 1.6993 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 291/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9354 - acc: 0.5488 - recall_5: 0.3537 - precision_5: 0.6057\n",
            "Epoch 291: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9295 - acc: 0.5552 - recall_5: 0.3547 - precision_5: 0.6131 - val_loss: 1.7023 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 292/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9420 - acc: 0.5610 - recall_5: 0.3582 - precision_5: 0.6184\n",
            "Epoch 292: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9400 - acc: 0.5625 - recall_5: 0.3590 - precision_5: 0.6190 - val_loss: 1.6725 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 293/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9383 - acc: 0.5465 - recall_5: 0.3542 - precision_5: 0.6208\n",
            "Epoch 293: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 0.9309 - acc: 0.5523 - recall_5: 0.3605 - precision_5: 0.6294 - val_loss: 1.6828 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 294/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9441 - acc: 0.5274 - recall_5: 0.3369 - precision_5: 0.5847\n",
            "Epoch 294: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9437 - acc: 0.5291 - recall_5: 0.3299 - precision_5: 0.5835 - val_loss: 1.6712 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 295/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9364 - acc: 0.5625 - recall_5: 0.3811 - precision_5: 0.6562\n",
            "Epoch 295: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 0.9387 - acc: 0.5640 - recall_5: 0.3794 - precision_5: 0.6558 - val_loss: 1.6336 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 296/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9359 - acc: 0.5427 - recall_5: 0.3338 - precision_5: 0.6204\n",
            "Epoch 296: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9360 - acc: 0.5422 - recall_5: 0.3358 - precision_5: 0.6226 - val_loss: 1.6578 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5806\n",
            "Epoch 297/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9166 - acc: 0.5625 - recall_5: 0.3628 - precision_5: 0.6182\n",
            "Epoch 297: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9198 - acc: 0.5610 - recall_5: 0.3648 - precision_5: 0.6152 - val_loss: 1.6375 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 298/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9332 - acc: 0.5453 - recall_5: 0.3484 - precision_5: 0.6177\n",
            "Epoch 298: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9346 - acc: 0.5465 - recall_5: 0.3474 - precision_5: 0.6097 - val_loss: 1.6445 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 299/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9238 - acc: 0.5833 - recall_5: 0.3854 - precision_5: 0.6302\n",
            "Epoch 299: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9273 - acc: 0.5799 - recall_5: 0.3823 - precision_5: 0.6262 - val_loss: 1.6842 - val_acc: 0.5750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 300/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9284 - acc: 0.5595 - recall_5: 0.3613 - precision_5: 0.6188\n",
            "Epoch 300: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9289 - acc: 0.5581 - recall_5: 0.3590 - precision_5: 0.6160 - val_loss: 1.7000 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6538\n",
            "Epoch 301/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9258 - acc: 0.5732 - recall_5: 0.3674 - precision_5: 0.6531\n",
            "Epoch 301: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9300 - acc: 0.5683 - recall_5: 0.3619 - precision_5: 0.6484 - val_loss: 1.5801 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 302/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9307 - acc: 0.5808 - recall_5: 0.3613 - precision_5: 0.6423\n",
            "Epoch 302: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9389 - acc: 0.5727 - recall_5: 0.3619 - precision_5: 0.6352 - val_loss: 1.6505 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6429\n",
            "Epoch 303/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9157 - acc: 0.5509 - recall_5: 0.3474 - precision_5: 0.6373\n",
            "Epoch 303: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9157 - acc: 0.5509 - recall_5: 0.3474 - precision_5: 0.6373 - val_loss: 1.6525 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 304/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9206 - acc: 0.5481 - recall_5: 0.3750 - precision_5: 0.6376\n",
            "Epoch 304: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9275 - acc: 0.5436 - recall_5: 0.3677 - precision_5: 0.6278 - val_loss: 1.5938 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 305/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9139 - acc: 0.5673 - recall_5: 0.3686 - precision_5: 0.6354\n",
            "Epoch 305: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9103 - acc: 0.5698 - recall_5: 0.3750 - precision_5: 0.6434 - val_loss: 1.6843 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5806\n",
            "Epoch 306/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9123 - acc: 0.5655 - recall_5: 0.3659 - precision_5: 0.6218\n",
            "Epoch 306: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9156 - acc: 0.5625 - recall_5: 0.3634 - precision_5: 0.6234 - val_loss: 1.6618 - val_acc: 0.6250 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 307/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9335 - acc: 0.5473 - recall_5: 0.3780 - precision_5: 0.6216\n",
            "Epoch 307: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9288 - acc: 0.5538 - recall_5: 0.3779 - precision_5: 0.6235 - val_loss: 1.7166 - val_acc: 0.5750 - val_recall_5: 0.4500 - val_precision_5: 0.5455\n",
            "Epoch 308/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9055 - acc: 0.5769 - recall_5: 0.3878 - precision_5: 0.6722\n",
            "Epoch 308: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9145 - acc: 0.5698 - recall_5: 0.3852 - precision_5: 0.6592 - val_loss: 1.7152 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 309/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9279 - acc: 0.5610 - recall_5: 0.3567 - precision_5: 0.6359\n",
            "Epoch 309: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9314 - acc: 0.5581 - recall_5: 0.3561 - precision_5: 0.6364 - val_loss: 1.6083 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 310/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9149 - acc: 0.5640 - recall_5: 0.3795 - precision_5: 0.6250\n",
            "Epoch 310: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9149 - acc: 0.5654 - recall_5: 0.3794 - precision_5: 0.6259 - val_loss: 1.6376 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 311/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9190 - acc: 0.5774 - recall_5: 0.3586 - precision_5: 0.6260\n",
            "Epoch 311: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9194 - acc: 0.5756 - recall_5: 0.3605 - precision_5: 0.6263 - val_loss: 1.7317 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 312/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9308 - acc: 0.5469 - recall_5: 0.3578 - precision_5: 0.5964\n",
            "Epoch 312: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9256 - acc: 0.5465 - recall_5: 0.3590 - precision_5: 0.5995 - val_loss: 1.6241 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 313/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9149 - acc: 0.5564 - recall_5: 0.3674 - precision_5: 0.6195\n",
            "Epoch 313: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9130 - acc: 0.5581 - recall_5: 0.3663 - precision_5: 0.6238 - val_loss: 1.6943 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 314/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9198 - acc: 0.5793 - recall_5: 0.3704 - precision_5: 0.6312\n",
            "Epoch 314: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9149 - acc: 0.5814 - recall_5: 0.3692 - precision_5: 0.6287 - val_loss: 1.7168 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 315/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9176 - acc: 0.5625 - recall_5: 0.3779 - precision_5: 0.6280\n",
            "Epoch 315: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9176 - acc: 0.5625 - recall_5: 0.3779 - precision_5: 0.6280 - val_loss: 1.7612 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6538\n",
            "Epoch 316/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9205 - acc: 0.5670 - recall_5: 0.3527 - precision_5: 0.6270\n",
            "Epoch 316: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9170 - acc: 0.5727 - recall_5: 0.3547 - precision_5: 0.6305 - val_loss: 1.6487 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 317/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9398 - acc: 0.5266 - recall_5: 0.3719 - precision_5: 0.6263\n",
            "Epoch 317: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9379 - acc: 0.5291 - recall_5: 0.3721 - precision_5: 0.6275 - val_loss: 1.6749 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5625\n",
            "Epoch 318/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9122 - acc: 0.5610 - recall_5: 0.3537 - precision_5: 0.6480\n",
            "Epoch 318: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9135 - acc: 0.5640 - recall_5: 0.3590 - precision_5: 0.6483 - val_loss: 1.7980 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6538\n",
            "Epoch 319/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9157 - acc: 0.5497 - recall_5: 0.3862 - precision_5: 0.6567\n",
            "Epoch 319: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9199 - acc: 0.5494 - recall_5: 0.3837 - precision_5: 0.6439 - val_loss: 1.7003 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 320/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9048 - acc: 0.5595 - recall_5: 0.3948 - precision_5: 0.6411\n",
            "Epoch 320: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9102 - acc: 0.5552 - recall_5: 0.3852 - precision_5: 0.6325 - val_loss: 1.6232 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 321/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9315 - acc: 0.5595 - recall_5: 0.3527 - precision_5: 0.6077\n",
            "Epoch 321: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9293 - acc: 0.5640 - recall_5: 0.3532 - precision_5: 0.6106 - val_loss: 1.6500 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5806\n",
            "Epoch 322/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9228 - acc: 0.5781 - recall_5: 0.3531 - precision_5: 0.6175\n",
            "Epoch 322: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9159 - acc: 0.5785 - recall_5: 0.3619 - precision_5: 0.6241 - val_loss: 1.6972 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 323/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9224 - acc: 0.5701 - recall_5: 0.3765 - precision_5: 0.6237\n",
            "Epoch 323: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9259 - acc: 0.5698 - recall_5: 0.3750 - precision_5: 0.6217 - val_loss: 1.6774 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5806\n",
            "Epoch 324/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9145 - acc: 0.5594 - recall_5: 0.3875 - precision_5: 0.6200\n",
            "Epoch 324: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9199 - acc: 0.5581 - recall_5: 0.3823 - precision_5: 0.6188 - val_loss: 1.7010 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6000\n",
            "Epoch 325/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9104 - acc: 0.5672 - recall_5: 0.3578 - precision_5: 0.6415\n",
            "Epoch 325: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9107 - acc: 0.5654 - recall_5: 0.3648 - precision_5: 0.6403 - val_loss: 1.7315 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5806\n",
            "Epoch 326/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9332 - acc: 0.5595 - recall_5: 0.3720 - precision_5: 0.6404\n",
            "Epoch 326: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9297 - acc: 0.5581 - recall_5: 0.3721 - precision_5: 0.6448 - val_loss: 1.7205 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6207\n",
            "Epoch 327/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9169 - acc: 0.5581 - recall_5: 0.3881 - precision_5: 0.6342\n",
            "Epoch 327: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9169 - acc: 0.5581 - recall_5: 0.3881 - precision_5: 0.6342 - val_loss: 1.6316 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6296\n",
            "Epoch 328/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9099 - acc: 0.5750 - recall_5: 0.3688 - precision_5: 0.6574\n",
            "Epoch 328: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9148 - acc: 0.5698 - recall_5: 0.3634 - precision_5: 0.6443 - val_loss: 1.6985 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 329/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9155 - acc: 0.5719 - recall_5: 0.3656 - precision_5: 0.6307\n",
            "Epoch 329: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9170 - acc: 0.5683 - recall_5: 0.3648 - precision_5: 0.6275 - val_loss: 1.6193 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 330/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9379 - acc: 0.5453 - recall_5: 0.3359 - precision_5: 0.6178\n",
            "Epoch 330: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9389 - acc: 0.5392 - recall_5: 0.3387 - precision_5: 0.6115 - val_loss: 1.6828 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5455\n",
            "Epoch 331/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9168 - acc: 0.5552 - recall_5: 0.3808 - precision_5: 0.6375\n",
            "Epoch 331: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9168 - acc: 0.5552 - recall_5: 0.3808 - precision_5: 0.6375 - val_loss: 1.6806 - val_acc: 0.6000 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 332/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9064 - acc: 0.5670 - recall_5: 0.3839 - precision_5: 0.6402\n",
            "Epoch 332: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9120 - acc: 0.5654 - recall_5: 0.3823 - precision_5: 0.6368 - val_loss: 1.6802 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6207\n",
            "Epoch 333/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9281 - acc: 0.5469 - recall_5: 0.3656 - precision_5: 0.6190\n",
            "Epoch 333: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9243 - acc: 0.5465 - recall_5: 0.3721 - precision_5: 0.6229 - val_loss: 1.6636 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 334/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9103 - acc: 0.5561 - recall_5: 0.3750 - precision_5: 0.6500\n",
            "Epoch 334: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9118 - acc: 0.5567 - recall_5: 0.3765 - precision_5: 0.6557 - val_loss: 1.6306 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 335/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9263 - acc: 0.5640 - recall_5: 0.3628 - precision_5: 0.6247\n",
            "Epoch 335: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9205 - acc: 0.5654 - recall_5: 0.3677 - precision_5: 0.6294 - val_loss: 1.6226 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 336/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9131 - acc: 0.5721 - recall_5: 0.3878 - precision_5: 0.6205\n",
            "Epoch 336: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9117 - acc: 0.5683 - recall_5: 0.3866 - precision_5: 0.6157 - val_loss: 1.7160 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 337/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9063 - acc: 0.5686 - recall_5: 0.3735 - precision_5: 0.6397\n",
            "Epoch 337: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9153 - acc: 0.5625 - recall_5: 0.3706 - precision_5: 0.6296 - val_loss: 1.5905 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 338/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9146 - acc: 0.5625 - recall_5: 0.3766 - precision_5: 0.6267\n",
            "Epoch 338: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9185 - acc: 0.5610 - recall_5: 0.3750 - precision_5: 0.6277 - val_loss: 1.6220 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 339/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9060 - acc: 0.5732 - recall_5: 0.3780 - precision_5: 0.6492\n",
            "Epoch 339: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9158 - acc: 0.5669 - recall_5: 0.3779 - precision_5: 0.6373 - val_loss: 1.6158 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 340/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9228 - acc: 0.5699 - recall_5: 0.3571 - precision_5: 0.6400\n",
            "Epoch 340: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9224 - acc: 0.5712 - recall_5: 0.3561 - precision_5: 0.6347 - val_loss: 1.6642 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 341/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9200 - acc: 0.5427 - recall_5: 0.3521 - precision_5: 0.6294\n",
            "Epoch 341: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9163 - acc: 0.5480 - recall_5: 0.3605 - precision_5: 0.6327 - val_loss: 1.6412 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 342/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9062 - acc: 0.5500 - recall_5: 0.3938 - precision_5: 0.6269\n",
            "Epoch 342: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9126 - acc: 0.5451 - recall_5: 0.3852 - precision_5: 0.6134 - val_loss: 1.6566 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5758\n",
            "Epoch 343/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9050 - acc: 0.5596 - recall_5: 0.3881 - precision_5: 0.6297\n",
            "Epoch 343: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9050 - acc: 0.5596 - recall_5: 0.3881 - precision_5: 0.6297 - val_loss: 1.6303 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 344/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9114 - acc: 0.5625 - recall_5: 0.3910 - precision_5: 0.6455\n",
            "Epoch 344: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9113 - acc: 0.5625 - recall_5: 0.3983 - precision_5: 0.6524 - val_loss: 1.6621 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 345/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9354 - acc: 0.5672 - recall_5: 0.3672 - precision_5: 0.5995\n",
            "Epoch 345: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9225 - acc: 0.5785 - recall_5: 0.3735 - precision_5: 0.6105 - val_loss: 1.6741 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 346/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9249 - acc: 0.5688 - recall_5: 0.3781 - precision_5: 0.6352\n",
            "Epoch 346: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9193 - acc: 0.5727 - recall_5: 0.3837 - precision_5: 0.6392 - val_loss: 1.7300 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5806\n",
            "Epoch 347/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9186 - acc: 0.5375 - recall_5: 0.3734 - precision_5: 0.6051\n",
            "Epoch 347: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9121 - acc: 0.5407 - recall_5: 0.3765 - precision_5: 0.6094 - val_loss: 1.6583 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 348/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9156 - acc: 0.5625 - recall_5: 0.3875 - precision_5: 0.6327\n",
            "Epoch 348: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9092 - acc: 0.5610 - recall_5: 0.3895 - precision_5: 0.6381 - val_loss: 1.7019 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5806\n",
            "Epoch 349/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9097 - acc: 0.5777 - recall_5: 0.3857 - precision_5: 0.6294\n",
            "Epoch 349: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9083 - acc: 0.5785 - recall_5: 0.3837 - precision_5: 0.6271 - val_loss: 1.7438 - val_acc: 0.5500 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 350/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9164 - acc: 0.5609 - recall_5: 0.3670 - precision_5: 0.6361\n",
            "Epoch 350: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9253 - acc: 0.5538 - recall_5: 0.3619 - precision_5: 0.6241 - val_loss: 1.6928 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 351/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9197 - acc: 0.5442 - recall_5: 0.3659 - precision_5: 0.6061\n",
            "Epoch 351: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9175 - acc: 0.5494 - recall_5: 0.3692 - precision_5: 0.6120 - val_loss: 1.7031 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6429\n",
            "Epoch 352/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9074 - acc: 0.5672 - recall_5: 0.3969 - precision_5: 0.6287\n",
            "Epoch 352: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9045 - acc: 0.5698 - recall_5: 0.3910 - precision_5: 0.6315 - val_loss: 1.7217 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 353/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8948 - acc: 0.5625 - recall_5: 0.4183 - precision_5: 0.6525\n",
            "Epoch 353: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9019 - acc: 0.5552 - recall_5: 0.4142 - precision_5: 0.6448 - val_loss: 1.7006 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6800\n",
            "Epoch 354/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9155 - acc: 0.5849 - recall_5: 0.3606 - precision_5: 0.5968\n",
            "Epoch 354: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9116 - acc: 0.5901 - recall_5: 0.3706 - precision_5: 0.6071 - val_loss: 1.6154 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 355/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8984 - acc: 0.5844 - recall_5: 0.4062 - precision_5: 0.6566\n",
            "Epoch 355: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8993 - acc: 0.5814 - recall_5: 0.4012 - precision_5: 0.6479 - val_loss: 1.6950 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6667\n",
            "Epoch 356/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9104 - acc: 0.5562 - recall_5: 0.3688 - precision_5: 0.6361\n",
            "Epoch 356: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9106 - acc: 0.5538 - recall_5: 0.3692 - precision_5: 0.6334 - val_loss: 1.6585 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 357/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9034 - acc: 0.5701 - recall_5: 0.3704 - precision_5: 0.6183\n",
            "Epoch 357: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9067 - acc: 0.5669 - recall_5: 0.3750 - precision_5: 0.6217 - val_loss: 1.6902 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6207\n",
            "Epoch 358/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9055 - acc: 0.5672 - recall_5: 0.3906 - precision_5: 0.6173\n",
            "Epoch 358: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9099 - acc: 0.5625 - recall_5: 0.3866 - precision_5: 0.6129 - val_loss: 1.7175 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 359/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9105 - acc: 0.5595 - recall_5: 0.3887 - precision_5: 0.6375\n",
            "Epoch 359: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9045 - acc: 0.5654 - recall_5: 0.3953 - precision_5: 0.6415 - val_loss: 1.7179 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5455\n",
            "Epoch 360/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8965 - acc: 0.5817 - recall_5: 0.4006 - precision_5: 0.6410\n",
            "Epoch 360: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9032 - acc: 0.5785 - recall_5: 0.4026 - precision_5: 0.6427 - val_loss: 1.6742 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 361/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9092 - acc: 0.5750 - recall_5: 0.3891 - precision_5: 0.6194\n",
            "Epoch 361: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9215 - acc: 0.5625 - recall_5: 0.3779 - precision_5: 0.6103 - val_loss: 1.6565 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6429\n",
            "Epoch 362/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9186 - acc: 0.5640 - recall_5: 0.3704 - precision_5: 0.6279\n",
            "Epoch 362: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9127 - acc: 0.5683 - recall_5: 0.3765 - precision_5: 0.6348 - val_loss: 1.7188 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5455\n",
            "Epoch 363/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9075 - acc: 0.5727 - recall_5: 0.3939 - precision_5: 0.6288\n",
            "Epoch 363: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9075 - acc: 0.5727 - recall_5: 0.3939 - precision_5: 0.6288 - val_loss: 1.6696 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 364/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9079 - acc: 0.5494 - recall_5: 0.3910 - precision_5: 0.6256\n",
            "Epoch 364: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9079 - acc: 0.5494 - recall_5: 0.3910 - precision_5: 0.6256 - val_loss: 1.6688 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 365/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9036 - acc: 0.5887 - recall_5: 0.4215 - precision_5: 0.6561\n",
            "Epoch 365: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9036 - acc: 0.5887 - recall_5: 0.4215 - precision_5: 0.6561 - val_loss: 1.6751 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5806\n",
            "Epoch 366/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9034 - acc: 0.5701 - recall_5: 0.3643 - precision_5: 0.6051\n",
            "Epoch 366: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8966 - acc: 0.5799 - recall_5: 0.3706 - precision_5: 0.6145 - val_loss: 1.7229 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 367/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9002 - acc: 0.5641 - recall_5: 0.4141 - precision_5: 0.6310\n",
            "Epoch 367: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9009 - acc: 0.5683 - recall_5: 0.4157 - precision_5: 0.6370 - val_loss: 1.8211 - val_acc: 0.5750 - val_recall_5: 0.3750 - val_precision_5: 0.8333\n",
            "Epoch 368/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8937 - acc: 0.5734 - recall_5: 0.3969 - precision_5: 0.6530\n",
            "Epoch 368: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9031 - acc: 0.5654 - recall_5: 0.3881 - precision_5: 0.6357 - val_loss: 1.7635 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 369/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8879 - acc: 0.5945 - recall_5: 0.4085 - precision_5: 0.6553\n",
            "Epoch 369: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8968 - acc: 0.5872 - recall_5: 0.4041 - precision_5: 0.6480 - val_loss: 1.6535 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 370/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9153 - acc: 0.5737 - recall_5: 0.3702 - precision_5: 0.6160\n",
            "Epoch 370: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9083 - acc: 0.5843 - recall_5: 0.3765 - precision_5: 0.6181 - val_loss: 1.7143 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 371/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9145 - acc: 0.5686 - recall_5: 0.3796 - precision_5: 0.6336\n",
            "Epoch 371: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9162 - acc: 0.5698 - recall_5: 0.3837 - precision_5: 0.6346 - val_loss: 1.6576 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 372/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9026 - acc: 0.5705 - recall_5: 0.3990 - precision_5: 0.6401\n",
            "Epoch 372: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9064 - acc: 0.5669 - recall_5: 0.3953 - precision_5: 0.6340 - val_loss: 1.6807 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 373/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8892 - acc: 0.5823 - recall_5: 0.3902 - precision_5: 0.6368\n",
            "Epoch 373: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8947 - acc: 0.5756 - recall_5: 0.3866 - precision_5: 0.6364 - val_loss: 1.7710 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 374/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8963 - acc: 0.5793 - recall_5: 0.3918 - precision_5: 0.6490\n",
            "Epoch 374: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8951 - acc: 0.5799 - recall_5: 0.3968 - precision_5: 0.6500 - val_loss: 1.8088 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 375/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9184 - acc: 0.5480 - recall_5: 0.3910 - precision_5: 0.6004\n",
            "Epoch 375: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9184 - acc: 0.5480 - recall_5: 0.3910 - precision_5: 0.6004 - val_loss: 1.6140 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 376/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9034 - acc: 0.5641 - recall_5: 0.3844 - precision_5: 0.6228\n",
            "Epoch 376: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8985 - acc: 0.5669 - recall_5: 0.3895 - precision_5: 0.6336 - val_loss: 1.6668 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 377/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8905 - acc: 0.5859 - recall_5: 0.4094 - precision_5: 0.6534\n",
            "Epoch 377: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9031 - acc: 0.5741 - recall_5: 0.3997 - precision_5: 0.6351 - val_loss: 1.6537 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 378/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8844 - acc: 0.6076 - recall_5: 0.3895 - precision_5: 0.6617\n",
            "Epoch 378: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8844 - acc: 0.6076 - recall_5: 0.3895 - precision_5: 0.6617 - val_loss: 1.7290 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 379/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8931 - acc: 0.5946 - recall_5: 0.4167 - precision_5: 0.6650\n",
            "Epoch 379: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8889 - acc: 0.5945 - recall_5: 0.4128 - precision_5: 0.6651 - val_loss: 1.7044 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5806\n",
            "Epoch 380/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9173 - acc: 0.5593 - recall_5: 0.3830 - precision_5: 0.6240\n",
            "Epoch 380: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9095 - acc: 0.5683 - recall_5: 0.3910 - precision_5: 0.6344 - val_loss: 1.6571 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5938\n",
            "Epoch 381/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8955 - acc: 0.5705 - recall_5: 0.3814 - precision_5: 0.6485\n",
            "Epoch 381: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8913 - acc: 0.5799 - recall_5: 0.3939 - precision_5: 0.6530 - val_loss: 1.7287 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 382/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9060 - acc: 0.5698 - recall_5: 0.3939 - precision_5: 0.6273\n",
            "Epoch 382: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9060 - acc: 0.5698 - recall_5: 0.3939 - precision_5: 0.6273 - val_loss: 1.8226 - val_acc: 0.4250 - val_recall_5: 0.3750 - val_precision_5: 0.5769\n",
            "Epoch 383/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8969 - acc: 0.5878 - recall_5: 0.3914 - precision_5: 0.6542\n",
            "Epoch 383: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8968 - acc: 0.5843 - recall_5: 0.3895 - precision_5: 0.6537 - val_loss: 1.6949 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6296\n",
            "Epoch 384/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9074 - acc: 0.5701 - recall_5: 0.3918 - precision_5: 0.6457\n",
            "Epoch 384: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9051 - acc: 0.5712 - recall_5: 0.3924 - precision_5: 0.6459 - val_loss: 1.8057 - val_acc: 0.4250 - val_recall_5: 0.3750 - val_precision_5: 0.6522\n",
            "Epoch 385/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9203 - acc: 0.5593 - recall_5: 0.3878 - precision_5: 0.6436\n",
            "Epoch 385: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9063 - acc: 0.5683 - recall_5: 0.3953 - precision_5: 0.6523 - val_loss: 1.7380 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 386/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.9013 - acc: 0.5625 - recall_5: 0.3997 - precision_5: 0.6180\n",
            "Epoch 386: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9013 - acc: 0.5625 - recall_5: 0.3997 - precision_5: 0.6180 - val_loss: 1.6672 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6000\n",
            "Epoch 387/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.9109 - acc: 0.5565 - recall_5: 0.3631 - precision_5: 0.6010\n",
            "Epoch 387: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.9102 - acc: 0.5596 - recall_5: 0.3648 - precision_5: 0.6034 - val_loss: 1.6923 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 388/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.9126 - acc: 0.5529 - recall_5: 0.3702 - precision_5: 0.6311\n",
            "Epoch 388: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8986 - acc: 0.5640 - recall_5: 0.3823 - precision_5: 0.6462 - val_loss: 1.7378 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6071\n",
            "Epoch 389/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8952 - acc: 0.5801 - recall_5: 0.4135 - precision_5: 0.6386\n",
            "Epoch 389: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8922 - acc: 0.5843 - recall_5: 0.4201 - precision_5: 0.6465 - val_loss: 1.7055 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.5625\n",
            "Epoch 390/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9026 - acc: 0.5766 - recall_5: 0.4109 - precision_5: 0.6262\n",
            "Epoch 390: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8999 - acc: 0.5770 - recall_5: 0.4070 - precision_5: 0.6250 - val_loss: 1.6483 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5588\n",
            "Epoch 391/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8940 - acc: 0.5913 - recall_5: 0.4038 - precision_5: 0.6161\n",
            "Epoch 391: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8969 - acc: 0.5959 - recall_5: 0.4026 - precision_5: 0.6183 - val_loss: 1.6613 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 392/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.9065 - acc: 0.5688 - recall_5: 0.3781 - precision_5: 0.6302\n",
            "Epoch 392: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9002 - acc: 0.5727 - recall_5: 0.3852 - precision_5: 0.6401 - val_loss: 1.7109 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 393/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8785 - acc: 0.5916 - recall_5: 0.4302 - precision_5: 0.6607\n",
            "Epoch 393: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8785 - acc: 0.5916 - recall_5: 0.4302 - precision_5: 0.6607 - val_loss: 1.7031 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 394/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8993 - acc: 0.5688 - recall_5: 0.4078 - precision_5: 0.6366\n",
            "Epoch 394: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8885 - acc: 0.5785 - recall_5: 0.4186 - precision_5: 0.6486 - val_loss: 1.7984 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 395/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8925 - acc: 0.5781 - recall_5: 0.4047 - precision_5: 0.6395\n",
            "Epoch 395: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8979 - acc: 0.5814 - recall_5: 0.4041 - precision_5: 0.6333 - val_loss: 1.7048 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 396/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8830 - acc: 0.6058 - recall_5: 0.4359 - precision_5: 0.6492\n",
            "Epoch 396: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8915 - acc: 0.5988 - recall_5: 0.4346 - precision_5: 0.6472 - val_loss: 1.6775 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 397/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8969 - acc: 0.5766 - recall_5: 0.3953 - precision_5: 0.6389\n",
            "Epoch 397: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8928 - acc: 0.5785 - recall_5: 0.3953 - precision_5: 0.6400 - val_loss: 1.6694 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 398/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8924 - acc: 0.5744 - recall_5: 0.3988 - precision_5: 0.6351\n",
            "Epoch 398: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8947 - acc: 0.5712 - recall_5: 0.3968 - precision_5: 0.6319 - val_loss: 1.6911 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 399/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8780 - acc: 0.5769 - recall_5: 0.4247 - precision_5: 0.6511\n",
            "Epoch 399: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8810 - acc: 0.5770 - recall_5: 0.4259 - precision_5: 0.6497 - val_loss: 1.6973 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 400/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8833 - acc: 0.5906 - recall_5: 0.4281 - precision_5: 0.6417\n",
            "Epoch 400: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8898 - acc: 0.5858 - recall_5: 0.4273 - precision_5: 0.6447 - val_loss: 1.7330 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 401/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8971 - acc: 0.5641 - recall_5: 0.4094 - precision_5: 0.6268\n",
            "Epoch 401: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8946 - acc: 0.5698 - recall_5: 0.4142 - precision_5: 0.6319 - val_loss: 1.6564 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6667\n",
            "Epoch 402/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8882 - acc: 0.6094 - recall_5: 0.4187 - precision_5: 0.6521\n",
            "Epoch 402: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8887 - acc: 0.6017 - recall_5: 0.4172 - precision_5: 0.6449 - val_loss: 1.6237 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.5758\n",
            "Epoch 403/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8876 - acc: 0.5813 - recall_5: 0.4172 - precision_5: 0.6372\n",
            "Epoch 403: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8777 - acc: 0.5901 - recall_5: 0.4215 - precision_5: 0.6459 - val_loss: 1.6887 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6538\n",
            "Epoch 404/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8917 - acc: 0.5865 - recall_5: 0.4167 - precision_5: 0.6484\n",
            "Epoch 404: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8859 - acc: 0.5901 - recall_5: 0.4244 - precision_5: 0.6577 - val_loss: 1.7105 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 405/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8865 - acc: 0.5657 - recall_5: 0.4022 - precision_5: 0.6354\n",
            "Epoch 405: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8858 - acc: 0.5683 - recall_5: 0.4041 - precision_5: 0.6333 - val_loss: 1.6941 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 406/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.9096 - acc: 0.5686 - recall_5: 0.4040 - precision_5: 0.6250\n",
            "Epoch 406: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.9043 - acc: 0.5654 - recall_5: 0.4012 - precision_5: 0.6244 - val_loss: 1.6633 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.5862\n",
            "Epoch 407/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8967 - acc: 0.5701 - recall_5: 0.4055 - precision_5: 0.6087\n",
            "Epoch 407: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8894 - acc: 0.5741 - recall_5: 0.4142 - precision_5: 0.6182 - val_loss: 1.6809 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 408/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8694 - acc: 0.5833 - recall_5: 0.4215 - precision_5: 0.6232\n",
            "Epoch 408: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8700 - acc: 0.5858 - recall_5: 0.4288 - precision_5: 0.6277 - val_loss: 1.7508 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 409/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8833 - acc: 0.5769 - recall_5: 0.4327 - precision_5: 0.6338\n",
            "Epoch 409: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8846 - acc: 0.5770 - recall_5: 0.4346 - precision_5: 0.6321 - val_loss: 1.7339 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 410/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8749 - acc: 0.5884 - recall_5: 0.4329 - precision_5: 0.6574\n",
            "Epoch 410: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8737 - acc: 0.5887 - recall_5: 0.4317 - precision_5: 0.6585 - val_loss: 1.7796 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 411/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8915 - acc: 0.5781 - recall_5: 0.4094 - precision_5: 0.6344\n",
            "Epoch 411: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8988 - acc: 0.5785 - recall_5: 0.4055 - precision_5: 0.6270 - val_loss: 1.6914 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 412/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8820 - acc: 0.5808 - recall_5: 0.4162 - precision_5: 0.6305\n",
            "Epoch 412: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8805 - acc: 0.5872 - recall_5: 0.4201 - precision_5: 0.6338 - val_loss: 1.6688 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6296\n",
            "Epoch 413/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8836 - acc: 0.5844 - recall_5: 0.4250 - precision_5: 0.6370\n",
            "Epoch 413: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8802 - acc: 0.5814 - recall_5: 0.4230 - precision_5: 0.6396 - val_loss: 1.7010 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6538\n",
            "Epoch 414/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8957 - acc: 0.5844 - recall_5: 0.4219 - precision_5: 0.6398\n",
            "Epoch 414: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8857 - acc: 0.5916 - recall_5: 0.4273 - precision_5: 0.6462 - val_loss: 1.7307 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 415/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8865 - acc: 0.5878 - recall_5: 0.4271 - precision_5: 0.6378\n",
            "Epoch 415: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8879 - acc: 0.5916 - recall_5: 0.4288 - precision_5: 0.6399 - val_loss: 1.6838 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 416/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8872 - acc: 0.5814 - recall_5: 0.4172 - precision_5: 0.6406\n",
            "Epoch 416: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.8872 - acc: 0.5814 - recall_5: 0.4172 - precision_5: 0.6406 - val_loss: 1.7878 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 417/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8956 - acc: 0.5654 - recall_5: 0.4230 - precision_5: 0.6299\n",
            "Epoch 417: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8956 - acc: 0.5654 - recall_5: 0.4230 - precision_5: 0.6299 - val_loss: 1.7147 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 418/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8947 - acc: 0.5828 - recall_5: 0.4094 - precision_5: 0.6534\n",
            "Epoch 418: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8914 - acc: 0.5814 - recall_5: 0.4099 - precision_5: 0.6543 - val_loss: 1.6872 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6800\n",
            "Epoch 419/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8814 - acc: 0.5875 - recall_5: 0.4172 - precision_5: 0.6403\n",
            "Epoch 419: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8816 - acc: 0.5872 - recall_5: 0.4215 - precision_5: 0.6388 - val_loss: 1.6945 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 420/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8776 - acc: 0.5844 - recall_5: 0.4094 - precision_5: 0.6422\n",
            "Epoch 420: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8867 - acc: 0.5843 - recall_5: 0.4012 - precision_5: 0.6374 - val_loss: 1.8381 - val_acc: 0.4250 - val_recall_5: 0.3750 - val_precision_5: 0.6000\n",
            "Epoch 421/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8743 - acc: 0.5869 - recall_5: 0.4253 - precision_5: 0.6443\n",
            "Epoch 421: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8739 - acc: 0.5872 - recall_5: 0.4244 - precision_5: 0.6460 - val_loss: 1.7154 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.7083\n",
            "Epoch 422/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8698 - acc: 0.6042 - recall_5: 0.4455 - precision_5: 0.6557\n",
            "Epoch 422: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8791 - acc: 0.6047 - recall_5: 0.4462 - precision_5: 0.6504 - val_loss: 1.8171 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 423/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8832 - acc: 0.5565 - recall_5: 0.4286 - precision_5: 0.6302\n",
            "Epoch 423: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8813 - acc: 0.5610 - recall_5: 0.4288 - precision_5: 0.6317 - val_loss: 1.6688 - val_acc: 0.4750 - val_recall_5: 0.4500 - val_precision_5: 0.6429\n",
            "Epoch 424/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8918 - acc: 0.5908 - recall_5: 0.4092 - precision_5: 0.6336\n",
            "Epoch 424: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8907 - acc: 0.5916 - recall_5: 0.4128 - precision_5: 0.6368 - val_loss: 1.7256 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 425/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8868 - acc: 0.5887 - recall_5: 0.4375 - precision_5: 0.6404\n",
            "Epoch 425: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8868 - acc: 0.5887 - recall_5: 0.4375 - precision_5: 0.6404 - val_loss: 1.7296 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 426/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8870 - acc: 0.5729 - recall_5: 0.4062 - precision_5: 0.6500\n",
            "Epoch 426: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8897 - acc: 0.5698 - recall_5: 0.4041 - precision_5: 0.6450 - val_loss: 1.7093 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 427/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8833 - acc: 0.5849 - recall_5: 0.4231 - precision_5: 0.6423\n",
            "Epoch 427: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8807 - acc: 0.5930 - recall_5: 0.4244 - precision_5: 0.6475 - val_loss: 1.6848 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 428/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8795 - acc: 0.5781 - recall_5: 0.4062 - precision_5: 0.6280\n",
            "Epoch 428: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8816 - acc: 0.5799 - recall_5: 0.4113 - precision_5: 0.6289 - val_loss: 1.6960 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 429/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8859 - acc: 0.5671 - recall_5: 0.4177 - precision_5: 0.6462\n",
            "Epoch 429: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8852 - acc: 0.5727 - recall_5: 0.4186 - precision_5: 0.6516 - val_loss: 1.6584 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 430/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8755 - acc: 0.5875 - recall_5: 0.4297 - precision_5: 0.6425\n",
            "Epoch 430: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8720 - acc: 0.5843 - recall_5: 0.4302 - precision_5: 0.6393 - val_loss: 1.7967 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.5161\n",
            "Epoch 431/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8785 - acc: 0.5823 - recall_5: 0.4040 - precision_5: 0.6177\n",
            "Epoch 431: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8774 - acc: 0.5858 - recall_5: 0.4055 - precision_5: 0.6214 - val_loss: 1.6752 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6667\n",
            "Epoch 432/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8827 - acc: 0.5578 - recall_5: 0.4109 - precision_5: 0.6322\n",
            "Epoch 432: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8804 - acc: 0.5625 - recall_5: 0.4113 - precision_5: 0.6303 - val_loss: 1.7558 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 433/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8859 - acc: 0.5781 - recall_5: 0.4359 - precision_5: 0.6399\n",
            "Epoch 433: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8814 - acc: 0.5799 - recall_5: 0.4375 - precision_5: 0.6432 - val_loss: 1.6715 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 434/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8887 - acc: 0.5872 - recall_5: 0.4491 - precision_5: 0.6561\n",
            "Epoch 434: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8887 - acc: 0.5872 - recall_5: 0.4491 - precision_5: 0.6561 - val_loss: 1.7009 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 435/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8768 - acc: 0.5705 - recall_5: 0.4151 - precision_5: 0.6475\n",
            "Epoch 435: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8784 - acc: 0.5683 - recall_5: 0.4157 - precision_5: 0.6413 - val_loss: 1.6872 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6667\n",
            "Epoch 436/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8930 - acc: 0.5750 - recall_5: 0.4172 - precision_5: 0.6372\n",
            "Epoch 436: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8914 - acc: 0.5814 - recall_5: 0.4259 - precision_5: 0.6411 - val_loss: 1.6934 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 437/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8706 - acc: 0.5906 - recall_5: 0.4344 - precision_5: 0.6376\n",
            "Epoch 437: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8817 - acc: 0.5828 - recall_5: 0.4259 - precision_5: 0.6328 - val_loss: 1.7973 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.5333\n",
            "Epoch 438/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8904 - acc: 0.5683 - recall_5: 0.4041 - precision_5: 0.6362\n",
            "Epoch 438: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8904 - acc: 0.5683 - recall_5: 0.4041 - precision_5: 0.6362 - val_loss: 1.7125 - val_acc: 0.4250 - val_recall_5: 0.3250 - val_precision_5: 0.6190\n",
            "Epoch 439/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8879 - acc: 0.5759 - recall_5: 0.4092 - precision_5: 0.6486\n",
            "Epoch 439: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8831 - acc: 0.5785 - recall_5: 0.4142 - precision_5: 0.6522 - val_loss: 1.7114 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 440/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8955 - acc: 0.5712 - recall_5: 0.4390 - precision_5: 0.6331\n",
            "Epoch 440: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8955 - acc: 0.5712 - recall_5: 0.4390 - precision_5: 0.6331 - val_loss: 1.6685 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 441/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8864 - acc: 0.5625 - recall_5: 0.4047 - precision_5: 0.6196\n",
            "Epoch 441: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8821 - acc: 0.5698 - recall_5: 0.4099 - precision_5: 0.6239 - val_loss: 1.6577 - val_acc: 0.4750 - val_recall_5: 0.4250 - val_precision_5: 0.6296\n",
            "Epoch 442/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8778 - acc: 0.5804 - recall_5: 0.4241 - precision_5: 0.6522\n",
            "Epoch 442: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8765 - acc: 0.5828 - recall_5: 0.4259 - precision_5: 0.6526 - val_loss: 1.7335 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 443/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8692 - acc: 0.5804 - recall_5: 0.4256 - precision_5: 0.6590\n",
            "Epoch 443: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8736 - acc: 0.5756 - recall_5: 0.4215 - precision_5: 0.6561 - val_loss: 1.6840 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 444/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8873 - acc: 0.5762 - recall_5: 0.4207 - precision_5: 0.6202\n",
            "Epoch 444: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8913 - acc: 0.5698 - recall_5: 0.4113 - precision_5: 0.6179 - val_loss: 1.6669 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 445/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8791 - acc: 0.5744 - recall_5: 0.3988 - precision_5: 0.6247\n",
            "Epoch 445: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8774 - acc: 0.5741 - recall_5: 0.3983 - precision_5: 0.6241 - val_loss: 1.7266 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.7273\n",
            "Epoch 446/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8668 - acc: 0.6042 - recall_5: 0.4375 - precision_5: 0.6547\n",
            "Epoch 446: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8826 - acc: 0.5916 - recall_5: 0.4273 - precision_5: 0.6419 - val_loss: 1.7071 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 447/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8806 - acc: 0.5906 - recall_5: 0.4266 - precision_5: 0.6469\n",
            "Epoch 447: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8823 - acc: 0.5843 - recall_5: 0.4215 - precision_5: 0.6488 - val_loss: 1.6716 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6667\n",
            "Epoch 448/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8726 - acc: 0.5712 - recall_5: 0.4288 - precision_5: 0.6498\n",
            "Epoch 448: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8726 - acc: 0.5712 - recall_5: 0.4288 - precision_5: 0.6498 - val_loss: 1.7392 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.5926\n",
            "Epoch 449/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8933 - acc: 0.5785 - recall_5: 0.4103 - precision_5: 0.6416\n",
            "Epoch 449: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8829 - acc: 0.5887 - recall_5: 0.4157 - precision_5: 0.6530 - val_loss: 1.7008 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.5926\n",
            "Epoch 450/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8724 - acc: 0.5922 - recall_5: 0.4484 - precision_5: 0.6553\n",
            "Epoch 450: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8754 - acc: 0.5959 - recall_5: 0.4491 - precision_5: 0.6574 - val_loss: 1.7013 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 451/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8816 - acc: 0.5891 - recall_5: 0.4203 - precision_5: 0.6626\n",
            "Epoch 451: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8771 - acc: 0.5901 - recall_5: 0.4244 - precision_5: 0.6621 - val_loss: 1.6840 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 452/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8650 - acc: 0.5727 - recall_5: 0.4419 - precision_5: 0.6609\n",
            "Epoch 452: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8650 - acc: 0.5727 - recall_5: 0.4419 - precision_5: 0.6609 - val_loss: 1.6703 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 453/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8850 - acc: 0.5762 - recall_5: 0.4223 - precision_5: 0.6239\n",
            "Epoch 453: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8827 - acc: 0.5828 - recall_5: 0.4288 - precision_5: 0.6317 - val_loss: 1.7227 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6667\n",
            "Epoch 454/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8767 - acc: 0.5891 - recall_5: 0.4406 - precision_5: 0.6498\n",
            "Epoch 454: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8741 - acc: 0.5974 - recall_5: 0.4375 - precision_5: 0.6558 - val_loss: 1.7234 - val_acc: 0.4250 - val_recall_5: 0.3750 - val_precision_5: 0.6522\n",
            "Epoch 455/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8736 - acc: 0.5959 - recall_5: 0.4244 - precision_5: 0.6621\n",
            "Epoch 455: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8736 - acc: 0.5959 - recall_5: 0.4244 - precision_5: 0.6621 - val_loss: 1.6607 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 456/500\n",
            "38/43 [=========================>....] - ETA: 0s - loss: 0.8537 - acc: 0.5905 - recall_5: 0.4391 - precision_5: 0.6418\n",
            "Epoch 456: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8641 - acc: 0.5770 - recall_5: 0.4259 - precision_5: 0.6301 - val_loss: 1.6924 - val_acc: 0.4250 - val_recall_5: 0.3750 - val_precision_5: 0.6818\n",
            "Epoch 457/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8872 - acc: 0.5655 - recall_5: 0.4003 - precision_5: 0.6184\n",
            "Epoch 457: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8893 - acc: 0.5640 - recall_5: 0.4012 - precision_5: 0.6188 - val_loss: 1.6973 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 458/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8681 - acc: 0.5804 - recall_5: 0.4301 - precision_5: 0.6408\n",
            "Epoch 458: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8704 - acc: 0.5785 - recall_5: 0.4259 - precision_5: 0.6356 - val_loss: 1.6940 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6667\n",
            "Epoch 459/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8703 - acc: 0.5945 - recall_5: 0.4329 - precision_5: 0.6559\n",
            "Epoch 459: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8765 - acc: 0.5916 - recall_5: 0.4302 - precision_5: 0.6520 - val_loss: 1.6982 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 460/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8764 - acc: 0.5759 - recall_5: 0.4182 - precision_5: 0.6475\n",
            "Epoch 460: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8810 - acc: 0.5741 - recall_5: 0.4201 - precision_5: 0.6422 - val_loss: 1.6959 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 461/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8631 - acc: 0.5797 - recall_5: 0.4422 - precision_5: 0.6476\n",
            "Epoch 461: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 0.8684 - acc: 0.5770 - recall_5: 0.4419 - precision_5: 0.6414 - val_loss: 1.6894 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 462/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8648 - acc: 0.5988 - recall_5: 0.4433 - precision_5: 0.6462\n",
            "Epoch 462: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8648 - acc: 0.5988 - recall_5: 0.4433 - precision_5: 0.6462 - val_loss: 1.7253 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 463/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8775 - acc: 0.6006 - recall_5: 0.4680 - precision_5: 0.6504\n",
            "Epoch 463: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8832 - acc: 0.5974 - recall_5: 0.4578 - precision_5: 0.6455 - val_loss: 1.7089 - val_acc: 0.4250 - val_recall_5: 0.3750 - val_precision_5: 0.6522\n",
            "Epoch 464/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8701 - acc: 0.5833 - recall_5: 0.4167 - precision_5: 0.6280\n",
            "Epoch 464: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8740 - acc: 0.5814 - recall_5: 0.4186 - precision_5: 0.6302 - val_loss: 1.7148 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 465/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8640 - acc: 0.5859 - recall_5: 0.4469 - precision_5: 0.6560\n",
            "Epoch 465: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8723 - acc: 0.5785 - recall_5: 0.4404 - precision_5: 0.6419 - val_loss: 1.7423 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.5926\n",
            "Epoch 466/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8669 - acc: 0.5997 - recall_5: 0.4167 - precision_5: 0.6512\n",
            "Epoch 466: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8640 - acc: 0.6017 - recall_5: 0.4201 - precision_5: 0.6524 - val_loss: 1.7683 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 467/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8560 - acc: 0.5984 - recall_5: 0.4531 - precision_5: 0.6591\n",
            "Epoch 467: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8598 - acc: 0.5959 - recall_5: 0.4506 - precision_5: 0.6596 - val_loss: 1.6703 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 468/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8632 - acc: 0.5609 - recall_5: 0.4406 - precision_5: 0.6424\n",
            "Epoch 468: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8646 - acc: 0.5669 - recall_5: 0.4404 - precision_5: 0.6406 - val_loss: 1.7369 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 469/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8663 - acc: 0.6078 - recall_5: 0.4672 - precision_5: 0.6586\n",
            "Epoch 469: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8660 - acc: 0.6032 - recall_5: 0.4578 - precision_5: 0.6549 - val_loss: 1.8022 - val_acc: 0.4250 - val_recall_5: 0.4250 - val_precision_5: 0.5484\n",
            "Epoch 470/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8748 - acc: 0.5908 - recall_5: 0.4390 - precision_5: 0.6498\n",
            "Epoch 470: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8749 - acc: 0.5887 - recall_5: 0.4375 - precision_5: 0.6487 - val_loss: 1.6895 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 471/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8635 - acc: 0.5930 - recall_5: 0.4491 - precision_5: 0.6547\n",
            "Epoch 471: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8635 - acc: 0.5930 - recall_5: 0.4491 - precision_5: 0.6547 - val_loss: 1.7560 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 472/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8840 - acc: 0.5865 - recall_5: 0.4231 - precision_5: 0.6471\n",
            "Epoch 472: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8832 - acc: 0.5814 - recall_5: 0.4186 - precision_5: 0.6386 - val_loss: 1.6917 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 473/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8892 - acc: 0.5497 - recall_5: 0.4054 - precision_5: 0.6201\n",
            "Epoch 473: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8737 - acc: 0.5625 - recall_5: 0.4172 - precision_5: 0.6421 - val_loss: 1.7083 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 474/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8908 - acc: 0.5750 - recall_5: 0.4187 - precision_5: 0.6291\n",
            "Epoch 474: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8789 - acc: 0.5843 - recall_5: 0.4273 - precision_5: 0.6391 - val_loss: 1.7662 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 475/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8971 - acc: 0.5753 - recall_5: 0.4391 - precision_5: 0.6343\n",
            "Epoch 475: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8740 - acc: 0.5887 - recall_5: 0.4535 - precision_5: 0.6514 - val_loss: 1.7522 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 476/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8770 - acc: 0.5848 - recall_5: 0.4301 - precision_5: 0.6338\n",
            "Epoch 476: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8734 - acc: 0.5887 - recall_5: 0.4346 - precision_5: 0.6389 - val_loss: 1.7532 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6667\n",
            "Epoch 477/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8695 - acc: 0.5838 - recall_5: 0.4451 - precision_5: 0.6562\n",
            "Epoch 477: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8704 - acc: 0.5785 - recall_5: 0.4419 - precision_5: 0.6552 - val_loss: 1.7487 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6667\n",
            "Epoch 478/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8677 - acc: 0.6061 - recall_5: 0.4404 - precision_5: 0.6299\n",
            "Epoch 478: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8677 - acc: 0.6061 - recall_5: 0.4404 - precision_5: 0.6299 - val_loss: 1.7755 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 479/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8625 - acc: 0.6076 - recall_5: 0.4564 - precision_5: 0.6501\n",
            "Epoch 479: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8625 - acc: 0.6076 - recall_5: 0.4564 - precision_5: 0.6501 - val_loss: 1.7148 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6667\n",
            "Epoch 480/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8580 - acc: 0.5915 - recall_5: 0.4558 - precision_5: 0.6416\n",
            "Epoch 480: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8592 - acc: 0.5901 - recall_5: 0.4578 - precision_5: 0.6455 - val_loss: 1.7726 - val_acc: 0.4250 - val_recall_5: 0.4250 - val_precision_5: 0.6071\n",
            "Epoch 481/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8644 - acc: 0.5843 - recall_5: 0.4506 - precision_5: 0.6418\n",
            "Epoch 481: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8644 - acc: 0.5843 - recall_5: 0.4506 - precision_5: 0.6418 - val_loss: 1.7384 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 482/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8637 - acc: 0.5865 - recall_5: 0.4359 - precision_5: 0.6430\n",
            "Epoch 482: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8574 - acc: 0.5858 - recall_5: 0.4419 - precision_5: 0.6468 - val_loss: 1.7334 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 483/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8604 - acc: 0.5881 - recall_5: 0.4439 - precision_5: 0.6427\n",
            "Epoch 483: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8657 - acc: 0.5814 - recall_5: 0.4462 - precision_5: 0.6369 - val_loss: 1.7047 - val_acc: 0.4750 - val_recall_5: 0.4750 - val_precision_5: 0.6129\n",
            "Epoch 484/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8757 - acc: 0.5915 - recall_5: 0.4466 - precision_5: 0.6383\n",
            "Epoch 484: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8673 - acc: 0.5930 - recall_5: 0.4535 - precision_5: 0.6420 - val_loss: 1.7730 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 485/500\n",
            "41/43 [===========================>..] - ETA: 0s - loss: 0.8593 - acc: 0.5838 - recall_5: 0.4665 - precision_5: 0.6581\n",
            "Epoch 485: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8600 - acc: 0.5814 - recall_5: 0.4666 - precision_5: 0.6578 - val_loss: 1.7367 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.6400\n",
            "Epoch 486/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8410 - acc: 0.6012 - recall_5: 0.4613 - precision_5: 0.6624\n",
            "Epoch 486: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8442 - acc: 0.6017 - recall_5: 0.4564 - precision_5: 0.6583 - val_loss: 1.7952 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 487/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8712 - acc: 0.6062 - recall_5: 0.4484 - precision_5: 0.6322\n",
            "Epoch 487: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8615 - acc: 0.6192 - recall_5: 0.4637 - precision_5: 0.6497 - val_loss: 1.8122 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6667\n",
            "Epoch 488/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8690 - acc: 0.6234 - recall_5: 0.4663 - precision_5: 0.6481\n",
            "Epoch 488: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8629 - acc: 0.6235 - recall_5: 0.4564 - precision_5: 0.6474 - val_loss: 1.7942 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.5714\n",
            "Epoch 489/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8659 - acc: 0.6057 - recall_5: 0.4554 - precision_5: 0.6483\n",
            "Epoch 489: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8694 - acc: 0.6017 - recall_5: 0.4549 - precision_5: 0.6440 - val_loss: 1.7475 - val_acc: 0.4750 - val_recall_5: 0.3750 - val_precision_5: 0.6522\n",
            "Epoch 490/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8645 - acc: 0.5756 - recall_5: 0.4346 - precision_5: 0.6335\n",
            "Epoch 490: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8645 - acc: 0.5756 - recall_5: 0.4346 - precision_5: 0.6335 - val_loss: 1.7806 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.5926\n",
            "Epoch 491/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8480 - acc: 0.5929 - recall_5: 0.4391 - precision_5: 0.6478\n",
            "Epoch 491: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8562 - acc: 0.5843 - recall_5: 0.4273 - precision_5: 0.6377 - val_loss: 1.8057 - val_acc: 0.4250 - val_recall_5: 0.4250 - val_precision_5: 0.5484\n",
            "Epoch 492/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8648 - acc: 0.5916 - recall_5: 0.4549 - precision_5: 0.6688\n",
            "Epoch 492: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8648 - acc: 0.5916 - recall_5: 0.4549 - precision_5: 0.6688 - val_loss: 1.7681 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 493/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8520 - acc: 0.5913 - recall_5: 0.4663 - precision_5: 0.6584\n",
            "Epoch 493: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8610 - acc: 0.5887 - recall_5: 0.4608 - precision_5: 0.6550 - val_loss: 1.7531 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 494/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8661 - acc: 0.6042 - recall_5: 0.4503 - precision_5: 0.6550\n",
            "Epoch 494: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8477 - acc: 0.6163 - recall_5: 0.4564 - precision_5: 0.6667 - val_loss: 1.7409 - val_acc: 0.4750 - val_recall_5: 0.4000 - val_precision_5: 0.5926\n",
            "Epoch 495/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8757 - acc: 0.5872 - recall_5: 0.4695 - precision_5: 0.6260\n",
            "Epoch 495: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8757 - acc: 0.5872 - recall_5: 0.4695 - precision_5: 0.6260 - val_loss: 1.6741 - val_acc: 0.4500 - val_recall_5: 0.4000 - val_precision_5: 0.6957\n",
            "Epoch 496/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8672 - acc: 0.5766 - recall_5: 0.4266 - precision_5: 0.6379\n",
            "Epoch 496: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8690 - acc: 0.5770 - recall_5: 0.4317 - precision_5: 0.6373 - val_loss: 1.7248 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.5714\n",
            "Epoch 497/500\n",
            "40/43 [==========================>...] - ETA: 0s - loss: 0.8641 - acc: 0.5969 - recall_5: 0.4391 - precision_5: 0.6535\n",
            "Epoch 497: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8633 - acc: 0.6003 - recall_5: 0.4404 - precision_5: 0.6544 - val_loss: 1.7120 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.6154\n",
            "Epoch 498/500\n",
            "42/43 [============================>.] - ETA: 0s - loss: 0.8655 - acc: 0.5938 - recall_5: 0.4509 - precision_5: 0.6474\n",
            "Epoch 498: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8654 - acc: 0.5945 - recall_5: 0.4477 - precision_5: 0.6457 - val_loss: 1.7355 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.5926\n",
            "Epoch 499/500\n",
            "43/43 [==============================] - ETA: 0s - loss: 0.8587 - acc: 0.6032 - recall_5: 0.4477 - precision_5: 0.6609\n",
            "Epoch 499: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 0.8587 - acc: 0.6032 - recall_5: 0.4477 - precision_5: 0.6609 - val_loss: 1.7599 - val_acc: 0.4250 - val_recall_5: 0.4250 - val_precision_5: 0.5484\n",
            "Epoch 500/500\n",
            "39/43 [==========================>...] - ETA: 0s - loss: 0.8469 - acc: 0.6202 - recall_5: 0.4679 - precision_5: 0.6547\n",
            "Epoch 500: val_loss did not improve from 1.27970\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 0.8419 - acc: 0.6192 - recall_5: 0.4695 - precision_5: 0.6592 - val_loss: 1.7266 - val_acc: 0.4250 - val_recall_5: 0.4000 - val_precision_5: 0.5926\n"
          ]
        }
      ],
      "source": [
        "best_weights_file = 'weights.best.hdf5'\n",
        "checkpoint = ModelCheckpoint(best_weights_file,monitor='val_loss',verbose=1,\n",
        "                             save_best_only=True,mode='min')\n",
        "\n",
        "callbacks = [checkpoint]\n",
        "\n",
        "\n",
        "train_history = model.fit([x_delta_aug2[40:],x_theta_aug2[40:],x_alpha_aug2[40:],\n",
        "                           x_beta_aug2[40:],x_gamma_aug2[40:]],y_train_aug2[40:],\n",
        "                          batch_size=16,epochs=500,callbacks=callbacks,\n",
        "                          validation_data=validation_data,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = train_history.epoch[-1]+1\n",
        "history = train_history.history"
      ],
      "metadata": {
        "id": "UeDq8adZ2KBg"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
        "\n",
        "axes[0].plot(range(1, num_epochs+1), history['loss'])\n",
        "axes[0].plot(range(1, num_epochs+1), history['val_loss'])\n",
        "axes[0].legend(['Train loss curve', 'Validation loss curve'])\n",
        "axes[0].set_xlabel('epoch')\n",
        "axes[0].set_ylabel('loss')\n",
        "\n",
        "\n",
        "axes[1].plot(range(1, num_epochs+1), history['acc'])\n",
        "axes[1].plot(range(1, num_epochs+1), history['val_acc'])\n",
        "axes[1].legend(['Train accuracy curve', 'Validation accuracy curve'])\n",
        "axes[1].set_xlabel('epoch')\n",
        "axes[1].set_ylabel('accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "leqtoAtf28yj",
        "outputId": "efac7db9-9fb7-4b6c-e162-83afdaa6fa66"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# early stopping\n",
        "early_stop_epoch = np.argmax(history['val_acc'])\n",
        "print('The early stop epoch is: ', early_stop_epoch)\n",
        "print('Train acc (Early stopping): %.2f' % history['acc'][early_stop_epoch])\n",
        "print('Validation acc (Early stopping): %.2f' % history['val_acc'][early_stop_epoch])"
      ],
      "metadata": {
        "id": "d3W6JmXP3P-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d5b5a8-bf96-45c0-a48e-7cff79489397"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The early stop epoch is:  0\n",
            "Train acc (Early stopping): 0.39\n",
            "Validation acc (Early stopping): 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SexDioE2XZZH"
      },
      "execution_count": 80,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hBIzO3XHML7S"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}